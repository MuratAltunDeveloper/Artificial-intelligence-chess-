{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"t4jK41UULfwI"},"outputs":[],"source":["#ULTRA SUPER SIGMA CHESS AI"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfMjit5oLfwI","outputId":"e1894f7b-7121-412b-d17e-a7bad521e28f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: chess in /usr/local/lib/python3.10/dist-packages (1.11.1)\n"]}],"source":["!pip install chess"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYnWlePILfwJ","outputId":"e15232ae-1c2d-4dc2-ebc7-d66caeea50d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!chmod +x \"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBlKKoQmLfwJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2d0b07d-eddf-473e-d96b-3f0fb11efd42"},"outputs":[{"output_type":"stream","name":"stdout","text":["All devices:  [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import math\n","\n","import chess\n","\n","from collections import deque\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import HeNormal\n","\n","import chess.engine\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","import threading\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","class GameState:\n","    row = 8\n","    col = 8\n","    promotion_indexes = {\n","        chess.KNIGHT: 0,\n","        chess.ROOK: 1,\n","        chess.BISHOP: 2\n","    }\n","\n","    def __init__(self) -> None:\n","        self.board = chess.Board()\n","        self.repetition_count = 0\n","        self.player_color: chess.Color = chess.WHITE\n","\n","    def get_initial_state(self):\n","        self.board.reset()\n","\n","        return self.get_current_state()\n","\n","    def get_current_state(self, T=8):\n","        input_tensor = np.zeros((8, 8, 119), dtype=np.uint8)\n","\n","        for t in range(T):\n","            _t = T - t - 1\n","            if len(self.board.move_stack) < _t:\n","                continue\n","\n","            self.create_input(input_tensor, _t)\n","\n","        color = 0 if self.board.turn == chess.WHITE else 1\n","        input_tensor[:, :, 112] = color\n","\n","        input_tensor[:, :, 113] = len(self.board.move_stack) > 0\n","\n","        p1_castling = (1 * self.board.has_kingside_castling_rights(chess.WHITE)) | (2 * self.board.has_queenside_castling_rights(chess.WHITE))\n","        p1_castling_bit = format(p1_castling, \"02b\")\n","        input_tensor[:, :, 114] = int(p1_castling_bit[0])\n","        input_tensor[:, :, 115] = int(p1_castling_bit[1])\n","\n","        p2_castling = (1 * self.board.has_kingside_castling_rights(chess.BLACK)) | (2 * self.board.has_queenside_castling_rights(chess.BLACK))\n","        p2_castling_bit = format(p2_castling, \"02b\")\n","        input_tensor[:, :, 116] = int(p2_castling_bit[0])\n","        input_tensor[:, :, 117] = int(p2_castling_bit[1])\n","\n","        input_tensor[:, :, 118] = int(self.board.is_fifty_moves())\n","\n","        return np.expand_dims(input_tensor, axis=0)\n","\n","    def get_next_state(self, action: int):\n","        source_index = action // 73\n","        destination_index = 0\n","        move_type = action % 73\n","\n","        promotion = None\n","\n","        if move_type < 56:\n","            direction = move_type // 7\n","            movement = (move_type % 7) + 1\n","\n","            destination_index = source_index + (movement * 8) if direction == 0 else destination_index\n","            destination_index = source_index + (movement * 9) if direction == 1 else destination_index\n","            destination_index = source_index + movement if direction == 2 else destination_index\n","            destination_index = source_index + (movement * -7) if direction == 3 else destination_index\n","            destination_index = source_index + (movement * -8) if direction == 4 else destination_index\n","            destination_index = source_index + (movement * -9) if direction == 5 else destination_index\n","            destination_index = source_index + (-movement) if direction == 6 else destination_index\n","            destination_index = source_index + (movement * 7) if direction == 7 else destination_index\n","        elif move_type >= 56 and move_type < 64:\n","            direction = move_type - 56\n","\n","            destination_index = source_index + 17 if direction == 0 else destination_index\n","            destination_index = source_index + 10 if direction == 1 else destination_index\n","            destination_index = source_index - 6 if direction == 2 else destination_index\n","            destination_index = source_index - 15 if direction == 3 else destination_index\n","            destination_index = source_index - 17 if direction == 4 else destination_index\n","            destination_index = source_index - 10 if direction == 5 else destination_index\n","            destination_index = source_index + 6 if direction == 6 else destination_index\n","            destination_index = source_index + 15 if direction == 7 else destination_index\n","        else:\n","            direction = (move_type - 64) // 3\n","            promotion_index = (move_type - 64) % 3\n","\n","            promotion = chess.KNIGHT if promotion_index == 0 else (chess.ROOK if promotion_index == 1 else chess.BISHOP)\n","\n","            color_value = 1 if self.board.turn == chess.WHITE else -1\n","\n","            if direction == 0:\n","                destination_index = source_index + (8 * color_value)\n","            elif direction == 1:\n","                destination_index = source_index + (9 * color_value)\n","            else:\n","                destination_index = source_index + (7 * color_value)\n","\n","        from_square = chess.Square(source_index)\n","        to_square = chess.Square(destination_index)\n","\n","        promotion_rank = 7 if self.board.turn == chess.WHITE else 0\n","\n","        if promotion is None:\n","            if self.board.piece_type_at(from_square) == chess.PAWN and chess.square_rank(to_square) == promotion_rank:\n","                promotion = chess.QUEEN\n","\n","        move = chess.Move(from_square, to_square, promotion)\n","\n","        self.apply_action(move)\n","\n","        return move, self.get_current_state()\n","\n","    def apply_action(self, move: chess.Move):\n","        try:\n","            self.board.push(move)\n","        except Exception as e:\n","            print(list(self.board.legal_moves))\n","            print(self.get_valid_moves())\n","\n","            print(e)\n","\n","            raise Exception(\"Error\")\n","\n","    def create_input(self, input_tensor: np.ndarray, t: int):\n","        piece_types = {\n","            chess.PAWN: 0,\n","            chess.KNIGHT: 1,\n","            chess.BISHOP: 2,\n","            chess.ROOK: 3,\n","            chess.QUEEN: 4,\n","            chess.KING: 5\n","        }\n","\n","        board = self.board.copy()\n","        for _ in range(t):\n","            board.pop()\n","\n","        transposition_key = board._transposition_key()\n","\n","        for square in chess.SQUARES:\n","            piece = board.piece_at(square)\n","\n","            if piece is None:\n","                continue\n","\n","            piece_index = piece_types[piece.piece_type]\n","            piece_color = 0 if piece.color == chess.WHITE else 1\n","\n","            index = (t * 14) + (piece_color * 6) + piece_index\n","            input_tensor[square // 8][square % 8][index] = 1\n","\n","        repetition_count = 0\n","        index = (t * 14) + 12\n","\n","        try:\n","            while board.move_stack:\n","                move = board.pop()\n","                if board.is_irreversible(move):\n","                    break\n","\n","                if board._transposition_key() == transposition_key:\n","                    repetition_count += 1\n","\n","                if repetition_count == 3:\n","                    break\n","        finally:\n","            repetition_count = 3 if repetition_count > 3 else repetition_count\n","\n","            repetition_count_bits = [int(x) for x in format(repetition_count, \"02b\")]\n","            input_tensor[:, :, index] = repetition_count_bits[0]\n","            input_tensor[:, :, index + 1] = repetition_count_bits[1]\n","\n","    def get_valid_moves(self):\n","        legal_moves = []\n","\n","        for valid_move in self.board.legal_moves:\n","            s_row, s_col, from_square_index = self.index_of_square(valid_move.from_square)\n","            d_row, d_col, to_square_index = self.index_of_square(valid_move.to_square)\n","\n","            if valid_move.promotion:\n","                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n","\n","                if valid_move.promotion == chess.QUEEN:\n","                    index = (from_square_index * 73) + (direction * 7)\n","                    legal_moves.append(index)\n","                else:\n","                    promotion_index = self.promotion_indexes[valid_move.promotion]\n","\n","                    if direction > 2 and direction < 6:\n","                        direction = 0 if direction == 4 else (1 if direction == 5 else 2)\n","                    elif direction == 7:\n","                        direction = 2\n","\n","                    index = (from_square_index * 73) + ((direction * 3) + promotion_index + 64)\n","                    legal_moves.append(index)\n","            elif self.board.piece_type_at(valid_move.from_square) == chess.KNIGHT:\n","                direction = self.direction_of_move_for_knights(s_row, s_col, d_row, d_col)\n","\n","                index = (from_square_index * 73) + direction + 56\n","                legal_moves.append(index)\n","\n","            else:\n","                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n","                count_of_square = self.count_of_square_for_movement(s_row, s_col, d_row, d_col) - 1\n","\n","                index = (from_square_index * 73) + ((direction * 7) + count_of_square)\n","                legal_moves.append(index)\n","\n","        return legal_moves\n","\n","    def index_of_square(self, square: chess.Square):\n","        row = chess.square_rank(square)\n","        col = chess.square_file(square)\n","        index = (row * 8) + col\n","\n","        return row, col, index\n","\n","    def direction_of_move_for_ray_directions(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        if delta_x == 0:\n","            return 0 if delta_y > 0 else 4\n","\n","        if delta_y == 0:\n","            return 2 if delta_x > 0 else 6\n","\n","        if delta_x < 0:\n","            return 7 if delta_y > 0 else 5\n","\n","        return 1 if delta_y > 0 else 3\n","\n","    def direction_of_move_for_knights(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        if delta_x == 1:\n","            return 0 if delta_y > 0 else 3\n","\n","        if delta_x == 2:\n","            return 1 if delta_y > 0 else 2\n","\n","        if delta_x == -1:\n","            return 7 if delta_y > 0 else 4\n","\n","        return 6 if delta_y > 0 else 5\n","\n","    def count_of_square_for_movement(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        return max(abs(delta_x), abs(delta_y))\n","\n","    def get_winner(self):\n","        result = self.board.result()\n","\n","        if result == \"1-0\":\n","            return chess.WHITE\n","\n","        if result == \"0-1\":\n","            return chess.BLACK\n","\n","        return 2\n","\n","    def is_terminal(self):\n","        return self.board.is_game_over()\n","\n","    def clone(self):\n","        cloned_state = GameState()\n","        cloned_state.board = self.board.copy()\n","\n","        return cloned_state\n","\n","class Node:\n","    def __init__(self, state, parent=None, prior_prob=1.0):\n","        self.state = state\n","        self.parent = parent\n","        self.children = {}\n","        self.visits = 0\n","        self.value_sum = 0\n","        self.prior_prob = prior_prob\n","        self.is_expanded = False\n","\n","    @property\n","    def value(self):\n","        return self.value_sum / (self.visits + 1e-5)\n","\n","    def expand(self, action_probs):\n","        if not self.children:\n","            for action, prob in enumerate(action_probs):\n","                if prob > 0 and action not in self.children:\n","                    next_state = self.state.clone()\n","                    next_state.get_next_state(action)\n","                    self.children[action] = Node(next_state, parent=self, prior_prob=prob)\n","\n","            self.is_expanded = len(self.children) > 0\n","\n","    def select(self, c_puct=1.0):\n","      if not self.children:  # If no children exist\n","          return None, None\n","\n","      max_ucb = -float('inf')\n","      best_action = None\n","      best_child = None\n","\n","      for action, child in self.children.items():\n","          ucb = child.value + c_puct * child.prior_prob * (math.sqrt(self.visits) / (1 + child.visits))\n","          if ucb > max_ucb:\n","              max_ucb = ucb\n","              best_action = action\n","              best_child = child\n","\n","      return best_action, best_child\n","\n","    def backup(self, value):\n","        self.visits += 1\n","        self.value_sum += value\n","        if self.parent:\n","            self.parent.backup(-value)\n","\n","class MCTS:\n","    def __init__(self, model, c_puct=1.0, simulations=300):\n","        self.model = model\n","        self.c_puct = c_puct\n","        self.simulations = simulations\n","\n","    def add_dirichlet_noise(self, node, valid_moves):\n","        noise = np.random.dirichlet([0.3] * len(valid_moves))\n","        for idx, action in enumerate(valid_moves):\n","            if action in node.children:\n","                node.children[action].prior_prob = \\\n","                    0.75 * node.children[action].prior_prob + 0.25 * noise[idx]\n","\n","    def run(self, initial_state, temperature=1.0):\n","        root = Node(initial_state)\n","\n","        action_probs, value = self.evaluate(initial_state)\n","        valid_moves = initial_state.get_valid_moves()\n","\n","        noise = np.random.dirichlet([0.3] * len(valid_moves))\n","\n","        for idx, action in enumerate(valid_moves):\n","            prob = action_probs[action]\n","            noisy_prob = 0.75 * prob + 0.25 * noise[idx]\n","            next_state = initial_state.clone()\n","            next_state.get_next_state(action)\n","            root.children[action] = Node(next_state, parent=root, prior_prob=noisy_prob)\n","\n","        for _ in range(self.simulations):\n","            node = root\n","\n","            while node.children and not node.state.is_terminal():\n","                action, child_node = node.select(self.c_puct)\n","\n","                if child_node is None:\n","                    break\n","\n","                node = child_node\n","\n","            if not node.state.is_terminal():\n","                action_probs, value = self.evaluate(node.state)\n","                valid_moves = node.state.get_valid_moves()\n","                node.expand(action_probs)\n","            else:\n","                value = node.state.get_winner()\n","                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","\n","            node.backup(value)\n","\n","        return self.get_action_probs(root, temperature)\n","\n","    def evaluate(self, state):\n","        state_tensor = state.get_current_state()\n","        # state_tensor = np.expand_dims(state_tensor, axis=0)\n","\n","        policy, value = self.model.predict(state_tensor, verbose=0)\n","\n","        # Mask invalid moves\n","        valid_moves = state.get_valid_moves()\n","        mask = np.zeros(policy.shape[1])\n","        mask[valid_moves] = 1\n","\n","        policy = policy[0] * mask\n","\n","        # Normalize\n","        sum_policy = np.sum(policy)\n","        if sum_policy > 0:\n","            policy /= sum_policy\n","        else:\n","            # If all moves were masked, use uniform distribution over valid moves\n","            policy = mask / np.sum(mask)\n","\n","        print(value[0][0])\n","        return policy, value[0][0]\n","\n","    def get_action_probs(self, root, temperature=1.0):\n","        visits = np.array([child.visits for action, child in root.children.items()])\n","        actions = list(root.children.keys())\n","\n","        if temperature == 0:  # Pure exploitation\n","            action_idx = np.argmax(visits)\n","            probs = np.zeros_like(visits)\n","            probs[action_idx] = 1\n","        else:\n","            # Apply temperature\n","            visits = visits ** (1 / temperature)\n","            probs = visits / np.sum(visits)\n","\n","        # Convert to full move probability vector\n","        full_probs = np.zeros(4672)  # Adjust size based on your action space\n","        for action, prob in zip(actions, probs):\n","            full_probs[action] = prob\n","\n","        return full_probs\n","\n","class ReplayBuffer:\n","    def __init__(self, maxlen=500000):\n","        self.buffer = deque(maxlen=maxlen)\n","        self.current_size = 0\n","        self.lock = threading.Lock()\n","\n","    def store(self, state, policy, value):\n","        \"\"\"Store a single game state transition\"\"\"\n","        self.buffer.append({\n","            'state': state,\n","            'policy': policy,\n","            'value': value\n","        })\n","        self.current_size = len(self.buffer)\n","\n","    def store_multiple_data(self, states, policies, value):\n","        with self.lock:\n","            for s, p, v in zip(states, policies, [value]):\n","                self.store(s, p, v)\n","\n","    def sample(self, batch_size):\n","        \"\"\"Sample a batch with augmentations\"\"\"\n","        if self.current_size < batch_size:\n","            batch_size = self.current_size\n","\n","        indices = np.random.choice(self.current_size, batch_size)\n","        states, policies, values = [], [], []\n","\n","        for idx in indices:\n","            sample = self.buffer[idx]\n","            # Get augmented samples\n","            aug_states, aug_policies = self._augment_sample(\n","                sample['state'],\n","                sample['policy']\n","            )\n","\n","            # Add all augmentations\n","            states.extend(aug_states)\n","            policies.extend(aug_policies)\n","            values.extend([sample['value']] * len(aug_states))\n","\n","        return np.array(states), np.array(policies), np.array(values)\n","\n","    def _augment_sample(self, state, policy):\n","        \"\"\"Generate valid augmentations for a single sample\"\"\"\n","        # Remove batch dimension if present\n","        if len(state.shape) == 4:\n","            state = np.squeeze(state, axis=0)\n","\n","        augmented_states = [state]\n","        augmented_policies = [policy]\n","\n","        # Horizontal flip\n","        flip_h = np.flip(state, axis=1)\n","        augmented_states.append(flip_h)\n","        augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        # Vertical flip\n","        flip_v = np.flip(state, axis=0)\n","        augmented_states.append(flip_v)\n","        augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        # Diagonal flip (only if shape allows)\n","        if state.shape[0] == state.shape[1]:\n","            diag = np.transpose(state, (1, 0, 2))\n","            augmented_states.append(diag)\n","            augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        return augmented_states, augmented_policies\n","\n","    def __len__(self):\n","        return self.current_size\n","\n","# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","# tf.tpu.experimental.initialize_tpu_system(tpu)\n","# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n","\n","print(\"All devices: \", tf.config.list_logical_devices('GPU'))\n","\n","policy = tf.keras.mixed_precision.Policy('mixed_float16')\n","tf.keras.mixed_precision.set_global_policy(policy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEEbn7CxLfwL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vsoo7SmaO1tk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f0aa881-2ae2-423e-8c02-9e80a331150e"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Iteration 1/7000\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c396011fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["False r1bQ1knr/ppp2ppp/8/4p3/4P3/N4N2/PPP2PPP/R1B1KB1R\n","True r3k1nr/pppb1ppp/2n5/2b1p2q/8/PP5P/2PPB3/RNBK3q\n","True rn2k2r/pp2bppp/6b1/1P2p3/2ppn3/P1P1P3/1R1PKqPP/2BQ1BNR\n","True r3kb1r/1p3ppp/p7/3pp3/PP1n2n1/6Pb/2qK3P/RNB1Q2R\n","True r4rk1/1p3pbp/2n2np1/p1p5/P1pp4/1P4Pb/RBP2P1P/5qKR\n","True r4b1r/pp1k1ppp/2n1b3/2p2q2/1P2K1n1/P2PP1P1/2P2P1P/R1B2BR1\n","False 3k1bnr/3Q1p2/2B1p2p/p4bp1/8/2N5/PPPB1PPP/R3K1NR\n","True rnb1kb1r/ppp1pppp/8/8/P5n1/6P1/1PPP1p1P/RNBKqBNR\n","True rnb1k1nr/pppp1ppp/8/4p3/8/2P2Pq1/PP1PP3/R1BQKBN1\n","False rn1k1Qnr/1b1p3p/1p1P1Np1/p4p2/2p1P3/2B2N2/PPP1BPPP/R3R1K1\n","False r1bqkQ2/pppp1p2/7B/4n3/3PP3/8/PPP2PPP/RN2KBNR\n","True 4rbnr/pp1k1ppp/8/3p4/1p3PbP/P3P1P1/2n1qK2/RNB3R1\n","False 2r2k1Q/5p1p/1pp3p1/p1P3B1/8/2N2N1P/PPP2PBP/R3K2R\n","False r2k1bnr/1b1Qp3/p1p3B1/1pP1N2p/1P6/2N5/1PP2PPP/R1B1R1K1\n","False 1n1Q3R/1b1k4/2pp4/8/3PPN2/4B3/PPP2PPK/4R2R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94s/step - loss: 12.3022 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3201 - value_output_loss: 2.9821 - value_output_mse: 2.9823\n","Epoch 1: loss improved from inf to 12.30221, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 105s/step - loss: 12.3022 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3201 - value_output_loss: 2.9821 - value_output_mse: 2.9823\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 12.2009 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3204 - value_output_loss: 2.8805 - value_output_mse: 2.8796\n","Epoch 1: loss improved from 12.30221 to 12.20093, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 12.2009 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3204 - value_output_loss: 2.8805 - value_output_mse: 2.8796   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 12.0395 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1547 - value_output_loss: 2.8848 - value_output_mse: 2.8843\n","Epoch 1: loss improved from 12.20093 to 12.03947, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 12.0395 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1547 - value_output_loss: 2.8848 - value_output_mse: 2.8843   \n","\n","Average Policy Output Loss: 9.265071233113607\n","Average Value Output Loss: 2.9158002535502114\n","Iteration 2/7000\n","False rnbqkbr1/1pppnBpp/8/p3N3/4P3/8/PPPP1PPP/RNBQK2R\n","False 1nb4k/1pppn2Q/6B1/8/p3P3/b7/PPPP1PPP/RNB1K2R\n","True 3rk2r/1pp1nppp/2n5/p2b4/P2P4/bPp3PP/2Pq1P1R/1Q2KB2\n","False 3R1k2/3p2rp/p6B/2PB1pp1/8/5N2/P1P1QPPP/4K2R\n","False Bnk1Q3/2p2p2/3p2pB/p1P5/7p/2N5/PPP2PPP/R3K1NR\n","False rn1kQ3/p6p/P1p2Np1/3p4/3P1BP1/1PNB3P/1P3P2/R3K2R\n","True r1b1kb1r/ppp2ppp/2n2n2/3qp3/2K4P/1P3NP1/P4P2/RNBn1B1R\n","True r2qk1nr/ppp2pp1/2nbb3/P3p3/4P2p/2P5/5PPP/3q2K1\n","True r1b1k2r/1p1n1pp1/2p4p/2bpp3/2P5/P1P2P2/3BPq2/R2Q1KNn\n","False 4R3/r1pp1ppp/p3k2n/4P3/4P3/P1NB1N2/P1P2PPP/2BQK2R\n","False 1rbq1b1r/1pp1nP2/n3kQp1/pN2N1Bp/3PP3/8/PP3PPP/R3KB1R\n","False rn4Q1/p2pk1pp/b1p1p3/1pP1NpB1/4P3/2PB4/PPN2PPP/R4RK1\n","False 1nbk3r/p1NpBp1p/1ppQ2pn/4P3/4P3/2P5/PP3PPP/R3KBNR\n","True rn1qkbnr/ppp2ppp/8/3p4/P1P5/4P2b/RP1P1P1P/1NBQKq2\n","False r3kbnr/p2B4/3pP3/2p3Bp/3P4/2N2N2/PPP2PPP/R2QK2R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 12.0526 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0686 - value_output_loss: 2.9841 - value_output_mse: 2.9844\n","Epoch 1: loss improved from inf to 12.05264, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 12.0526 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0686 - value_output_loss: 2.9841 - value_output_mse: 2.9844   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 12.1110 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1858 - value_output_loss: 2.9252 - value_output_mse: 2.9265\n","Epoch 1: loss did not improve from 12.05264\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 12.1110 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1858 - value_output_loss: 2.9252 - value_output_mse: 2.9265\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 12.2637 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1988 - value_output_loss: 3.0649 - value_output_mse: 3.0651\n","Epoch 1: loss did not improve from 12.05264\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 12.2637 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1988 - value_output_loss: 3.0649 - value_output_mse: 3.0651\n","\n","Average Policy Output Loss: 9.151068369547525\n","Average Value Output Loss: 2.9913904666900635\n","Iteration 3/7000\n","False rnbqkb1r/ppppnBpp/8/4N3/4P3/8/PPPP1PPP/RNBQK2R\n","False r1bq1kr1/2ppnQpp/1pn5/1N2NP2/p2P4/8/PPP2PPP/R1B1RBK1\n","False rnb1k2r/2p1Q2p/p2p1PP1/1p6/8/3B1N2/PPPB1PPP/RN2K2R\n","False rn1q1br1/p3kPpp/1pp1Q3/3p4/3N1B2/2N5/PPP2PPP/R3R1K1\n","False r5rb/1bNp1pkp/p6Q/n1P1p1P1/B3PB2/8/PPP1NPP1/1R2K2R\n","True r2qk2r/1pp2ppp/2n3P1/5n2/P7/1bpppP1N/1b5P/1NK2B1R\n","False r3kbnr/pb1Qpp1p/1p4p1/1Bp5/4P2P/5N2/PPP2PP1/RNB1K2R\n","True r3k1nr/ppp2pp1/4b3/2b4p/1n1p2P1/2K2P1P/PPPP4/RNB1qBNR\n","True rnb1k1nr/ppp2ppp/8/3p4/5P1q/8/PPPPP3/RNBQKBN1\n","True rn2kbnr/pp3ppp/4b3/2ppp3/8/PPP2PqP/3PP3/RNBQKBNR\n","False r1bk4/p2pQ3/n2Bp1pp/1Bp5/4P3/2N2N1P/PPP2PP1/R3K2R\n","True r2qk2r/ppp2ppp/3b4/8/1Pp1n3/P3Pb1P/2nP1K2/RNB2B1q\n","False 1Q1k1br1/4p2p/3p1p2/NB6/4P1p1/2N1B3/PPP2PPP/R3K2R\n","False 1Q1k3r/p7/P2p2p1/1B1N1p2/2pP3p/2P5/1P1N1PPP/R1B1K2R\n","False r2qk3/4bQ2/3p4/pN2N2p/4P3/3BB3/PPP2PPP/R3K2R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 12.3577 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.4161 - value_output_loss: 2.9416 - value_output_mse: 2.9424\n","Epoch 1: loss improved from inf to 12.35772, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - loss: 12.3577 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.4161 - value_output_loss: 2.9416 - value_output_mse: 2.9424 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 12.2855 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2393 - value_output_loss: 3.0462 - value_output_mse: 3.0472\n","Epoch 1: loss improved from 12.35772 to 12.28550, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 12.2855 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2393 - value_output_loss: 3.0462 - value_output_mse: 3.0472   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - loss: 12.3631 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2981 - value_output_loss: 3.0650 - value_output_mse: 3.0654\n","Epoch 1: loss did not improve from 12.28550\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 12.3631 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2981 - value_output_loss: 3.0650 - value_output_mse: 3.0654\n","\n","Average Policy Output Loss: 9.317826271057129\n","Average Value Output Loss: 3.0176193714141846\n","Iteration 4/7000\n","False rnbqkbr1/p2ppBPp/8/7Q/8/2N5/PPPP1PPP/R1B1K1NR\n","False 2Q2kr1/p2p1pp1/8/3Np2p/3P4/P7/P1P2KPP/R1B2BNR\n","False 4Q1k1/p1p3pp/8/1p6/8/8/PPPP1PPP/RNB1KBNR\n","False 1r2kQ2/pp2p3/4B2B/P2P4/3p4/2N3P1/1PP2PP1/R3K2R\n","True 4kbnr/1pp2ppp/p1n1b3/4p3/2P2P2/P3P1PN/3r1K1P/3q3R\n","True 1r1q1r1k/p1p2ppp/Pp4b1/3p4/3P4/bPn1P2P/4BnP1/RK6\n","False rn4nr/pNp3p1/3k1p1p/1B1P4/8/P1N5/P1P2PPP/R1BQR1K1\n","True 3rk2r/ppp2ppp/2n5/1n2p3/1P6/K1PP1P2/P1qN4/R7\n","False 3k1b1r/1rpQ1p2/pp2p1p1/4N2p/3P4/2N5/PPP2PPP/R1B1K1R1\n","True rn1qk2r/ppp2pp1/8/4p3/2p5/PP1P1N1b/R3PbKP/1N1n1B1R\n","True rnb1k1nr/ppp2ppp/8/3pp3/8/1P3Pq1/P1PPP3/RNBQKBN1\n","True 2kr3r/1p2bpp1/1qn2n1p/p3p3/P1b1P1P1/5PRP/2P3B1/2r1K3\n","True rn3rk1/ppp2ppp/8/4p3/P6P/4BbP1/7R/b3q1K1\n","True r3k2r/1pp2pp1/p2bb2p/3p3n/P2n4/R4Pp1/1PP1P1R1/1Nq1KB2\n","True r3k2r/pp3ppp/3b4/2p2b2/2Pn4/P3p3/1P3qPP/R4KNR\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 12.0247 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0426 - value_output_loss: 2.9821 - value_output_mse: 2.9813\n","Epoch 1: loss improved from inf to 12.02469, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 12.0247 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0426 - value_output_loss: 2.9821 - value_output_mse: 2.9813 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 11.3939 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8322 - value_output_loss: 2.5617 - value_output_mse: 2.5620\n","Epoch 1: loss improved from 12.02469 to 11.39394, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 11.3939 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8322 - value_output_loss: 2.5617 - value_output_mse: 2.5620   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 10.5600 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8155 - value_output_loss: 1.7445 - value_output_mse: 1.7444\n","Epoch 1: loss improved from 11.39394 to 10.56003, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 10.5600 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8155 - value_output_loss: 1.7445 - value_output_mse: 1.7444   \n","\n","Average Policy Output Loss: 8.89680004119873\n","Average Value Output Loss: 2.429417689641317\n","Iteration 5/7000\n","False 1nbk2r1/r1p1Q3/4p1p1/p5Bp/3P4/1P1B4/1PP2PPP/RN2K2R\n","False r1b5/P2p3p/1p2p2n/2k5/1n2PB2/P1Q2P2/1PP3PP/R3KBNR\n","True rnb1k2r/ppp2ppp/8/4p3/P3PbPq/3p3P/1PP5/R1BQKB2\n","True r3kr2/p1pb3p/p3ppnp/4b3/6P1/1p1PKP2/P7/4q3\n","False r3R2r/2Q2P2/1kn5/p1p4p/2B2B2/5N2/PPP2PPP/2R3K1\n","True rnb1k1nr/ppp2pp1/6p1/8/1b1P1p2/5P1P/3KP3/1q3B1R\n","True 1r2r1k1/ppp1n1pp/4bp2/1P2b3/P2p2PP/Kq1PnB2/8/RN6\n","False r1b2knr/2Bp1Q2/1pn4p/p1P1Np2/4P3/8/PPP2PPP/RN2KB1R\n","False rn3b1r/1b1pPp2/pP5p/6p1/1BNk4/2PB1Q2/PP3PPP/R4R1K\n","False rnbq3r/ppp1bP1p/2k3p1/1Q6/3P1B2/8/PPP2PPP/RN2KBNR\n","True rnb1kbnr/pppp1ppp/8/4p3/5P2/6qP/PPPPP3/RNBQKBNR\n","True rn2k2r/1p2bppp/p3bn2/2p5/2q3P1/2K3p1/1P1PP2P/2B1QB2\n","False 1B3b2/3p1n1B/1p1P1pN1/p1pPp1p1/7k/8/PPP2PPP/R2QR1K1\n","False 2Q2kn1/3p1pp1/2pP3r/p1P1p1Bp/8/2NB1N2/1PP2PPP/R2QK2R\n","False 1Q6/kp6/2r1p3/pBPpB3/8/2N5/PPP1NPPP/R2QK2R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 11.7927 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2374 - value_output_loss: 2.5553 - value_output_mse: 2.5552\n","Epoch 1: loss improved from inf to 11.79270, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step - loss: 11.7927 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2374 - value_output_loss: 2.5553 - value_output_mse: 2.5552 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 10.3441 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0321 - value_output_loss: 1.3120 - value_output_mse: 1.3122\n","Epoch 1: loss improved from 11.79270 to 10.34415, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 10.3441 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0321 - value_output_loss: 1.3120 - value_output_mse: 1.3122   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 9.3838 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.9291 - value_output_loss: 0.4547 - value_output_mse: 0.4547\n","Epoch 1: loss improved from 10.34415 to 9.38380, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 9.3838 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.9291 - value_output_loss: 0.4547 - value_output_mse: 0.4547   \n","\n","Average Policy Output Loss: 9.066213289896647\n","Average Value Output Loss: 1.4406685133775075\n","\n","[After 5 Iterations]\n","Average Policy Output Loss: 9.139395840962727\n","Average Value Output Loss: 2.5589792589346567\n","Iteration 6/7000\n","False 1nb2knr/1ppp1Q1p/r3p3/p1P2PN1/8/2NB4/PPP2PPP/R3K2R\n","False r4b1r/pbBBk2p/3Ppp1n/4N3/4P3/2N5/PPPQ1PPP/R3K2R\n","True r1b1k1nr/ppp2ppp/P1q5/3K4/2P4P/1P1pP3/1bnP1PP1/1N3BNR\n","True r3k2r/ppp2ppp/2np4/4p3/P5n1/BP1bb3/2P3PP/1NQ1KqNR\n","False r1bq4/1pp1nP1N/8/k1BBP3/8/1PN2Q2/2P2PPP/R4RK1\n","True r1b1k2r/ppp2ppp/2n2n2/2b1p3/8/B4PP1/4NqKP/R1r2B1R\n","False rnbk1Qnr/ppqp3p/8/2p1p1N1/4P3/8/PPPP1PPP/RNB1KB1R\n","False r2k1b1r/p1pBp1pp/2PpNp2/8/4P3/1P2B3/1PP2PPP/RN1QK2R\n","True r3kbnr/1pp2ppp/2n5/1B2p3/4P1b1/2P2PP1/P2P3q/q3K3\n","True r3kb1r/ppp1ppp1/5n2/nN5p/8/3b1PqP/PP6/R4K2\n","False rnbk1Qnr/ppqpp2p/8/2p4Q/8/8/PPPP1PPP/RNB1KBNR\n","True r1b2rk1/ppp2ppp/7n/4p3/1bp2PP1/8/PPnqP2P/R2K1BNR\n","True r4rk1/ppp2ppp/2n2n2/2bp4/5p2/5PPP/P1P1q3/R5KR\n","False 3k2R1/r2b1p1p/1p1Qp3/p1p1P3/4P3/2N1B2P/PPP2P2/R3KBNR\n","True r1bqk2r/1pp1npp1/p7/3p4/1b1n3P/N2PpP2/PP2P3/R2K1q2\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 8.7429 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.5874 - value_output_loss: 0.1554 - value_output_mse: 0.1554\n","Epoch 1: loss improved from inf to 8.74286, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - loss: 8.7429 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.5874 - value_output_loss: 0.1554 - value_output_mse: 0.1554 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 8.8940 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.5621 - value_output_loss: 0.3319 - value_output_mse: 0.3318\n","Epoch 1: loss did not improve from 8.74286\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 8.8940 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.5621 - value_output_loss: 0.3319 - value_output_mse: 0.3318\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - loss: 8.5561 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.4695 - value_output_loss: 0.0866 - value_output_mse: 0.0866\n","Epoch 1: loss improved from 8.74286 to 8.55610, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 8.5561 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.4695 - value_output_loss: 0.0866 - value_output_mse: 0.0866   \n","\n","Average Policy Output Loss: 8.539674441019693\n","Average Value Output Loss: 0.1913025826215744\n","Iteration 7/7000\n","False rnbqkbnr/pp1pp2p/2p5/5pBQ/3PP3/8/PPP2PPP/RN2KBNR\n","False rnbB1b1r/pppp3p/3kpP2/2Q5/3P4/8/PPP2PPP/RN2KBNR\n","False rnbqk1nr/pp1ppQ1p/7b/2p3N1/4P2P/8/PPPP1PP1/RNB1KB1R\n","True r3k1r1/1pp2ppp/p1nb1n2/8/1P4bN/P1pP2P1/R1P1qP1P/2B1K2R\n","False r1bqkbnr/p1ppp2p/5p2/1BP3pQ/4P3/2N5/PPP2PPP/R1B1K1NR\n","False r1B1Qbr1/p1p2kpp/5p1n/1p6/3P4/2N2NP1/PPP2P1P/R1B1R1K1\n","True r3k2r/pppb1ppp/2n2n2/3pb3/2P5/1P3PPP/Pq2P3/2KQ1BNR\n","False r4k1r/1pBP1Qp1/3N1p2/P6p/3P3P/2N5/PP2BPP1/R3K2R\n","False rnbq1bnN/pp1p1k1p/8/2p1Q3/8/8/PPPP1PPP/RNB1KBNR\n","False 1Q3b1r/R2kp1p1/2p5/3p1P1p/3P2n1/2NB1N2/1PP2PPP/2BQ1RK1\n","True rn2k2r/pp3ppp/4p3/2qp1b2/Pb3P2/3K4/1p2PnPP/R1BQ1BNR\n","True r3k2r/ppp1nppp/4b3/2b1K3/1n1q4/4p1PP/PPPP4/RNBQ1B1R\n","True rn2k1nr/pp2b1pp/5p2/2p5/1P3B2/P1p1P1Pb/2P2P2/2R1Kq2\n","True 3qk2r/1pp2ppp/5n2/1P1b1n2/5P2/2ppb2P/r3PK2/1N2BB1R\n","True r4k1r/pp2bppp/P1p2n2/5b2/1Pp2p2/3n1P2/3P1q2/5K2\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 8.5353 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.4691 - value_output_loss: 0.0662 - value_output_mse: 0.0662\n","Epoch 1: loss improved from inf to 8.53528, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13s/step - loss: 8.5353 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.4691 - value_output_loss: 0.0662 - value_output_mse: 0.0662 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 8.6036 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.5543 - value_output_loss: 0.0493 - value_output_mse: 0.0493\n","Epoch 1: loss did not improve from 8.53528\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - loss: 8.6036 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.5543 - value_output_loss: 0.0493 - value_output_mse: 0.0493\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - loss: 8.3343 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2665 - value_output_loss: 0.0678 - value_output_mse: 0.0678\n","Epoch 1: loss improved from 8.53528 to 8.33429, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 8.3343 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2665 - value_output_loss: 0.0678 - value_output_mse: 0.0678   \n","\n","Average Policy Output Loss: 8.429956436157227\n","Average Value Output Loss: 0.06108467032512029\n","Iteration 8/7000\n","True rn2kbnr/ppp2ppp/4b3/3pp3/1P6/5PqP/P1PPP3/RNBQKB1R\n","True r1b1kb1r/ppp1pppp/2n5/7n/P4PP1/4P3/1P3K1P/R1Bq1qNR\n","False r1b1kQ2/3p1p1p/2n3pB/1p1N4/pP2P3/5N2/PP2BPPP/R4RK1\n","False 1Q3br1/4pkpp/8/pp3Qp1/2pP4/2NB4/PPPB1PPP/R3R2K\n","True r1b1k2r/1p3pp1/2pqnn2/3p4/P5p1/1R1PbpKP/3p4/1N1Q1B1R\n","True r4r1k/pppn1ppp/8/4p1q1/bPP4b/2P2P2/PB1nB2P/R5KR\n","True r2q1rk1/1ppb1pp1/2n2n2/4p2p/3PP1P1/2p2P1P/pbP4R/1K1Q1BN1\n","False 1Q1k3r/pp1pNp1p/7n/1Bp1P1B1/4P3/5N2/PPP2PPP/R3K2R\n","True rnb1k2r/ppp2ppp/7n/4p3/2pb1q2/1P4K1/P1P1P1PP/R4BNR\n","False r1b1k2r/1p1pQ2p/8/p1B1P3/1n2P3/2N5/PPP2PPP/R3KBNR\n","False rnbr4/4Q2p/ppp1kp2/4pN2/2N1P3/3PB3/PPPK1PPP/1R3B1R\n","False 2R1kbnr/2pbpp2/8/1B1p2Bp/3PP3/2N2N2/1PP2PPP/3QK2R\n","True r1b1k2r/ppp2ppp/5n2/3p4/PbPP1p2/1P1PqP2/5KPP/1N1Q1BNR\n","True 4r1k1/pp2nrpp/2p5/1N2p1p1/2bpn2P/6P1/3bKPR1/q7\n","True 2kr1b1r/ppp2ppp/2n1p3/5N2/2pPnP2/P2bP2N/1P2K2P/R1B4q\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 7.8205 - policy_output_accuracy: 0.0667 - policy_output_loss: 7.7888 - value_output_loss: 0.0317 - value_output_mse: 0.0317\n","Epoch 1: loss improved from inf to 7.82050, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - loss: 7.8205 - policy_output_accuracy: 0.0667 - policy_output_loss: 7.7888 - value_output_loss: 0.0317 - value_output_mse: 0.0317 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 7.8968 - policy_output_accuracy: 0.1500 - policy_output_loss: 7.8705 - value_output_loss: 0.0263 - value_output_mse: 0.0263\n","Epoch 1: loss did not improve from 7.82050\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 7.8968 - policy_output_accuracy: 0.1500 - policy_output_loss: 7.8705 - value_output_loss: 0.0263 - value_output_mse: 0.0263\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - loss: 7.3009 - policy_output_accuracy: 0.2000 - policy_output_loss: 7.2784 - value_output_loss: 0.0225 - value_output_mse: 0.0225\n","Epoch 1: loss improved from 7.82050 to 7.30090, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 7.3009 - policy_output_accuracy: 0.2000 - policy_output_loss: 7.2784 - value_output_loss: 0.0225 - value_output_mse: 0.0225   \n","\n","Average Policy Output Loss: 7.645902792612712\n","Average Value Output Loss: 0.02681611975034078\n","Iteration 9/7000\n","True rnb1kbnr/pppp1ppp/8/8/2P2p2/6qP/PP1PP3/RNBQKBNR\n","False rnbq1bN1/1ppp3p/p2kpp2/2Q5/2BPP3/8/PPP2PP1/RNB1K1NR\n","False rnbqkbnr/1pppp3/5pQp/p7/3PP3/2N5/PPP2PPP/R1B1KBNR\n","True 1rb4r/1p1k1pp1/p1n1p2n/8/2pp4/b4P2/P2KPNPP/2q2BR1\n","True r2qk2r/1pp1npp1/p1n5/3p1b1p/1P1b4/3KpPPP/P1P1P3/1NBQR3\n","True rnb1kbnr/ppp2ppp/8/4p3/1P5P/P5P1/2P1PPB1/R1qq1KNR\n","False 8/k2Np1b1/2Q3pn/pN3p1p/3P4/P2BB3/1PP2PPP/R3K2R\n","False r1bk4/ppp1Q3/n3p3/3PNpB1/N2P4/P7/1PP2PPP/R3K2R\n","True r3k2r/ppp2ppp/2n2n2/4p2P/P3p3/b6b/1PPPPPqK/1NBQ3R\n","False B4bnr/3np3/pp3p2/6kp/3PQ3/2N5/PPP1NPPP/R1BQK2R\n","False 1rbr1k2/p2p1Q2/2p5/1p4N1/3pP3/3B4/PPPN1PPP/R3K2R\n","True r4k1r/1p3pp1/5n2/2p5/1qKnPpbp/3P3R/PP6/R1B2B2\n","False rn1Q1k2/5pr1/p5pB/P1p5/4P3/P1pB3N/4K1PP/1R1N3R\n","False 6nr/kQ1n3p/3N4/p2P2B1/3Pp3/2P5/PP3PPP/R3K1NR\n","True rn2k2r/1pp2ppp/8/1p1n1b1P/P7/2K3pB/1p2PP2/bN1q2NR\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 7.9867 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.9678 - value_output_loss: 0.0189 - value_output_mse: 0.0189\n","Epoch 1: loss improved from inf to 7.98666, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 7.9867 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.9678 - value_output_loss: 0.0189 - value_output_mse: 0.0189   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 7.9627 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.9454 - value_output_loss: 0.0173 - value_output_mse: 0.0173\n","Epoch 1: loss improved from 7.98666 to 7.96274, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 7.9627 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.9454 - value_output_loss: 0.0173 - value_output_mse: 0.0173   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 7.5542 - policy_output_accuracy: 0.0167 - policy_output_loss: 7.5401 - value_output_loss: 0.0141 - value_output_mse: 0.0141\n","Epoch 1: loss improved from 7.96274 to 7.55419, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 7.5542 - policy_output_accuracy: 0.0167 - policy_output_loss: 7.5401 - value_output_loss: 0.0141 - value_output_mse: 0.0141   \n","\n","Average Policy Output Loss: 7.817765871683757\n","Average Value Output Loss: 0.016763079290588696\n","Iteration 10/7000\n","True r1bqk2r/pp3ppp/2n1p3/3p4/3Pn3/5P2/P1pbP1PP/R3KBR1\n","True r1b1kb1r/ppp2ppp/q7/K3n3/5pn1/P3P3/1PPP3P/RNB3QR\n","True r1b1k2r/ppp1nppp/2n5/3p4/PP1b2P1/3B3P/2P4R/6qK\n","True rnb1kb1r/ppp2ppp/8/3pp3/2P1nP2/N5qP/PP1PP3/R1BQKBNR\n","True r1b1k1nr/1pp2ppp/p1nb4/3pp3/8/1PP2PqP/P1NPP3/R1BQKBNR\n","True 2b1k2r/1p2pp1p/r5pb/p1p5/PnP1P3/1K1q1P1P/QP4n1/R5N1\n","False rnbq1bn1/3pkBpr/pp1Q4/2p1P1Np/4P3/8/PPP2PPP/RNB1K2R\n","True r3k2r/p1p1npp1/3p1b2/P2b3p/5P2/2PKP3/1n4q1/8\n","False rnbq1bnr/1pp2Q2/3k2pp/1B1P4/p1NP4/2N5/PPP2PPP/R1B2RK1\n","False rnbq1rk1/pppp1p1Q/4p3/6P1/4P3/P1N5/P1PP1PP1/R1B1KBNR\n","False 1nbqkbnr/rpp1p1Pp/p7/3pN2Q/3P4/8/PPP2PPP/RNB1KB1R\n","True r1b1k1nr/pp1p1pp1/8/2p1p3/1qP3p1/K3P2P/P4P2/5BNR\n","True r1b1k2r/pp3ppp/2n5/4p3/Pb3P2/4n1qP/1Pp1PR2/4KBN1\n","False 1k3rnr/p2N1P1p/2Q3pb/1p6/8/2N4P/PPP2P1P/R1B1R1K1\n","False R5k1/8/3pP3/2pP1NNP/2B5/1PP5/1P1B1PP1/3K3R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 7.6735 - policy_output_accuracy: 0.0500 - policy_output_loss: 7.6497 - value_output_loss: 0.0237 - value_output_mse: 0.0237\n","Epoch 1: loss improved from inf to 7.67347, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 7.6735 - policy_output_accuracy: 0.0500 - policy_output_loss: 7.6497 - value_output_loss: 0.0237 - value_output_mse: 0.0237   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 7.5146 - policy_output_accuracy: 0.0167 - policy_output_loss: 7.5028 - value_output_loss: 0.0118 - value_output_mse: 0.0118\n","Epoch 1: loss improved from 7.67347 to 7.51457, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 7.5146 - policy_output_accuracy: 0.0167 - policy_output_loss: 7.5028 - value_output_loss: 0.0118 - value_output_mse: 0.0118   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 8.3551 - policy_output_accuracy: 0.0333 - policy_output_loss: 7.9299 - value_output_loss: 0.4252 - value_output_mse: 0.4255\n","Epoch 1: loss did not improve from 7.51457\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 8.3551 - policy_output_accuracy: 0.0333 - policy_output_loss: 7.9299 - value_output_loss: 0.4252 - value_output_mse: 0.4255\n","\n","Average Policy Output Loss: 7.6941375732421875\n","Average Value Output Loss: 0.15357480539629856\n","\n","[After 10 Iterations]\n","Average Policy Output Loss: 8.582441631952921\n","Average Value Output Loss: 1.3244437552057209\n","Iteration 11/7000\n","True rnb1kb1r/ppp2ppp/8/3pp2n/8/PP3PqP/1BPPP3/RN1QKBNR\n","False rn1q1b1r/p1pp1Q1p/2bk1p2/1p1P2N1/3PPB2/2N5/PP3PPP/R3KB1R\n","False rn6/ppp2npp/5k2/5Q2/3P2P1/N4N2/PPP1Q1PP/R1B1KB1R\n","True r1b1k2r/ppp2ppp/2n5/3p4/6n1/P2PP3/R1P2q1P/1N1K3q\n","False 1nQ4R/r3p3/1p1k1q2/pNpp4/3PP3/8/PPP2PP1/R1BQKBN1\n","True r1b2rk1/ppp2ppp/2n2n2/4p3/1bP2P2/4P1PN/P2q3P/R2K1BR1\n","True r1bqk1nr/ppp2ppp/2n5/2b1p3/8/PP4P1/R1PPNp1P/1NBQKB1R\n","True r1b1k1nr/ppp2ppp/2n5/3pp3/Pb4Pq/5P1P/2PPP3/1NBQKBNR\n","True 2qr2k1/1p2p3/5ppb/3pnb1p/2P4P/2n2KP1/r7/6NR\n","True 1r1k1r2/pp2qpn1/2p1b3/Pq1p4/K3p3/b1Pn3P/6P1/6RR\n","True 3rk2r/pp1n1ppp/2p1p3/3p1n2/4bP2/N1P1b2P/PP1q4/R2K4\n","False 5bnr/2p2Qkp/p2p2p1/4ppB1/2B1P3/8/PPP2PPP/RN1QK1NR\n","False r1q1k1Q1/2ppn3/n5pr/pB1QR3/7p/1P3N1P/P1P2PP1/RNB3K1\n","False r1b4q/2pp2Q1/np1kp2p/pB2Q3/3P4/8/PPP2PPP/RNB1K1NR\n","False rn1k4/p1Q4p/1pb1R3/3p4/P2P4/2NB1N2/1PP2PPB/R2Q2K1\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 7.9283 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.7298 - value_output_loss: 0.1985 - value_output_mse: 0.1985\n","Epoch 1: loss improved from inf to 7.92825, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 7.9283 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.7298 - value_output_loss: 0.1985 - value_output_mse: 0.1985   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 7.2993 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.2904 - value_output_loss: 0.0089 - value_output_mse: 0.0089\n","Epoch 1: loss improved from 7.92825 to 7.29934, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 7.2993 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.2904 - value_output_loss: 0.0089 - value_output_mse: 0.0089   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 7.6305 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.6207 - value_output_loss: 0.0099 - value_output_mse: 0.0099\n","Epoch 1: loss did not improve from 7.29934\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 7.6305 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.6207 - value_output_loss: 0.0099 - value_output_mse: 0.0099\n","\n","Average Policy Output Loss: 7.546960989634196\n","Average Value Output Loss: 0.07242012737939756\n","Iteration 12/7000\n","False rn1Bk2r/pp1bQ2p/2p1p1p1/8/3P4/N7/PPP2PPP/R3KBNR\n","False r4bnr/ppp2k1p/4QBp1/3B4/4p2P/2N5/PPP2PP1/R3K1NR\n","False r2k2r1/3Q1p1p/b7/p3N3/3PP3/N1P5/PP3PPP/R3K2R\n","True r3kb1r/pppnppp1/8/1P2q2p/4b1nP/B1PpPK2/P2P2P1/R6R\n","False rnbqkb1r/1p1pp2p/p4p2/2pPP1pQ/8/8/PPP2PPP/RNB1KBNR\n","True rnb1k2r/ppp2ppp/8/5P1P/8/4qpP1/P1P2n2/b2K3R\n","False B5kr/p1pp1Q1p/6p1/1qP1N3/4n3/2N5/PPP2PPP/R1B2RK1\n","False r1bq1bnr/2p1pQkp/np6/p3N3/2BP4/8/PPP2PP1/RNB1K2R\n","False rn2k1Q1/1b1pBp2/p1pP2p1/1p5p/4P3/2N2N2/PPP2PPP/R3KB1R\n","False r1bq1bnr/p1p1p2p/1pBk4/1N1P1ppQ/3P4/8/PPP2PPP/R1B1K1NR\n","True rnb1k1nr/pppp1ppp/8/4p3/P6q/5P2/2PPPb2/RNBQKBN1\n","False r5nN/p3b1p1/k2PPp2/1Q6/8/3B4/PPP2PPP/RNB1K2R\n","False rn3b2/pp2p3/k3Bpp1/3N4/Q2P1B1p/7P/PPP1NP1P/R3K2R\n","True r4rk1/1pp4p/p1n1b2p/3p1p2/P2P4/K1P1b3/1P1n4/q5N1\n","True r3k3/ppp1npp1/2n5/4p2r/8/1PbqPPp1/P7/2N1K3\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 6.8875 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.8800 - value_output_loss: 0.0075 - value_output_mse: 0.0075\n","Epoch 1: loss improved from inf to 6.88753, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 6.8875 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.8800 - value_output_loss: 0.0075 - value_output_mse: 0.0075   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 7.1653 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.1584 - value_output_loss: 0.0069 - value_output_mse: 0.0069\n","Epoch 1: loss did not improve from 6.88753\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 7.1653 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.1584 - value_output_loss: 0.0069 - value_output_mse: 0.0069\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 6.9184 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.9061 - value_output_loss: 0.0123 - value_output_mse: 0.0123\n","Epoch 1: loss did not improve from 6.88753\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 6.9184 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.9061 - value_output_loss: 0.0123 - value_output_mse: 0.0123\n","\n","Average Policy Output Loss: 6.981496810913086\n","Average Value Output Loss: 0.008915774524211884\n","Iteration 13/7000\n","True rn2kbnr/1pp1pppp/8/6P1/1p2p1b1/B6N/P1PPqP1P/RN2K2R\n","False 1nb4r/2ppNB1p/r7/p7/k1QPP1p1/4N3/PPP2PPP/R3K2R\n","True r3kbnr/pppn1ppp/8/3pp3/P1P2P1q/3P4/RP1NP2P/3QKB1b\n","True rn2k2r/ppp1bp1p/4pn2/6p1/1PP4K/4q3/P6P/RN3B1b\n","True r3k2r/ppp2ppp/8/2P1p2P/Pn6/1P1Pb3/2K1b1P1/qN3n2\n","False rnbqkbn1/ppppp3/6QB/5p2/3PP3/8/PPP2PPP/RN2KBNR\n","True r3k1r1/ppp2ppp/n4n2/8/1bp4P/4PP1b/P2qK3/R6R\n","False r1b2bnr/ppp5/1k2PpB1/1Q6/8/2N5/PPPP1PPP/R1B1K1NR\n","False r2q1bnr/5kpp/2P1Q3/p1pp1P2/2B5/2N2N2/PPP2PPP/R1B2RK1\n","True rnb1kbnr/ppp2ppp/8/3pp3/6Pq/1P3P2/P1PPP2P/RNBQKBNR\n","False 2bk1b1r/1RPn3p/7n/6p1/p1B5/P1N2N2/P1P2PP1/2BQR1K1\n","False 1Q3k2/p5p1/1p4B1/1PP3B1/8/2N2r2/1PP2PPP/R4R1K\n","False Q2k1bnr/2p1p2p/5pp1/8/Q2P4/P4N2/1P3PPP/RNB1KB1R\n","True r2qkb2/1pp2pp1/2n5/4p3/pP2bp2/8/2K4P/1RBn4\n","False 1r2nb1r/pk2pN2/1pQ4p/1B4p1/3P4/7P/PPP2PP1/RNB1K2R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 7.3886 - policy_output_accuracy: 0.0500 - policy_output_loss: 7.3811 - value_output_loss: 0.0075 - value_output_mse: 0.0075\n","Epoch 1: loss improved from inf to 7.38857, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 16s/step - loss: 7.3886 - policy_output_accuracy: 0.0500 - policy_output_loss: 7.3811 - value_output_loss: 0.0075 - value_output_mse: 0.0075 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 7.3543 - policy_output_accuracy: 0.1000 - policy_output_loss: 7.3437 - value_output_loss: 0.0107 - value_output_mse: 0.0107\n","Epoch 1: loss improved from 7.38857 to 7.35434, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 7.3543 - policy_output_accuracy: 0.1000 - policy_output_loss: 7.3437 - value_output_loss: 0.0107 - value_output_mse: 0.0107   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - loss: 6.7222 - policy_output_accuracy: 0.0667 - policy_output_loss: 6.7178 - value_output_loss: 0.0043 - value_output_mse: 0.0043\n","Epoch 1: loss improved from 7.35434 to 6.72216, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 6.7222 - policy_output_accuracy: 0.0667 - policy_output_loss: 6.7178 - value_output_loss: 0.0043 - value_output_mse: 0.0043   \n","\n","Average Policy Output Loss: 7.1475396156311035\n","Average Value Output Loss: 0.007485596928745508\n","Iteration 14/7000\n","False rnbqkbnr/ppppp2p/8/5ppQ/3PP3/8/PPP2PPP/RNB1KBNR\n","False 2r1kb1r/p2Qpp2/1p3Ppp/1Bp5/8/8/PPP2PPP/RNB1K1NR\n","True rn2kb1r/ppp2ppp/5n2/3p4/3P2q1/2N4K/PPP1P2P/2RQ1B1R\n","True rnbqk1nr/1p1p1ppp/4p3/8/2pP3P/5PP1/P2bP2R/2qK1BN1\n","False rn1qkbnr/p1ppp2P/b7/1p5Q/8/8/PPPP1PPP/RNB1KBNR\n","False r1bqkb2/1ppppB2/2n5/p3Npp1/4P3/8/PPPP1PPP/RNBQK2R\n","True rn1qk2r/ppp2ppp/4b3/8/PP5P/K1Pn1N2/4p1B1/RNb5\n","True r3k1r1/ppp2ppp/2bbn3/8/3p4/P3P2P/1PP3q1/R1B2nK1\n","True rn2k1r1/pp3p1p/7p/1b1p1p2/1b1p3q/P3PP2/RPP2K1P/1N4NR\n","True rn3b1r/pp1k1ppp/2p5/3pp3/B2q2bn/NP1K4/P1PP2PP/R1B4R\n","True rnb1k1nr/ppp2ppp/8/2b1p3/2p5/P4NPB/RP1PPq1P/1NBQK2R\n","True rn2k1nr/ppp2ppp/8/3p4/1P2P1bP/P5b1/2PP1qP1/1NQ1KB1R\n","False 1Q1k2r1/2p2pp1/2BPp3/7p/3pP3/7N/PP1N1PPP/R1B1K2R\n","True r1b1k2r/ppp2ppp/2n5/3pp3/PbP2Pn1/8/1P1PPq1P/1NBQKB1R\n","False 2b3N1/2nkQ3/1Bppp1p1/6Np/r2P4/3B4/PPP2PPP/1R2R1K1\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 6.9988 - policy_output_accuracy: 0.1000 - policy_output_loss: 6.9948 - value_output_loss: 0.0040 - value_output_mse: 0.0040\n","Epoch 1: loss improved from inf to 6.99883, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 6.9988 - policy_output_accuracy: 0.1000 - policy_output_loss: 6.9948 - value_output_loss: 0.0040 - value_output_mse: 0.0040   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - loss: 6.8303 - policy_output_accuracy: 0.0167 - policy_output_loss: 6.8264 - value_output_loss: 0.0038 - value_output_mse: 0.0038\n","Epoch 1: loss improved from 6.99883 to 6.83026, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 6.8303 - policy_output_accuracy: 0.0167 - policy_output_loss: 6.8264 - value_output_loss: 0.0038 - value_output_mse: 0.0038   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 7.0440 - policy_output_accuracy: 0.0333 - policy_output_loss: 7.0406 - value_output_loss: 0.0034 - value_output_mse: 0.0034\n","Epoch 1: loss did not improve from 6.83026\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 7.0440 - policy_output_accuracy: 0.0333 - policy_output_loss: 7.0406 - value_output_loss: 0.0034 - value_output_mse: 0.0034\n","\n","Average Policy Output Loss: 6.953934828440349\n","Average Value Output Loss: 0.0037626109551638365\n","Iteration 15/7000\n","False r1bq1b2/pppp2p1/n3k1B1/3QPpB1/4P2N/2N5/PPP2PPP/R3K2R\n","True r1bqkbn1/ppp2pp1/B7/3pP3/3nP3/P4P2/1PP1K3/RNBr2q1\n","True r3kbnr/ppp2ppp/2n5/8/P5bP/RPpP2P1/2P1q3/4K1R1\n","True 2k4r/pp2nppp/2p5/4q3/1nP1p1b1/4P3/PP3P1P/R1BrK3\n","False r1bqk2Q/p1pppp1p/2n3p1/1p6/8/2N5/PPPP1PPP/R1BQKBNR\n","True r3kb1r/p1pn1ppp/8/2P1p2P/P3n3/1b6/1P2Pq2/R3K3\n","False rn3b2/k1Q1p3/pp5p/3p4/3P1Q2/8/PPP2PPP/RNB1KBNR\n","True rnb1kbnr/pppp1ppp/8/8/1P2P2q/8/P1PPB1pP/RNBQK1NR\n","False rnq5/pbppp2B/1p3kpB/4PpN1/3P4/2N5/PPP2PPP/R2QK2R\n","True rnb1kbnr/ppp2ppp/8/3pp3/P5Pq/5P2/1PPPP2P/RNBQKBNR\n","False r1bqkbn1/ppppp3/n4pQ1/8/3PP3/8/PPP2PPP/RNB1K1NR\n","True r1b3n1/pp2kpp1/2p4r/2bqp3/7P/5KP1/PP2Bn2/RN4NR\n","False Q2k2nr/p1pq1p2/8/nBB1P3/6Pp/2N2N2/PPP2PP1/2KR4\n","True r3k2r/ppp2ppp/2n2n2/4p3/1bP4P/4PP2/PB1q4/1b1K1BNR\n","True r4rk1/pppn1ppp/8/8/3p1Pnb/PPp2b1P/2PqQ3/1R1K1B2\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 6.6070 - policy_output_accuracy: 0.2500 - policy_output_loss: 6.6038 - value_output_loss: 0.0032 - value_output_mse: 0.0032\n","Epoch 1: loss improved from inf to 6.60704, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 6.6070 - policy_output_accuracy: 0.2500 - policy_output_loss: 6.6038 - value_output_loss: 0.0032 - value_output_mse: 0.0032 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 6.5062 - policy_output_accuracy: 0.2333 - policy_output_loss: 6.5023 - value_output_loss: 0.0038 - value_output_mse: 0.0038\n","Epoch 1: loss improved from 6.60704 to 6.50617, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 6.5062 - policy_output_accuracy: 0.2333 - policy_output_loss: 6.5023 - value_output_loss: 0.0038 - value_output_mse: 0.0038   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 6.2107 - policy_output_accuracy: 0.2167 - policy_output_loss: 6.2078 - value_output_loss: 0.0029 - value_output_mse: 0.0029\n","Epoch 1: loss improved from 6.50617 to 6.21066, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 6.2107 - policy_output_accuracy: 0.2167 - policy_output_loss: 6.2078 - value_output_loss: 0.0029 - value_output_mse: 0.0029   \n","\n","Average Policy Output Loss: 6.4379862149556475\n","Average Value Output Loss: 0.0033036419190466404\n","\n","[After 15 Iterations]\n","Average Policy Output Loss: 8.059488985273573\n","Average Value Output Loss: 0.8893550202509182\n","Iteration 16/7000\n","True rnb1kbnr/ppp2pp1/8/3pp2p/P5Pq/5P2/1PPPP2P/RNBQKBNR\n","False r1bq1b1r/1pnpkBp1/2nQ3p/p1P1N3/N3P3/8/PPP2PPP/R1B1K2R\n","False rn1q1bnN/1b1pk1p1/p7/1Bp1Q3/4P3/2N5/PPP2PPP/R1B1K2R\n","True r5nr/pppk1ppp/2n1b3/8/1b3q2/N2P4/P1p1P1PP/4KBR1\n","False rn1qk1nr/2pp1Qp1/bp5p/p3P3/2B1P3/P1N5/P1P2PPP/R1B1K1NR\n","False rk2Q3/p2p2p1/1p6/1NpB4/3PPp2/8/PPP2PPP/RNB1K2R\n","False 1rb4r/1p1p1Q1p/2n4k/p2p1p1B/3P1B2/N7/PPP2PPP/R4RNK\n","False rn1qkb1r/1pp1pQpp/3pP3/p7/3P4/5N2/PPP2PPP/RNB1KB1R\n","True rnb1k1nr/ppp2ppp/8/3pp3/2P5/bP3PqP/PB1PP3/R2QKBNR\n","False rn5k/pb1p1P2/5QBB/PNP4p/8/2P5/P4PPP/R4RK1\n","False 2Qq1bnr/2ppp1pp/p5k1/1p3Q2/4P1N1/P1N5/1PP2PPP/R1B1KB1R\n","True r1b1k1nr/pp3pp1/2pb2p1/3p4/4p1P1/2PnPP2/PP1PKq1P/RNBQ2NR\n","False 1n1qkbnr/2ppp3/1p4B1/p5p1/3P3p/5N2/PPP2PPP/RNBQ1RK1\n","False 4k1r1/rp3RP1/3pRB2/2p3Np/p1BP4/P1N4P/1PP2PP1/2K5\n","True r3k2r/pp2npp1/7p/3ppK2/b5PP/N1q5/P3PP1R/n4BN1\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 7.0611 - policy_output_accuracy: 0.1500 - policy_output_loss: 7.0371 - value_output_loss: 0.0240 - value_output_mse: 0.0240\n","Epoch 1: loss improved from inf to 7.06110, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 7.0611 - policy_output_accuracy: 0.1500 - policy_output_loss: 7.0371 - value_output_loss: 0.0240 - value_output_mse: 0.0240   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 6.3605 - policy_output_accuracy: 0.2000 - policy_output_loss: 6.3562 - value_output_loss: 0.0043 - value_output_mse: 0.0043\n","Epoch 1: loss improved from 7.06110 to 6.36047, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 6.3605 - policy_output_accuracy: 0.2000 - policy_output_loss: 6.3562 - value_output_loss: 0.0043 - value_output_mse: 0.0043   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 6.3963 - policy_output_accuracy: 0.2000 - policy_output_loss: 6.3930 - value_output_loss: 0.0034 - value_output_mse: 0.0034\n","Epoch 1: loss did not improve from 6.36047\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - loss: 6.3963 - policy_output_accuracy: 0.2000 - policy_output_loss: 6.3930 - value_output_loss: 0.0034 - value_output_mse: 0.0034\n","\n","Average Policy Output Loss: 6.595423698425293\n","Average Value Output Loss: 0.010543126768122116\n","Iteration 17/7000\n","True rnb1kbnr/ppp2ppp/8/3pp3/6Pq/5P2/PPPPP2P/RNBQKBNR\n","False rnb3n1/1ppp3p/p1k1pq2/2Q5/3Q4/N7/PPPP1PPP/R1B1KBNR\n","False rnbqk2r/pp1ppQ2/2p4b/6N1/8/8/PPPP1PPP/RNB1KB1R\n","False 5br1/p6p/6k1/3B2Q1/4P3/5N2/PPP2PPP/RNB1K2R\n","False 1r2kb1r/pbpQp1p1/2P5/1p2N2p/8/2N4P/PPP2PP1/R1B1KB1R\n","False 1nb3nN/2B2Q2/2p1k3/3p4/1p1P4/2PB4/P1PN1PPP/R3K2R\n","True rnbqk2r/ppp2ppp/8/3p4/1bP2P1P/N3n3/PP1pP1P1/R2K1BNR\n","True r3k2r/1pp2ppp/p1n5/3p4/P1P1n1b1/8/5qPP/4KBNR\n","True rn3rk1/ppp2ppp/3b4/P2p1b2/1PnP4/B1P5/2QKP2P/RN3q2\n","False r1q3kr/pppPQ1pp/1b1P4/3B1p2/8/P4N2/1PP2PPP/RNB1K2R\n","False r1bqkb1r/pppppQpp/8/5PN1/3n4/8/PPPP1PPP/RNB1KB1R\n","False r1b2knr/ppBp1Q2/2P5/4Np1p/4P3/4P3/PPP3PP/RN2KB1R\n","True r3kbnr/1pp2ppp/2b1p3/8/P1p5/2Pn4/1P1P1P2/1RB3Kq\n","True r3k2r/1pp1nppp/nq6/8/P3pb2/2P5/4R1b1/Kq6\n","True r4rk1/ppp2ppp/2n5/8/3p4/3bKPq1/PP1P1n2/RNB2B2\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 5.4585 - policy_output_accuracy: 0.0333 - policy_output_loss: 5.4562 - value_output_loss: 0.0023 - value_output_mse: 0.0023\n","Epoch 1: loss improved from inf to 5.45849, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step - loss: 5.4585 - policy_output_accuracy: 0.0333 - policy_output_loss: 5.4562 - value_output_loss: 0.0023 - value_output_mse: 0.0023 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 6.8416 - policy_output_accuracy: 0.0833 - policy_output_loss: 6.8332 - value_output_loss: 0.0085 - value_output_mse: 0.0085\n","Epoch 1: loss did not improve from 5.45849\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 6.8416 - policy_output_accuracy: 0.0833 - policy_output_loss: 6.8332 - value_output_loss: 0.0085 - value_output_mse: 0.0085\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 5.3771 - policy_output_accuracy: 0.0333 - policy_output_loss: 5.3750 - value_output_loss: 0.0021 - value_output_mse: 0.0021\n","Epoch 1: loss improved from 5.45849 to 5.37714, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 5.3771 - policy_output_accuracy: 0.0333 - policy_output_loss: 5.3750 - value_output_loss: 0.0021 - value_output_mse: 0.0021   \n","\n","Average Policy Output Loss: 5.888107935587565\n","Average Value Output Loss: 0.0043074279092252254\n","Iteration 18/7000\n","False r1bqkbnr/p1ppp2p/n4p2/1p4NQ/3PP3/2NB4/PPP2PPP/R1B1K2R\n","False rn1R1k1r/ppp2pp1/8/1Bbp2Bp/3P4/2N2N2/PPP2PPP/R2QK2R\n","True r3k1nr/ppp2ppp/2nb4/1B1pp3/8/PP1PPP2/2b4P/RNB2q1K\n","True rnb1k2r/ppp2ppp/5n2/3p4/5P2/b2P4/PPPK2PP/R1r1qB1R\n","False rn4Q1/p2kr3/1B3ppp/1P1R4/4P3/2N2N2/PP3PPP/4KB1R\n","False 1rbQkbnr/p1P2p2/2p1p2p/1p4P1/4P3/2N2N2/PPP2PP1/R1B1KB1R\n","False rnq4r/p1p1Qkp1/3P1N2/5P1p/8/b2B1N2/PPP2PPP/R1B2RK1\n","False r2k2Q1/p1ppnp1p/B7/1p4B1/4P3/N4N2/PPP2PPP/R2QK2R\n","False k4r2/1R1n4/1ppN4/3p1BBp/3P4/5P2/1PP1N1PP/Q3K2R\n","False r1bk3r/2ppQ2p/p4p2/1pPNp3/4P3/8/PPP2PPP/R3KBNR\n","False rnbqkbnr/ppppp3/5pQp/8/3PP3/8/PPP2PPP/RNB1KBNR\n","False rn2Qk2/1b1p2r1/2pN2pB/pp3PNp/P1BP3P/8/1PPK1PP1/R6R\n","False r2q2nr/p1pp1Qb1/3kp2p/2P1Npp1/1P1PP3/R7/1P3PPP/1NB2RK1\n","True r4rk1/ppp2ppp/2n2n2/4bb2/PP2Rp1P/7N/1q4P1/1K3BR1\n","True r3k2r/p2n1pp1/2p1pn2/1p6/1P5p/PK1P1b1P/1qP1Bp2/R1b5\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 7.5659 - policy_output_accuracy: 0.0333 - policy_output_loss: 7.5045 - value_output_loss: 0.0614 - value_output_mse: 0.0614\n","Epoch 1: loss improved from inf to 7.56586, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 7.5659 - policy_output_accuracy: 0.0333 - policy_output_loss: 7.5045 - value_output_loss: 0.0614 - value_output_mse: 0.0614   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 7.1177 - policy_output_accuracy: 0.1667 - policy_output_loss: 7.1024 - value_output_loss: 0.0153 - value_output_mse: 0.0154\n","Epoch 1: loss improved from 7.56586 to 7.11771, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 7.1177 - policy_output_accuracy: 0.1667 - policy_output_loss: 7.1024 - value_output_loss: 0.0153 - value_output_mse: 0.0154   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 7.0596 - policy_output_accuracy: 0.0667 - policy_output_loss: 7.0538 - value_output_loss: 0.0058 - value_output_mse: 0.0058\n","Epoch 1: loss improved from 7.11771 to 7.05956, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 7.0596 - policy_output_accuracy: 0.0667 - policy_output_loss: 7.0538 - value_output_loss: 0.0058 - value_output_mse: 0.0058   \n","\n","Average Policy Output Loss: 7.220211664835612\n","Average Value Output Loss: 0.027499737838904064\n","Iteration 19/7000\n","True r1b1kbnr/ppp2ppp/2n5/3pp3/5NPq/5P2/PPPPP2P/RNBQKB1R\n","False r1bk3Q/1pppbp2/6p1/p5Bp/8/2N5/PPP2PPP/R3KBNR\n","True r1b1k2r/ppp2ppp/5q2/8/Pb6/1Pnp3N/2nNPPPP/R1QK1B1R\n","False r1bk3r/2Q3pp/pP3n2/1B4N1/4P3/8/PPP2PPP/RNB1K2R\n","True r4rk1/ppp2ppp/5n1P/2b5/b7/2Kqp3/PP1PNnP1/RNB2B1R\n","False 1n3b1r/1p1bpppp/r1kP4/p3NP2/8/2N1B2P/PPP2PP1/R2QK2R\n","True r1b1kb1r/pp2pppp/2n5/2p5/4nP1N/3P2P1/Pq5P/RNrK1B1R\n","False 1rk2b2/3npN2/1RQ1BP1r/p5Bp/3P4/8/1P3PPP/1N3RK1\n","True 1nb1k2r/2p2ppp/5n2/1p6/1b2pp1P/r7/PPP3PR/R1BK1qN1\n","False 1n4nr/ppN1pQkp/8/3B4/3PP3/8/PPP2PPP/R1B1K2R\n","False rn1qkbnr/p3p2p/1p2P3/2p1NppQ/3P4/2N5/PPP2PPP/R1B1KB1R\n","True r1b1r3/1p1k2pp/2n1p2n/3p2P1/1b1P2q1/1P5K/2Pp3P/6NR\n","True r1b1r1k1/pp3ppp/5n2/2P1p3/P2np3/3P4/4KPPP/2q1NB1R\n","True r1b1kbnr/pp3ppp/4p3/8/2pn4/P4PP1/1P5P/2K1q3\n","False 2Q2bB1/p3k3/2p5/4P1Bp/3P4/P7/P1PNNPP1/R3K2R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 5.2828 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 5.2790 - value_output_loss: 0.0038 - value_output_mse: 0.0038\n","Epoch 1: loss improved from inf to 5.28276, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step - loss: 5.2828 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 5.2790 - value_output_loss: 0.0038 - value_output_mse: 0.0038 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 6.2220 - policy_output_accuracy: 0.0333 - policy_output_loss: 6.2172 - value_output_loss: 0.0047 - value_output_mse: 0.0047\n","Epoch 1: loss did not improve from 5.28276\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 6.2220 - policy_output_accuracy: 0.0333 - policy_output_loss: 6.2172 - value_output_loss: 0.0047 - value_output_mse: 0.0047\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 6.2891 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.2864 - value_output_loss: 0.0028 - value_output_mse: 0.0028\n","Epoch 1: loss did not improve from 5.28276\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 6.2891 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.2864 - value_output_loss: 0.0028 - value_output_mse: 0.0028\n","\n","Average Policy Output Loss: 5.927520593007405\n","Average Value Output Loss: 0.00376140302978456\n","Iteration 20/7000\n","False r1bqkbnr/p2np2p/2P5/1B3pBQ/3P4/8/PPP2PPP/RN2K1NR\n","True rnbqk2r/ppp2ppp/8/3pp2P/2P1n3/NP4P1/P2PPb2/R1BQKBNR\n","False 1nbqkb1r/rpp1pB1p/p2p2pB/4N3/3PP3/8/PPP2PPP/RN1Q1RK1\n","True rnbqk2r/ppp2ppp/8/3pp3/1PP3n1/3P1P2/P2BPb1P/RN1QKBNR\n","True rn2kb1r/ppp2pp1/4b3/3pp3/8/1P4pP/PBPP4/RN1K1q1n\n","True r3k2r/pbp3pp/2n2p1b/2Pp2N1/3p2PP/P7/4P3/2q1KB1n\n","False rn4nQ/1p5p/p1p3k1/q2pN3/2B1PB2/N6b/PPP2PPP/R2QR1K1\n","False r1k1Qb1r/pp2p3/2ppN2p/5P2/3P1p2/2N5/PPP2PPP/R1B1K2R\n","False rnb2bnr/p1pp3p/8/1B1kQ1B1/3P4/8/PPP2PPP/RN2K1NR\n","False Q1k4r/p1p3pp/3bP2n/4N3/8/2N5/PPPP1PPP/R1B1K2R\n","False rnQ1kbr1/4pp2/p1p4p/1P4p1/3P1N2/3BBP2/RPP2NPP/3Q1RK1\n","False 1nb4R/2pp3p/2r1kQpb/p2N4/1p2P3/8/PPP2PPP/R1B1KBNR\n","False r2k4/1p1Q4/p4Np1/4Bp1p/2B1P1n1/8/PPP2PPP/R3K1NR\n","True 1rb1k2r/2p2ppp/p3p3/3p4/p2Pn3/8/P1KqPnPP/R4BNR\n","True r3k2r/1pp2ppp/1nn1b2b/pB2p3/4P3/P7/q6P/K5NR\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 5.7535 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 5.7516 - value_output_loss: 0.0019 - value_output_mse: 0.0019\n","Epoch 1: loss improved from inf to 5.75354, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - loss: 5.7535 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 5.7516 - value_output_loss: 0.0019 - value_output_mse: 0.0019 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 5.6265 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 5.6247 - value_output_loss: 0.0019 - value_output_mse: 0.0019\n","Epoch 1: loss improved from 5.75354 to 5.62654, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 5.6265 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 5.6247 - value_output_loss: 0.0019 - value_output_mse: 0.0019   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 5.7998 - policy_output_accuracy: 0.0500 - policy_output_loss: 5.7951 - value_output_loss: 0.0047 - value_output_mse: 0.0047\n","Epoch 1: loss did not improve from 5.62654\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - loss: 5.7998 - policy_output_accuracy: 0.0500 - policy_output_loss: 5.7951 - value_output_loss: 0.0047 - value_output_mse: 0.0047\n","\n","Average Policy Output Loss: 5.723803838094075\n","Average Value Output Loss: 0.002835190466915568\n","\n","[After 20 Iterations]\n","Average Policy Output Loss: 7.612370125452678\n","Average Value Output Loss: 0.6694636094888362\n","Iteration 21/7000\n","True r1b1k2r/ppp2ppp/2n5/1N1pp3/P1P1n3/5Pq1/1P1PP3/R1BQKBN1\n","False r1b2bnr/ppnppk2/6Q1/2P2P2/7p/2N1B3/PPP2PPP/R3KB1R\n","False rkb2br1/2Q2p1p/pp1P1P2/4N1p1/2B5/P1N1B3/1PP2PPP/R3K2R\n","False 3Q1kr1/5pp1/p1B5/2P1p3/4P3/N4N1p/PPPB1PPP/2KR3R\n","False rnb2b1r/2Npk3/1p2pQ2/p1P3Bp/4P3/P2B4/1PP2PPP/2KR3R\n","True rn2kb1r/ppp2ppp/4p3/3p4/P3nPb1/3P4/1PPBPqBP/RN1Q1KNR\n","False 2bqkb2/r1pppQ1Q/np6/8/p2P4/2N5/PPP2PPP/R1B1KBNR\n","True 1n1qk2r/1pp2ppp/2np4/1p6/1bb3P1/2K1PP1B/r1PQ3P/4R3\n","True rn1qk2r/pp3p2/2pb1n1p/1B1p2p1/6bK/1PP1P3/P2P1PPP/RNB3NR\n","True r2r2k1/1Rp2ppp/p1n2n2/2b1p3/2b5/2P1P1PP/P2q1P2/2N1KR2\n","False 1r3k2/p1p2Qrp/4p2B/2P1N3/4N2P/P7/P1P2PP1/R3R1K1\n","False 5bn1/2pQk1pr/1p1pPp2/p6p/4P3/4BN2/PPP2PPP/RN2K2R\n","True rn2k1nr/ppp2ppp/3b4/8/b1Pp1p2/3PP3/PP1K3P/RNBq1B1R\n","False rnbR3r/p1pp2k1/1p3Q2/4P1B1/7p/2NB3P/PPP2PP1/R3K1NR\n","False 1n3Q2/Q2k3p/2p1bP2/3p4/8/3B1N2/1PP2PPP/2B1K2R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 5.2192 - policy_output_accuracy: 0.0167 - policy_output_loss: 5.2157 - value_output_loss: 0.0035 - value_output_mse: 0.0035\n","Epoch 1: loss improved from inf to 5.21916, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 5.2192 - policy_output_accuracy: 0.0167 - policy_output_loss: 5.2157 - value_output_loss: 0.0035 - value_output_mse: 0.0035   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - loss: 5.1987 - policy_output_accuracy: 0.0333 - policy_output_loss: 5.1971 - value_output_loss: 0.0017 - value_output_mse: 0.0017\n","Epoch 1: loss improved from 5.21916 to 5.19872, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 5.1987 - policy_output_accuracy: 0.0333 - policy_output_loss: 5.1971 - value_output_loss: 0.0017 - value_output_mse: 0.0017   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 4.8716 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 4.8698 - value_output_loss: 0.0017 - value_output_mse: 0.0017\n","Epoch 1: loss improved from 5.19872 to 4.87156, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 4.8716 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 4.8698 - value_output_loss: 0.0017 - value_output_mse: 0.0017   \n","\n","Average Policy Output Loss: 5.094175020853679\n","Average Value Output Loss: 0.002301948688303431\n","Iteration 22/7000\n","False r1b1k2r/pp2Qp2/4pP1b/1qP3pp/8/2N1B3/PPP2PPP/R3K1NR\n","False rn5r/pp1N1kpp/3bQ2R/1B1p1p2/3P2P1/2N5/PPP2PP1/R1B1K3\n","True rnb1kbnr/ppp2ppp/8/3pp3/7q/2N2P2/PPPPP2P/R1BQKBNR\n","False 1rbq3r/2pp1p1p/p3k3/4Q3/8/5N2/PP2BPPP/RN2K2R\n","False rnb2r2/1p1kQ2p/2p3p1/p2p1PB1/3P4/8/PPP2PPP/RN2KB1R\n","True r1b1k1nr/pppp1p1p/2nb4/7P/P7/2P3qK/4P1P1/1N4N1\n","True r3kbnr/pp3ppp/8/P1ppn3/5p1q/2PKP2N/1P1P3P/RNBb1B1R\n","True r3k1nr/ppp2ppp/2n1b3/4p3/2p1P1P1/5P2/PP1q3P/R1bK1BNR\n","False r1b1qb1r/4pp1p/2P1kn2/p3Q1B1/3Pp3/2N5/1PP1BPPP/R3K1NR\n","False rnbqk1nr/p1ppp1Pp/1p5b/7Q/8/8/PPPP1PPP/RNB1KBNR\n","True rnb1k2r/ppp1pp1p/5n2/5p1P/P7/1P1p2b1/2PP2P1/1NBK1q2\n","False rnbq4/1p1p1p2/p1k1p3/2Q3Bp/3P4/P1N5/P1P2PPP/R2QKBNR\n","False r1bq4/p1pp3p/1p1k1B2/3Qp3/4P3/5N2/PPP2PPP/RN2KB1R\n","True r3kb1r/1pp2ppp/2np4/8/bpP5/5P2/1BnP2P1/3KqBR1\n","False 1Q6/1b6/1kQ5/p2pB3/B2P4/2N5/PPP2PPP/R5K1\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 4.7699 - policy_output_accuracy: 0.0500 - policy_output_loss: 4.7666 - value_output_loss: 0.0032 - value_output_mse: 0.0032\n","Epoch 1: loss improved from inf to 4.76988, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 4.7699 - policy_output_accuracy: 0.0500 - policy_output_loss: 4.7666 - value_output_loss: 0.0032 - value_output_mse: 0.0032   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 4.3308 - policy_output_accuracy: 0.0833 - policy_output_loss: 4.3276 - value_output_loss: 0.0032 - value_output_mse: 0.0032\n","Epoch 1: loss improved from 4.76988 to 4.33077, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 4.3308 - policy_output_accuracy: 0.0833 - policy_output_loss: 4.3276 - value_output_loss: 0.0032 - value_output_mse: 0.0032   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 4.4191 - policy_output_accuracy: 0.1000 - policy_output_loss: 4.4150 - value_output_loss: 0.0041 - value_output_mse: 0.0041\n","Epoch 1: loss did not improve from 4.33077\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - loss: 4.4191 - policy_output_accuracy: 0.1000 - policy_output_loss: 4.4150 - value_output_loss: 0.0041 - value_output_mse: 0.0041\n","\n","Average Policy Output Loss: 4.503082911173503\n","Average Value Output Loss: 0.003496114552641908\n","Iteration 23/7000\n"]}],"source":["import numpy as np\n","import threading\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","\n","def EfficientNet_block(inputs, filters=256, expansion_factor=6, stride=1, se_ratio=0.25):\n","    \"\"\"\n","    EfficientNet tarzı bir MBConv bloğu.\n","\n","    Args:\n","        inputs: Giriş tensörü.\n","        filters: Çıkış filtrelerinin sayısı.\n","        expansion_factor: Genişletme oranı (Bottleneck genişletme için).\n","        stride: Adım boyutu (ör. 1 veya 2).\n","        se_ratio: Squeeze-and-Excitation (SE) oranı.\n","\n","    Returns:\n","        EfficientNet MBConv bloğunun çıktısı.\n","    \"\"\"\n","    input_channels = inputs.shape[-1]  # Giriş kanal sayısı\n","\n","    # 1. Expand (Genişletme)\n","    x = layers.Conv2D(input_channels * expansion_factor, (1, 1), strides=1, padding=\"same\",\n","                      kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.ReLU()(x)\n","\n","    # 2. Depthwise Convolution (Derinlikçe Ayrık Evrişim)\n","    x = layers.DepthwiseConv2D((3, 3), strides=stride, padding=\"same\",\n","                               depthwise_initializer=\"he_normal\", use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.ReLU()(x)\n","\n","    # 3. Squeeze-and-Excitation (SE Blok)\n","    if se_ratio:\n","        se = layers.GlobalAveragePooling2D()(x)\n","        se = layers.Reshape((1, 1, x.shape[-1]))(se)\n","        se = layers.Conv2D(int(input_channels * se_ratio), (1, 1), activation=\"relu\", kernel_initializer=\"he_normal\")(se)\n","        se = layers.Conv2D(x.shape[-1], (1, 1), activation=\"sigmoid\", kernel_initializer=\"he_normal\")(se)\n","        x = layers.Multiply()([x, se])\n","\n","    # 4. Project (Daraltma)\n","    x = layers.Conv2D(filters, (1, 1), strides=1, padding=\"same\",\n","                      kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","\n","    # 5. Shortcut Connection (Skip Connection)\n","    if stride == 1 and inputs.shape[-1] == filters:\n","        x = layers.Add()([inputs, x])\n","\n","    return x\n","\n","\n","\n","def sigmachess_network(input_shape=(8, 8, 119)):\n","    inputs = layers.Input(shape=input_shape)\n","\n","    x = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.ReLU()(x)\n","\n","    for _ in range(19):\n","        x = EfficientNet_block(x)\n","\n","    policy = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    policy = layers.BatchNormalization()(policy)\n","    policy = layers.ReLU()(policy)\n","    policy = layers.Conv2D(73, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(policy)\n","    policy = layers.Flatten()(policy)\n","    policy = layers.Softmax(name=\"policy_output\")(policy)\n","\n","    value = layers.Conv2D(1, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    value = layers.BatchNormalization()(value)\n","    value = layers.ReLU()(value)\n","    value = layers.Flatten()(value)\n","    value = layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(value)\n","    value = layers.Dense(1, activation=\"tanh\", name=\"value_output\", kernel_initializer=\"he_normal\")(value)\n","\n","    model = models.Model(inputs=inputs, outputs=[policy, value])\n","\n","    return model\n","\n","def create_model():\n","    model = sigmachess_network()\n","\n","    model.compile(\n","        optimizer=Adam(learning_rate=0.00001),\n","        loss={\n","            \"policy_output\": \"categorical_crossentropy\",\n","            \"value_output\": \"mean_squared_error\"\n","        },\n","        metrics={\n","            \"policy_output\": \"accuracy\",\n","            \"value_output\": \"mse\"\n","        }\n","    )\n","\n","    return model\n","\n","def play_vs_stockfish(model, game, replay_buffer):\n","    state = GameState()\n","    temperature = 1.0 if game < 5 else 0.1\n","\n","    w_states, w_policies, w_rewards = [], [], []\n","    player = np.random.choice([chess.WHITE, chess.BLACK])\n","\n","    engine = chess.engine.SimpleEngine.popen_uci(r\"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\")\n","\n","    while not state.is_terminal():\n","        if state.board.turn == player:\n","            mcts = MCTS(model, 1.0, 10)\n","            action_probs = mcts.run(state, temperature)\n","\n","            w_states.append(state.get_current_state())\n","            w_policies.append(action_probs)\n","\n","            action = np.random.choice(len(action_probs), p=action_probs)\n","            state.get_next_state(action)\n","        else:\n","            result = engine.play(state.board, chess.engine.Limit(0.04))\n","            state.apply_action(result.move)\n","\n","    engine.close()\n","\n","    winner = state.get_winner()\n","    w_value = 1 if winner == player else (0 if winner == 2 else -1)\n","\n","    print(player, state.board.board_fen())\n","\n","    replay_buffer.store_multiple_data(w_states, w_policies, w_value)\n","\n","def self_play(model, num_games=100, max_workers=5):\n","    replay_buffer = ReplayBuffer(maxlen=500000)\n","\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = [\n","            executor.submit(play_vs_stockfish, model, i, replay_buffer)\n","            for i in range(num_games)\n","        ]\n","\n","        for future in as_completed(futures):\n","            future.result()\n","\n","    print(\"Self-play completed\")\n","\n","    return replay_buffer\n","\n","def prepare_validation_data(replay_buffer=None, filename=\"validation_data.npy\"):\n","    try:\n","        # First, try to load existing validation data\n","        data = np.load(filename, allow_pickle=True).item()\n","        print(f\"Loaded existing validation data from {filename}\")\n","        return data['x_val'], {\n","            \"policy_output\": data['y_val_policy'],\n","            \"value_output\": data['y_val_value']\n","        }\n","    except (FileNotFoundError, IOError):\n","        # If no existing data, create new validation dataset\n","        if replay_buffer is None:\n","            # If no replay buffer provided, generate synthetic data\n","            x_val = np.random.random((100, 8, 8, 119))\n","            y_val_policy = np.random.randint(0, 73, size=(100, 1))\n","            y_val_value = np.random.random((100, 1)) * 2 - 1  # Values between -1 and 1\n","        else:\n","            # Use replay buffer to generate validation data\n","            states, policies, rewards = replay_buffer.sample(100)\n","            x_val = np.squeeze(states)\n","            y_val_policy = policies\n","            y_val_value = np.array(rewards).reshape(-1, 1)\n","\n","        # One-hot encode policy output\n","        y_val_policy_onehot = np.eye(73)[y_val_policy.flatten()]\n","\n","        # Prepare data dictionary\n","        validation_data = {\n","            \"x_val\": x_val,\n","            \"y_val_policy\": y_val_policy_onehot,\n","            \"y_val_value\": y_val_value\n","        }\n","\n","        # Save the validation data\n","        np.save(filename, validation_data)\n","        print(f\"Created and saved new validation data to {filename}\")\n","\n","        return x_val, {\n","            \"policy_output\": y_val_policy_onehot,\n","            \"value_output\": y_val_value\n","        }\n","\n","def create_callbacks(checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n","    checkpoint = ModelCheckpoint(\n","        filepath=checkpoint_path,\n","        save_weights_only=True,\n","        monitor=\"loss\",\n","        mode=\"min\",\n","        save_best_only=True,\n","        save_freq=\"epoch\",\n","        verbose=1\n","    )\n","    return [checkpoint]\n","\n","def train_model(model, replay_buffer: ReplayBuffer, batch_size=256, epochs=3, checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n","    x_val, y_val = prepare_validation_data()\n","\n","    callbacks = create_callbacks(checkpoint_path)\n","\n","    total_policy_loss = 0\n","    total_value_loss = 0\n","    epoch_count = 0\n","\n","    for epoch in range(epochs):\n","        states, policies, values = replay_buffer.sample(batch_size)\n","\n","        states = np.squeeze(states)\n","        if len(states.shape) == 3:\n","            states = np.expand_dims(states, -1)\n","\n","        values = np.array(values).reshape(-1, 1)\n","\n","        history = model.fit(\n","            states,\n","            { \"policy_output\": policies, \"value_output\": values },\n","            batch_size=batch_size,\n","            epochs=1,\n","            callbacks=callbacks,\n","            verbose=1\n","        )\n","\n","        total_policy_loss += history.history['policy_output_loss'][0]\n","        total_value_loss += history.history['value_output_loss'][0]\n","        epoch_count += 1\n","\n","    avg_policy_loss = total_policy_loss / epoch_count\n","    avg_value_loss = total_value_loss / epoch_count\n","\n","    print(f\"\\nAverage Policy Output Loss: {avg_policy_loss}\")\n","    print(f\"Average Value Output Loss: {avg_value_loss}\")\n","\n","    return avg_policy_loss, avg_value_loss\n","\n","is_stop = False\n","\n","def train_sigmachess(model, num_iterations=100, num_games_per_iteration=100):\n","    global is_stop\n","\n","    # Değişkenleri başlatın\n","    total_policy_loss = 0\n","    total_value_loss = 0\n","    iteration_count = 0\n","\n","    for iteration in range(num_iterations):\n","        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n","\n","        replay_buffer = self_play(model, num_games_per_iteration)\n","        policy_loss, value_loss = train_model(model, replay_buffer)\n","\n","        # Kayıpları biriktir\n","        total_policy_loss += policy_loss\n","        total_value_loss += value_loss\n","        iteration_count += 1\n","\n","        # Her 5 iteration'da bir ortalamaları yazdır\n","        if iteration_count % 5 == 0:\n","            avg_policy_loss = total_policy_loss / iteration_count\n","            avg_value_loss = total_value_loss / iteration_count\n","            print(f\"\\n[After {iteration_count} Iterations]\")\n","            print(f\"Average Policy Output Loss: {avg_policy_loss}\")\n","            print(f\"Average Value Output Loss: {avg_value_loss}\")\n","\n","        if is_stop:\n","            break\n","\n","    model.save(\"/content/drive/My Drive/full_model.keras\")\n","\n","\n","def stop():\n","    global is_stop\n","\n","    while True:\n","        inp = input(\"\")\n","        if inp == \"stop\":\n","            is_stop = True\n","            print(\"After the iteration is completed, the training will be stopped and the model will be saved!\")\n","\n","            break\n","\n","t = threading.Thread(target=stop, daemon=True)\n","t.start()\n","\n","model = create_model()\n","train_sigmachess(model, num_iterations=7000, num_games_per_iteration=15)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDw_Ru4EPXCm"},"outputs":[],"source":["'''\n","def run(self, initial_state, temperature=1.0):\n","    root = Node(initial_state)\n","\n","    # First evaluate and expand root\n","    action_probs, value = self.evaluate(initial_state)\n","    valid_moves = initial_state.get_valid_moves()\n","\n","    # Add Dirichlet noise to root (alpha=0.3 for chess)\n","    noise = np.random.dirichlet([0.3] * len(valid_moves))\n","\n","    # Expand with noisy priors\n","    for idx, action in enumerate(valid_moves):\n","        prob = action_probs[action]\n","        noisy_prob = 0.75 * prob + 0.25 * noise[idx]\n","        next_state = initial_state.clone()\n","        next_state.get_next_state(action)\n","        root.children[action] = Node(next_state, parent=root, prior_prob=noisy_prob)\n","\n","    for _ in range(self.simulations):\n","        node = root\n","\n","        # Selection with additional safety checks\n","        while node.is_expanded and not node.state.is_terminal():\n","            action, child_node = node.select(self.c_puct)\n","\n","            # Safety check to prevent NoneType errors\n","            if child_node is None:\n","                break\n","\n","            node = child_node\n","\n","        # Expansion and Evaluation\n","        if not node.state.is_terminal():\n","            # Ensure node has valid children before expansion\n","            if not node.is_expanded and len(node.children) == 0:\n","                action_probs, value = self.evaluate(node.state)\n","                valid_moves = node.state.get_valid_moves()\n","                node.expand(action_probs)\n","\n","            if node.is_expanded and len(node.children) > 0:\n","                # Randomly select an unexpanded child if possible\n","                unexpanded_children = [child for child in node.children.values() if not child.is_expanded]\n","                if unexpanded_children:\n","                    node = np.random.choice(unexpanded_children)\n","\n","                action_probs, value = self.evaluate(node.state)\n","                valid_moves = node.state.get_valid_moves()\n","                node.expand(action_probs)\n","            else:\n","                # Fallback: re-evaluate the current node\n","                value = node.state.get_winner()\n","                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","        else:\n","            value = node.state.get_winner()\n","            value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","\n","        # Backup\n","        node.backup(value)\n","\n","    return self.get_action_probs(root, temperature)\n","'''\n","#run revize edilmiş"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"za1kYZLOlMC0"},"outputs":[],"source":["#!!!   üstteki hata önleme etkili değilse yapmamız gereken Eğer mevcut düğüm üzerinde genişletme veya seçim yapılması mümkün değilse, başka bir alt düğüme geçiş yapılır. Bu, unexpanded_children listesinden rastgele bir çocuk seçilerek yapılır.\n","'''\n","for _ in range(self.simulations):\n","    node = root\n","\n","    # Seçim aşamasında NoneType kontrolü\n","    while node.is_expanded and not node.state.is_terminal():\n","        action, child_node = node.select(self.c_puct)\n","\n","        # Eğer child_node None ise, bu düğüm üzerinde işlem yapılmaz, döngü devam eder\n","        if child_node is None:\n","            break\n","\n","        node = child_node\n","\n","    # Genişletme ve Değerlendirme\n","    if not node.state.is_terminal():\n","        # Düğümün çocukları yoksa ve genişletilemiyorsa\n","        if not node.is_expanded and len(node.children) == 0:\n","            action_probs, value = self.evaluate(node.state)\n","            valid_moves = node.state.get_valid_moves()\n","\n","            # Eğer geçerli hareketler varsa, genişletme işlemi yapılır\n","            if valid_moves:\n","                node.expand(action_probs)\n","            else:\n","                # Geçerli hareket yoksa, yedekleme yap\n","                value = node.state.get_winner()\n","                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","                node.backup(value)\n","                continue  # Diğer bir düğüme geçmek için döngüye devam et\n","\n","        # Çocuk düğümlerinin varlığı kontrol ediliyor\n","        if node.is_expanded and len(node.children) > 0:\n","            unexpanded_children = [child for child in node.children.values() if not child.is_expanded]\n","            if unexpanded_children:\n","                # Rastgele bir genişletilmemiş çocuk seç\n","                node = np.random.choice(unexpanded_children)\n","\n","            action_probs, value = self.evaluate(node.state)\n","            valid_moves = node.state.get_valid_moves()\n","            node.expand(action_probs)\n","        else:\n","            # Eğer hiçbir şey yapılamazsa, fallback değeriyle yedekleme yap\n","            value = node.state.get_winner()\n","            value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","            node.backup(value)\n","            continue  # Geçerli düğüm üzerinde işlem yapamıyorsak, başka bir düğüme geç\n","    else:\n","        # Eğer düğüm terminalse, kazananı belirle\n","        value = node.state.get_winner()\n","        value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","\n","    # Değeri yedekle\n","    node.backup(value)\n","'''"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6219321,"sourceId":10086949,"sourceType":"datasetVersion"},{"datasetId":6227771,"sourceId":10097989,"sourceType":"datasetVersion"}],"dockerImageVersionId":30805,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":0}