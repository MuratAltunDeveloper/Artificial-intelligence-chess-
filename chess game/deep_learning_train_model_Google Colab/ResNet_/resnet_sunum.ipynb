{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 10086949,
          "sourceType": "datasetVersion",
          "datasetId": 6219321
        }
      ],
      "dockerImageVersionId": 30804,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#ULTRA SUPER SIGMA CHESS AI"
      ],
      "metadata": {
        "trusted": true,
        "id": "RUlmIButCyE6"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chess"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T18:37:19.019974Z",
          "iopub.execute_input": "2024-12-03T18:37:19.020302Z",
          "iopub.status.idle": "2024-12-03T18:37:30.406565Z",
          "shell.execute_reply.started": "2024-12-03T18:37:19.020275Z",
          "shell.execute_reply": "2024-12-03T18:37:30.405717Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j5hDBiZCyE7",
        "outputId": "9eb8c39e-d6be-4aa8-f9a4-7f9afad6ef8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chess in /usr/local/lib/python3.10/dist-packages (1.11.1)\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9yiWlrkxC03v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aee45426-c06b-45eb-964f-753e8f089241"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!chmod +x \"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\"\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-03T18:37:30.408309Z",
          "iopub.execute_input": "2024-12-03T18:37:30.408566Z",
          "iopub.status.idle": "2024-12-03T18:37:33.926613Z",
          "shell.execute_reply.started": "2024-12-03T18:37:30.408541Z",
          "shell.execute_reply": "2024-12-03T18:37:33.925393Z"
        },
        "id": "DVYV0gxkCyE8"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import math\n",
        "\n",
        "import chess\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "\n",
        "import chess.engine\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "class GameState:\n",
        "    row = 8\n",
        "    col = 8\n",
        "    promotion_indexes = {\n",
        "        chess.KNIGHT: 0,\n",
        "        chess.ROOK: 1,\n",
        "        chess.BISHOP: 2\n",
        "    }\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.board = chess.Board()\n",
        "        self.repetition_count = 0\n",
        "        self.player_color: chess.Color = chess.WHITE\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        self.board.reset()\n",
        "\n",
        "        return self.get_current_state()\n",
        "\n",
        "    def get_current_state(self, T=8):\n",
        "        input_tensor = np.zeros((8, 8, 119), dtype=np.uint8)\n",
        "\n",
        "        for t in range(T):\n",
        "            _t = T - t - 1\n",
        "            if len(self.board.move_stack) < _t:\n",
        "                continue\n",
        "\n",
        "            self.create_input(input_tensor, _t)\n",
        "\n",
        "        color = 0 if self.board.turn == chess.WHITE else 1\n",
        "        input_tensor[:, :, 112] = color\n",
        "\n",
        "        input_tensor[:, :, 113] = len(self.board.move_stack) > 0\n",
        "\n",
        "        p1_castling = (1 * self.board.has_kingside_castling_rights(chess.WHITE)) | (2 * self.board.has_queenside_castling_rights(chess.WHITE))\n",
        "        p1_castling_bit = format(p1_castling, \"02b\")\n",
        "        input_tensor[:, :, 114] = int(p1_castling_bit[0])\n",
        "        input_tensor[:, :, 115] = int(p1_castling_bit[1])\n",
        "\n",
        "        p2_castling = (1 * self.board.has_kingside_castling_rights(chess.BLACK)) | (2 * self.board.has_queenside_castling_rights(chess.BLACK))\n",
        "        p2_castling_bit = format(p2_castling, \"02b\")\n",
        "        input_tensor[:, :, 116] = int(p2_castling_bit[0])\n",
        "        input_tensor[:, :, 117] = int(p2_castling_bit[1])\n",
        "\n",
        "        input_tensor[:, :, 118] = int(self.board.is_fifty_moves())\n",
        "\n",
        "        return np.expand_dims(input_tensor, axis=0)\n",
        "\n",
        "    def get_next_state(self, action: int):\n",
        "        source_index = action // 73\n",
        "        destination_index = 0\n",
        "        move_type = action % 73\n",
        "\n",
        "        promotion = None\n",
        "\n",
        "        if move_type < 56:\n",
        "            direction = move_type // 7\n",
        "            movement = (move_type % 7) + 1\n",
        "\n",
        "            destination_index = source_index + (movement * 8) if direction == 0 else destination_index\n",
        "            destination_index = source_index + (movement * 9) if direction == 1 else destination_index\n",
        "            destination_index = source_index + movement if direction == 2 else destination_index\n",
        "            destination_index = source_index + (movement * -7) if direction == 3 else destination_index\n",
        "            destination_index = source_index + (movement * -8) if direction == 4 else destination_index\n",
        "            destination_index = source_index + (movement * -9) if direction == 5 else destination_index\n",
        "            destination_index = source_index + (-movement) if direction == 6 else destination_index\n",
        "            destination_index = source_index + (movement * 7) if direction == 7 else destination_index\n",
        "        elif move_type >= 56 and move_type < 64:\n",
        "            direction = move_type - 56\n",
        "\n",
        "            destination_index = source_index + 17 if direction == 0 else destination_index\n",
        "            destination_index = source_index + 10 if direction == 1 else destination_index\n",
        "            destination_index = source_index - 6 if direction == 2 else destination_index\n",
        "            destination_index = source_index - 15 if direction == 3 else destination_index\n",
        "            destination_index = source_index - 17 if direction == 4 else destination_index\n",
        "            destination_index = source_index - 10 if direction == 5 else destination_index\n",
        "            destination_index = source_index + 6 if direction == 6 else destination_index\n",
        "            destination_index = source_index + 15 if direction == 7 else destination_index\n",
        "        else:\n",
        "            direction = move_type // 3\n",
        "            promotion_index = move_type % 3\n",
        "\n",
        "            promotion = chess.KNIGHT if promotion_index == 0 else (chess.ROOK if promotion_index == 1 else chess.BISHOP)\n",
        "\n",
        "            if direction == 0:\n",
        "                destination_index = source_index + (8 * (self.board.turn != chess.WHITE) * -1)\n",
        "            elif direction == 1:\n",
        "                destination_index = source_index + (9 * (self.board.turn != chess.WHITE) * -1)\n",
        "            else:\n",
        "                destination_index = source_index + (7 * (self.board.turn != chess.WHITE) * -1)\n",
        "\n",
        "        from_square = chess.Square(source_index)\n",
        "        to_square = chess.Square(destination_index)\n",
        "\n",
        "        move = chess.Move(from_square, to_square, promotion)\n",
        "        self.apply_action(move)\n",
        "\n",
        "        return move, self.get_current_state()\n",
        "\n",
        "    def apply_action(self, move: chess.Move):\n",
        "        try:\n",
        "            self.board.push(move)\n",
        "        except Exception as e:\n",
        "            print(list(self.board.legal_moves))\n",
        "            print(self.get_valid_moves())\n",
        "\n",
        "            print(e)\n",
        "\n",
        "            raise Exception(\"Error\")\n",
        "\n",
        "    def create_input(self, input_tensor: np.ndarray, t: int):\n",
        "        piece_types = {\n",
        "            chess.PAWN: 0,\n",
        "            chess.KNIGHT: 1,\n",
        "            chess.BISHOP: 2,\n",
        "            chess.ROOK: 3,\n",
        "            chess.QUEEN: 4,\n",
        "            chess.KING: 5\n",
        "        }\n",
        "\n",
        "        board = self.board.copy()\n",
        "        for _ in range(t):\n",
        "            board.pop()\n",
        "\n",
        "        transposition_key = board._transposition_key()\n",
        "\n",
        "        for square in chess.SQUARES:\n",
        "            piece = board.piece_at(square)\n",
        "\n",
        "            if piece is None:\n",
        "                continue\n",
        "\n",
        "            piece_index = piece_types[piece.piece_type]\n",
        "            piece_color = 0 if piece.color == chess.WHITE else 1\n",
        "\n",
        "            index = (t * 14) + (piece_color * 6) + piece_index\n",
        "            input_tensor[square // 8][square % 8][index] = 1\n",
        "\n",
        "        repetition_count = 0\n",
        "        index = (t * 14) + 12\n",
        "\n",
        "        try:\n",
        "            while board.move_stack:\n",
        "                move = board.pop()\n",
        "                if board.is_irreversible(move):\n",
        "                    break\n",
        "\n",
        "                if board._transposition_key() == transposition_key:\n",
        "                    repetition_count += 1\n",
        "\n",
        "                if repetition_count == 3:\n",
        "                    break\n",
        "        finally:\n",
        "            repetition_count = 3 if repetition_count > 3 else repetition_count\n",
        "\n",
        "            repetition_count_bits = [int(x) for x in format(repetition_count, \"02b\")]\n",
        "            input_tensor[:, :, index] = repetition_count_bits[0]\n",
        "            input_tensor[:, :, index + 1] = repetition_count_bits[1]\n",
        "\n",
        "    def get_valid_moves(self):\n",
        "        legal_moves = []\n",
        "\n",
        "        for valid_move in self.board.legal_moves:\n",
        "            s_row, s_col, from_square_index = self.index_of_square(valid_move.from_square)\n",
        "            d_row, d_col, to_square_index = self.index_of_square(valid_move.to_square)\n",
        "\n",
        "            if valid_move.promotion:\n",
        "                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n",
        "\n",
        "                if valid_move.promotion == chess.QUEEN:\n",
        "                    index = (from_square_index * 73) + (direction * 7)\n",
        "                    legal_moves.append(index)\n",
        "                else:\n",
        "                    promotion_index = self.promotion_indexes[valid_move.promotion]\n",
        "\n",
        "                    if direction > 2:\n",
        "                        direction = 0 if direction == 4 else (1 if direction == 5 else 2)\n",
        "                    else:\n",
        "                        direction = 2 if direction == 7 else direction\n",
        "\n",
        "                    index = (from_square_index * 73) + ((direction * 3) + promotion_index + 64)\n",
        "                    legal_moves.append(index)\n",
        "            elif self.board.piece_type_at(valid_move.from_square) == chess.KNIGHT:\n",
        "                direction = self.direction_of_move_for_knights(s_row, s_col, d_row, d_col)\n",
        "\n",
        "                index = (from_square_index * 73) + direction + 56\n",
        "                legal_moves.append(index)\n",
        "\n",
        "            else:\n",
        "                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n",
        "                count_of_square = self.count_of_square_for_movement(s_row, s_col, d_row, d_col) - 1\n",
        "\n",
        "                index = (from_square_index * 73) + ((direction * 7) + count_of_square)\n",
        "                legal_moves.append(index)\n",
        "\n",
        "        return legal_moves\n",
        "\n",
        "    def index_of_square(self, square: chess.Square):\n",
        "        row = chess.square_rank(square)\n",
        "        col = chess.square_file(square)\n",
        "        index = (row * 8) + col\n",
        "\n",
        "        return row, col, index\n",
        "\n",
        "    def direction_of_move_for_ray_directions(self, s_row: int, s_col: int, d_row: int, d_col: int):\n",
        "        delta_x = d_col - s_col\n",
        "        delta_y = d_row - s_row\n",
        "\n",
        "        if delta_x == 0:\n",
        "            return 0 if delta_y > 0 else 4\n",
        "\n",
        "        if delta_y == 0:\n",
        "            return 2 if delta_x > 0 else 6\n",
        "\n",
        "        if delta_x < 0:\n",
        "            return 7 if delta_y > 0 else 5\n",
        "\n",
        "        return 1 if delta_y > 0 else 3\n",
        "\n",
        "    def direction_of_move_for_knights(self, s_row: int, s_col: int, d_row: int, d_col: int):\n",
        "        delta_x = d_col - s_col\n",
        "        delta_y = d_row - s_row\n",
        "\n",
        "        if delta_x == 1:\n",
        "            return 0 if delta_y > 0 else 3\n",
        "\n",
        "        if delta_x == 2:\n",
        "            return 1 if delta_y > 0 else 2\n",
        "\n",
        "        if delta_x == -1:\n",
        "            return 7 if delta_y > 0 else 4\n",
        "\n",
        "        return 6 if delta_y > 0 else 5\n",
        "\n",
        "    def count_of_square_for_movement(self, s_row: int, s_col: int, d_row: int, d_col: int):\n",
        "        delta_x = d_col - s_col\n",
        "        delta_y = d_row - s_row\n",
        "\n",
        "        return max(abs(delta_x), abs(delta_y))\n",
        "\n",
        "    def get_winner(self):\n",
        "        result = self.board.result()\n",
        "\n",
        "        if result == \"1-0\":\n",
        "            return chess.WHITE\n",
        "\n",
        "        if result == \"0-1\":\n",
        "            return chess.BLACK\n",
        "\n",
        "        return 2\n",
        "\n",
        "    def is_terminal(self):\n",
        "        return self.board.is_game_over()\n",
        "\n",
        "    def clone(self):\n",
        "        cloned_state = GameState()\n",
        "        cloned_state.board = self.board.copy()\n",
        "\n",
        "        return cloned_state\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, state, parent=None, prior_prob=1.0):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.visits = 0\n",
        "        self.value_sum = 0\n",
        "        self.prior_prob = prior_prob\n",
        "        self.is_expanded = False\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.value_sum / (self.visits + 1e-5)\n",
        "\n",
        "    def expand(self, action_probs):\n",
        "        for action, prob in enumerate(action_probs):\n",
        "            if prob > 0:\n",
        "                next_state = self.state.clone()\n",
        "                next_state.get_next_state(action)\n",
        "                self.children[action] = Node(next_state, parent=self, prior_prob=prob)\n",
        "\n",
        "        if len(self.children) > 0:\n",
        "            self.is_expanded = True\n",
        "\n",
        "    def select(self, c_puct=1.0):\n",
        "        max_ucb = -float('inf')\n",
        "        best_action = None\n",
        "        best_child = None\n",
        "\n",
        "        for action, child in self.children.items():\n",
        "            ucb = child.value + c_puct * child.prior_prob * (math.sqrt(self.visits) / (1 + child.visits))\n",
        "            if ucb > max_ucb:\n",
        "                max_ucb = ucb\n",
        "                best_action = action\n",
        "                best_child = child\n",
        "        return best_action, best_child\n",
        "\n",
        "    def backup(self, value):\n",
        "        self.visits += 1\n",
        "        self.value_sum += value\n",
        "        if self.parent:\n",
        "            self.parent.backup(-value)\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, c_puct=1.0, simulations=50):\n",
        "        self.model = model\n",
        "        self.c_puct = c_puct\n",
        "        self.simulations = simulations\n",
        "\n",
        "    def add_dirichlet_noise(self, node, valid_moves):\n",
        "        noise = np.random.dirichlet([0.3] * len(valid_moves))\n",
        "        for idx, action in enumerate(valid_moves):\n",
        "            if action in node.children:\n",
        "                node.children[action].prior_prob = \\\n",
        "                    0.75 * node.children[action].prior_prob + 0.25 * noise[idx]\n",
        "\n",
        "    def run(self, initial_state, temperature=1.0):\n",
        "        root = Node(initial_state)\n",
        "\n",
        "        # First evaluate and expand root\n",
        "        action_probs, value = self.evaluate(initial_state)\n",
        "        valid_moves = initial_state.get_valid_moves()\n",
        "\n",
        "        # Add Dirichlet noise to root (alpha=0.3 for chess)\n",
        "        noise = np.random.dirichlet([0.3] * len(valid_moves))\n",
        "\n",
        "        # Expand with noisy priors\n",
        "        for idx, action in enumerate(valid_moves):\n",
        "            prob = action_probs[action]\n",
        "            noisy_prob = 0.75 * prob + 0.25 * noise[idx]\n",
        "            next_state = initial_state.clone()\n",
        "            next_state.get_next_state(action)\n",
        "            root.children[action] = Node(next_state, parent=root, prior_prob=noisy_prob)\n",
        "\n",
        "        for _ in range(self.simulations):\n",
        "            node = root\n",
        "\n",
        "            # Selection\n",
        "            while node.is_expanded and not node.state.is_terminal():\n",
        "                action, node = node.select(self.c_puct)\n",
        "\n",
        "            # Expansion and Evaluation\n",
        "            if not node.state.is_terminal():\n",
        "                action_probs, value = self.evaluate(node.state)\n",
        "                valid_moves = node.state.get_valid_moves()\n",
        "                node.expand(action_probs)\n",
        "            else:\n",
        "                value = node.state.get_winner()\n",
        "                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n",
        "\n",
        "            # Backup\n",
        "            node.backup(value)\n",
        "\n",
        "        return self.get_action_probs(root, temperature)\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        state_tensor = state.get_current_state()\n",
        "        # state_tensor = np.expand_dims(state_tensor, axis=0)\n",
        "\n",
        "        policy, value = self.model.predict(state_tensor, verbose=0)\n",
        "\n",
        "        # Mask invalid moves\n",
        "        valid_moves = state.get_valid_moves()\n",
        "        mask = np.zeros(policy.shape[1])\n",
        "        mask[valid_moves] = 1\n",
        "\n",
        "        policy = policy[0] * mask\n",
        "\n",
        "        # Normalize\n",
        "        sum_policy = np.sum(policy)\n",
        "        if sum_policy > 0:\n",
        "            policy /= sum_policy\n",
        "        else:\n",
        "            # If all moves were masked, use uniform distribution over valid moves\n",
        "            policy = mask / np.sum(mask)\n",
        "\n",
        "        return policy, value[0][0]\n",
        "\n",
        "    def get_action_probs(self, root, temperature=1.0):\n",
        "        visits = np.array([child.visits for action, child in root.children.items()])\n",
        "        actions = list(root.children.keys())\n",
        "\n",
        "        if temperature == 0:  # Pure exploitation\n",
        "            action_idx = np.argmax(visits)\n",
        "            probs = np.zeros_like(visits)\n",
        "            probs[action_idx] = 1\n",
        "        else:\n",
        "            # Apply temperature\n",
        "            visits = visits ** (1 / temperature)\n",
        "            probs = visits / np.sum(visits)\n",
        "\n",
        "        # Convert to full move probability vector\n",
        "        full_probs = np.zeros(4672)  # Adjust size based on your action space\n",
        "        for action, prob in zip(actions, probs):\n",
        "            full_probs[action] = prob\n",
        "\n",
        "        return full_probs\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, maxlen=500000):\n",
        "        self.buffer = deque(maxlen=maxlen)\n",
        "        self.current_size = 0\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def store(self, state, policy, value):\n",
        "        \"\"\"Store a single game state transition\"\"\"\n",
        "        self.buffer.append({\n",
        "            'state': state,\n",
        "            'policy': policy,\n",
        "            'value': value\n",
        "        })\n",
        "        self.current_size = len(self.buffer)\n",
        "\n",
        "    def store_multiple_data(self, states, policies, value):\n",
        "        with self.lock:\n",
        "            for s, p, v in zip(states, policies, [value]):\n",
        "                self.store(s, p, v)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch with augmentations\"\"\"\n",
        "        if self.current_size < batch_size:\n",
        "            batch_size = self.current_size\n",
        "\n",
        "        indices = np.random.choice(self.current_size, batch_size)\n",
        "        states, policies, values = [], [], []\n",
        "\n",
        "        for idx in indices:\n",
        "            sample = self.buffer[idx]\n",
        "            # Get augmented samples\n",
        "            aug_states, aug_policies = self._augment_sample(\n",
        "                sample['state'],\n",
        "                sample['policy']\n",
        "            )\n",
        "\n",
        "            # Add all augmentations\n",
        "            states.extend(aug_states)\n",
        "            policies.extend(aug_policies)\n",
        "            values.extend([sample['value']] * len(aug_states))\n",
        "\n",
        "        return np.array(states), np.array(policies), np.array(values)\n",
        "\n",
        "    def _augment_sample(self, state, policy):\n",
        "        \"\"\"Generate valid augmentations for a single sample\"\"\"\n",
        "        # Remove batch dimension if present\n",
        "        if len(state.shape) == 4:\n",
        "            state = np.squeeze(state, axis=0)\n",
        "\n",
        "        augmented_states = [state]\n",
        "        augmented_policies = [policy]\n",
        "\n",
        "        # Horizontal flip\n",
        "        flip_h = np.flip(state, axis=1)\n",
        "        augmented_states.append(flip_h)\n",
        "        augmented_policies.append(policy)  # Policy needs game-specific mapping\n",
        "\n",
        "        # Vertical flip\n",
        "        flip_v = np.flip(state, axis=0)\n",
        "        augmented_states.append(flip_v)\n",
        "        augmented_policies.append(policy)  # Policy needs game-specific mapping\n",
        "\n",
        "        # Diagonal flip (only if shape allows)\n",
        "        if state.shape[0] == state.shape[1]:\n",
        "            diag = np.transpose(state, (1, 0, 2))\n",
        "            augmented_states.append(diag)\n",
        "            augmented_policies.append(policy)  # Policy needs game-specific mapping\n",
        "\n",
        "        return augmented_states, augmented_policies\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.current_size\n",
        "\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "print(\"All devices: \", tf.config.list_logical_devices('GPU'))\n",
        "\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)\n"
      ],
      "metadata": {
        "_uuid": "d7c89aa5-8449-4c9b-b220-63cf5c8ab98c",
        "_cell_guid": "e45be06e-fe38-4b84-b26e-6ead59ee098e",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2024-12-03T18:40:52.940405Z",
          "iopub.execute_input": "2024-12-03T18:40:52.940740Z"
        },
        "id": "py_fPedFCyE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96845aec-681b-4a97-c06e-c0f45f89565e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class ConfusionMatrixCallback(Callback):\n",
        "    def __init__(self, validation_data, interval=5):\n",
        "        super().__init__()\n",
        "        self.validation_data = validation_data\n",
        "        self.interval = interval  # To plot every 5 epochs by default\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Only generate confusion matrix every 'interval' epochs\n",
        "        if (epoch + 1) % self.interval == 0:\n",
        "            # Assuming validation_data is a tuple of (x_val, y_val)\n",
        "            x_val, y_val = self.validation_data\n",
        "\n",
        "            # Predict policy outputs\n",
        "            policy_pred = self.model.predict(x_val)[0]\n",
        "\n",
        "            # Convert probabilistic predictions to class predictions\n",
        "            y_pred_classes = np.argmax(policy_pred, axis=1)\n",
        "            y_true_classes = np.argmax(y_val['policy_output'], axis=1)\n",
        "\n",
        "            # Compute confusion matrix\n",
        "            cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "\n",
        "            # Plot confusion matrix\n",
        "            plt.figure(figsize=(10,8))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "            plt.title(f'Confusion Matrix at Epoch {epoch + 1}')\n",
        "            plt.xlabel('Predicted Label')\n",
        "            plt.ylabel('True Label')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'confusion_matrix_epoch_{epoch + 1}.png')\n",
        "            plt.close()\n",
        "\n",
        "def residual_block(inputs, filters=256, kernel_size=(3, 3), stride=1):\n",
        "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding=\"same\", kernel_initializer=HeNormal(), use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = layers.Conv2D(filters, kernel_size, strides=stride, padding=\"same\", kernel_initializer=HeNormal(), use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    x = layers.Add()([x, inputs])\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def sigmachess_network(input_shape=(8, 8, 119)):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=HeNormal(), use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    for _ in range(19):\n",
        "        x = residual_block(x)\n",
        "\n",
        "    policy = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=HeNormal(), use_bias=False)(x)\n",
        "    policy = layers.BatchNormalization()(policy)\n",
        "    policy = layers.ReLU()(policy)\n",
        "    policy = layers.Conv2D(73, (1, 1), strides=1, padding=\"same\", kernel_initializer=HeNormal())(policy)\n",
        "    policy = layers.Flatten()(policy)\n",
        "    policy = layers.Softmax(name=\"policy_output\")(policy)\n",
        "\n",
        "    value = layers.Conv2D(1, (1, 1), strides=1, padding=\"same\", kernel_initializer=HeNormal(), use_bias=False)(x)\n",
        "    value = layers.BatchNormalization()(value)\n",
        "    value = layers.ReLU()(value)\n",
        "    value = layers.Flatten()(value)\n",
        "    value = layers.Dense(256, activation=\"relu\", kernel_initializer=HeNormal())(value)\n",
        "    value = layers.Dense(1, activation=\"tanh\", name=\"value_output\", kernel_initializer=HeNormal())(value)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=[policy, value])\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_model():\n",
        "    model = sigmachess_network()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.02),\n",
        "        loss={\n",
        "            \"policy_output\": \"categorical_crossentropy\",\n",
        "            \"value_output\": \"mean_squared_error\"\n",
        "        },\n",
        "        metrics={\n",
        "            \"policy_output\": \"accuracy\",\n",
        "            \"value_output\": \"mse\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# train_model.py\n",
        "\n",
        "def play_vs_stockfish(model, game, replay_buffer):\n",
        "    state = GameState()\n",
        "    temperature = 1.0 if game < 5 else 0.1\n",
        "\n",
        "    w_states, w_policies, w_rewards = [], [], []\n",
        "    player = np.random.choice([chess.WHITE, chess.BLACK])\n",
        "\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(r\"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\")\n",
        "\n",
        "    while not state.is_terminal():\n",
        "        if state.board.turn == player:\n",
        "            mcts = MCTS(model, 1.0, 10)\n",
        "            action_probs = mcts.run(state, temperature)\n",
        "\n",
        "            w_states.append(state.get_current_state())\n",
        "            w_policies.append(action_probs)\n",
        "\n",
        "            action = np.random.choice(len(action_probs), p=action_probs)\n",
        "            state.get_next_state(action)\n",
        "        else:\n",
        "            result = engine.play(state.board, chess.engine.Limit(0.04))\n",
        "            state.apply_action(result.move)\n",
        "\n",
        "    engine.close()\n",
        "\n",
        "    winner = state.get_winner()\n",
        "    w_value = 1 if winner == player else (0 if winner == 2 else -1)\n",
        "\n",
        "    print(player, state.board.board_fen())\n",
        "\n",
        "    replay_buffer.store_multiple_data(w_states, w_policies, w_value)\n",
        "\n",
        "def self_play(model, num_games=100, max_workers=5):\n",
        "    replay_buffer = ReplayBuffer(maxlen=500000)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [\n",
        "            executor.submit(play_vs_stockfish, model, i, replay_buffer)\n",
        "            for i in range(num_games)\n",
        "        ]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            future.result()\n",
        "\n",
        "    print(\"Girdi\")\n",
        "\n",
        "    return replay_buffer\n",
        "\n",
        "def create_callbacks(checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        monitor=\"loss\",\n",
        "        mode=\"min\",\n",
        "        save_best_only=True,\n",
        "        save_freq=\"epoch\",\n",
        "        verbose=1\n",
        "    )\n",
        "    return [checkpoint]\n",
        "def prepare_validation_data(replay_buffer=None, filename=\"validation_data.npy\"):\n",
        "    try:\n",
        "        # First, try to load existing validation data\n",
        "        data = np.load(filename, allow_pickle=True).item()\n",
        "        print(f\"Loaded existing validation data from {filename}\")\n",
        "        return data['x_val'], {\n",
        "            \"policy_output\": data['y_val_policy'],\n",
        "            \"value_output\": data['y_val_value']\n",
        "        }\n",
        "    except (FileNotFoundError, IOError):\n",
        "        # If no existing data, create new validation dataset\n",
        "        if replay_buffer is None:\n",
        "            # If no replay buffer provided, generate synthetic data\n",
        "            x_val = np.random.random((100, 8, 8, 119))\n",
        "            y_val_policy = np.random.randint(0, 73, size=(100, 1))\n",
        "            y_val_value = np.random.random((100, 1)) * 2 - 1  # Values between -1 and 1\n",
        "        else:\n",
        "            # Use replay buffer to generate validation data\n",
        "            states, policies, rewards = replay_buffer.sample(100)\n",
        "            x_val = np.squeeze(states)\n",
        "            y_val_policy = policies\n",
        "            y_val_value = np.array(rewards).reshape(-1, 1)\n",
        "\n",
        "        # One-hot encode policy output\n",
        "        y_val_policy_onehot = np.eye(73)[y_val_policy.flatten()]\n",
        "\n",
        "        # Prepare data dictionary\n",
        "        validation_data = {\n",
        "            \"x_val\": x_val,\n",
        "            \"y_val_policy\": y_val_policy_onehot,\n",
        "            \"y_val_value\": y_val_value\n",
        "        }\n",
        "\n",
        "        # Save the validation data\n",
        "        np.save(filename, validation_data)\n",
        "        print(f\"Created and saved new validation data to {filename}\")\n",
        "\n",
        "        return x_val, {\n",
        "            \"policy_output\": y_val_policy_onehot,\n",
        "            \"value_output\": y_val_value\n",
        "        }\n",
        "def train_model(model, replay_buffer: ReplayBuffer, batch_size=256, epochs=3, checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n",
        "    # Doğrulama verisini hazırla (Bu fonksiyonu verinize göre uygulamalısınız)\n",
        "    x_val, y_val = prepare_validation_data()  # Bu fonksiyonu oluşturmalısınız\n",
        "\n",
        "    # Callback'leri oluştur, konfizyon matrisi callback'ini de ekle\n",
        "    callbacks = create_callbacks(checkpoint_path)\n",
        "\n",
        "    # Konfizyon Matrisi Callback'ini ekle\n",
        "    cm_callback = ConfusionMatrixCallback((x_val, y_val), interval=5)\n",
        "    callbacks.append(cm_callback)\n",
        "\n",
        "    total_policy_loss = 0\n",
        "    total_value_loss = 0\n",
        "    epoch_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Replay buffer'dan veri çek\n",
        "        states, policies, values = replay_buffer.sample(batch_size)\n",
        "\n",
        "        states = np.squeeze(states)\n",
        "        if len(states.shape) == 3:  # Eğer eksik boyut varsa\n",
        "            states = np.expand_dims(states, -1)\n",
        "\n",
        "        values = np.array(values).reshape(-1, 1)\n",
        "\n",
        "        # Modeli 1 epoch için eğit\n",
        "        history = model.fit(\n",
        "            states,\n",
        "            { \"policy_output\": policies, \"value_output\": values },\n",
        "            batch_size=batch_size,\n",
        "            epochs=1,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Kayıpları biriktir\n",
        "        total_policy_loss += history.history['policy_output_loss'][0]\n",
        "        total_value_loss += history.history['value_output_loss'][0]\n",
        "        epoch_count += 1\n",
        "\n",
        "    # Ortalama kayıpları hesapla\n",
        "    avg_policy_loss = total_policy_loss / epoch_count\n",
        "    avg_value_loss = total_value_loss / epoch_count\n",
        "\n",
        "    # Ortalama kayıpları yazdır\n",
        "    print(f\"\\nOrtalama Policy Output Loss: {avg_policy_loss}\")\n",
        "    print(f\"Ortalama Value Output Loss: {avg_value_loss}\")\n",
        "\n",
        "\n",
        "\n",
        "is_stop = False\n",
        "\n",
        "def train_sigmachess(model, num_iterations=100, num_games_per_iteration=100):\n",
        "    global is_stop\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
        "\n",
        "        replay_buffer = self_play(model, num_games_per_iteration)\n",
        "        train_model(model, replay_buffer)\n",
        "\n",
        "        print()\n",
        "\n",
        "        if is_stop:\n",
        "            break\n",
        "\n",
        "    model.save(\"/content/drive/My Drive/full_model.keras\")\n",
        "\n",
        "# with tpu_strategy.scope():\n",
        "    # model = create_model()\n",
        "\n",
        "def stop():\n",
        "    global is_stop\n",
        "\n",
        "    while True:\n",
        "        inp = input(\"\")\n",
        "        if inp == \"stop\":\n",
        "            is_stop = True\n",
        "            print(\"After the iteration is completed, the training will be stopped and the model will be saved!\")\n",
        "\n",
        "            break\n",
        "\n",
        "t = threading.Thread(target=stop, daemon=True)\n",
        "t.start()\n",
        "\n",
        "model = create_model()\n",
        "train_sigmachess(model, num_iterations=7000, num_games_per_iteration=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zmJtM_9MHZcr",
        "outputId": "05c19372-fd34-4846-c028-f47be7405604"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1/7000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b5fd48f03a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False 1rbqkR2/pppppp1p/2n4B/8/3P4/8/PPP2PPP/RN1QKBNR\n",
            "False r1bqkQ2/ppppp2p/2n3pB/6N1/3P4/3B4/PPP2PPP/RN3RK1\n",
            "True rn2r1k1/ppp2ppp/5n2/8/1b1pQ3/6Pb/PPPP1P2/R1B1Kq2\n",
            "True r4bkr/ppp3p1/2n2n2/3p3P/3p4/2P4b/PP1PP2P/1RBQKq2\n",
            "True r3r1k1/pbb2ppp/4nn2/2p5/7P/4p3/PPPPPP2/R1BQK2q\n",
            "True r3kr2/1pp2ppp/2nb1n2/p6P/8/4p1P1/PPPPPPb1/1RBQK2q\n",
            "False r1b1qb1r/pppkp1pp/2p5/8/4P3/1QN1B3/PPP1BPPP/3RK1NR\n",
            "True r2qnrk1/1pp3pp/2n3p1/3p4/4p1b1/7P/PPPPPbP1/R1BQKB1R\n",
            "True rn1qk2r/1pp2pp1/p3bn2/7p/3pp3/5PbP/PPPPP1P1/R1BQKB2\n",
            "True r1b1k1nr/ppp2ppp/2n5/3pp3/5q2/5K2/PPPPP1PP/R1BQ1BR1\n",
            "False 1r2kbr1/ppp1pBpp/2p5/4N3/P3P3/2N1B3/1PP2PPP/2KR3R\n",
            "False 3Rkb1r/1pB1pppp/4b2n/8/P3P3/2N4P/1PP1NPP1/4KB1R\n",
            "True 3qr1k1/1pp2pp1/r1nb1n2/7p/6PP/5b2/PPPP1P2/R1B1q1K1\n",
            "True rnb1kb1r/ppp3pp/8/4p3/3pn3/7P/PPPPPqP1/R1BQKBR1\n",
            "True r3k2r/pppbbpp1/2n5/7p/4p1n1/8/PPPPPq1P/R1BQKB1R\n",
            "Girdi\n",
            "Loaded existing validation data from validation_data.npy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44s/step - loss: 12.1920 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0670 - value_output_loss: 3.1250 - value_output_mse: 3.1259\n",
            "Epoch 1: loss improved from inf to 12.19198, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 51s/step - loss: 12.1920 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0670 - value_output_loss: 3.1250 - value_output_mse: 3.1259\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 11.7138 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1052 - value_output_loss: 2.6086 - value_output_mse: 2.6088\n",
            "Epoch 1: loss improved from 12.19198 to 11.71383, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 11.7138 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1052 - value_output_loss: 2.6086 - value_output_mse: 2.6088  \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 12.0015 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0734 - value_output_loss: 2.9281 - value_output_mse: 2.9282\n",
            "Epoch 1: loss did not improve from 11.71383\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - loss: 12.0015 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0734 - value_output_loss: 2.9281 - value_output_mse: 2.9282\n",
            "\n",
            "Ortalama Policy Output Loss: 9.081884702046713\n",
            "Ortalama Value Output Loss: 2.8872123559316\n",
            "\n",
            "Iteration 2/7000\n",
            "False r1bq3r/ppppkQbp/n3P2p/4P3/5P2/2N5/PPP3PP/2KR1BNR\n",
            "True r3k1rb/pp3p1p/n1p1b3/3p1n1K/7q/8/PPPPPP1P/R1BQ4\n",
            "False 1rbq1kr1/pppppQbP/8/3NN3/5B2/3B4/PPP2PPP/R3K2R\n",
            "True r3k2r/ppp2pp1/2nbb2p/4p3/4n3/6qP/1PPPP3/2BQKB1R\n",
            "False r3kb2/pp1Qp1r1/4Pppp/3P4/1P5P/P1N2N2/2P1KPP1/R6R\n",
            "False 1rbq1br1/pppkppp1/7p/1B3P2/8/2N2N2/PPP2PPP/R1BQK2R\n",
            "True r3k2r/1pp3p1/p1n1b2p/2b1p3/8/3P4/PPPP1qBn/1RBQK3\n",
            "False 1rb4k/p1Bp3Q/1p2pNp1/4P3/8/5N2/PPP2PPP/R3R1K1\n",
            "True r3k2r/1p3pp1/1pn2n1p/1p2q3/2b5/3K1PbP/1PPP2P1/2BQ3R\n",
            "False r1b1kbr1/ppN1pppp/7n/8/P3PB2/5N2/1PP2PPP/3RKB1R\n",
            "True r3kb1r/ppp2ppp/2n1b3/3n4/2p4q/2P2P2/PP1PP1pP/1RBQKBR1\n",
            "True r2q2k1/1pp3pp/3b1p2/8/p5nP/3p2Pb/PPPP1P2/R1B1r2K\n",
            "True r5k1/ppp2ppp/2nb1n2/8/6b1/3p4/PPPPqPPP/1RB1K2R\n",
            "True 2r1kb1r/1p3ppp/p4n2/3bp3/8/3PpKPP/PP3P1R/4q2B\n",
            "True r1b1k2r/1p3pb1/p1n5/7p/3p2n1/7K/PPPPP2q/1RBQ4\n",
            "Girdi\n",
            "Loaded existing validation data from validation_data.npy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 12.2417 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1622 - value_output_loss: 3.0795 - value_output_mse: 3.0787\n",
            "Epoch 1: loss improved from inf to 12.24167, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 12.2417 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1622 - value_output_loss: 3.0795 - value_output_mse: 3.0787  \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 12.0662 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1365 - value_output_loss: 2.9297 - value_output_mse: 2.9295\n",
            "Epoch 1: loss improved from 12.24167 to 12.06624, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 12.0662 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1365 - value_output_loss: 2.9297 - value_output_mse: 2.9295  \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 11.9755 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0359 - value_output_loss: 2.9396 - value_output_mse: 2.9397\n",
            "Epoch 1: loss improved from 12.06624 to 11.97546, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 11.9755 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0359 - value_output_loss: 2.9396 - value_output_mse: 2.9397  \n",
            "\n",
            "Ortalama Policy Output Loss: 9.11152172088623\n",
            "Ortalama Value Output Loss: 2.982933680216471\n",
            "\n",
            "Iteration 3/7000\n",
            "False rnbqkb1r/ppppp2p/5p1p/7Q/2BPP3/2N5/PPP2PPP/R3K1NR\n",
            "True rn2k1nr/pp2bppp/3Q4/1bpp4/4p3/4P3/1PPP1PPP/2q1K2R\n",
            "False b1rqkbr1/p1pppp1p/2p2Np1/5P2/1PP5/3B1N2/P4PPP/R1BQR1K1\n",
            "True r3kb1r/ppp2ppp/2n2n2/4p3/2p5/6Pb/PPPPPP1P/1RBQKq2\n",
            "True 1nb1k2r/3r1ppp/2p5/1p2p3/1b2n3/BP2P2P/P1PPPq2/1R1QKB2\n",
            "True r3k2r/ppp2ppp/2n2n2/8/6b1/3P1qb1/1PPP4/2BQK3\n",
            "False 1rb1Qk1r/pppp1ppp/2n5/2P3B1/8/2NB1N2/PPP2PPP/R3R1K1\n",
            "False 1rbq1b1r/ppp1pppp/5k2/5P2/2B1NQ2/8/PPP2PPP/R1B2RK1\n",
            "False 1rk2br1/ppp1p1pp/4Q3/1B3P2/8/5N2/PPP2PPP/R1BR2K1\n",
            "True r2qr1k1/p4ppp/4bnQ1/1pp5/4pb2/7P/PPn1PPP1/1RB1KB1R\n",
            "True r3k2r/ppp3pp/2nb1n2/7q/3p1K2/4P2P/PPPP1P1P/R1B5\n",
            "False r1bk3r/ppppQppp/n5N1/8/2PP4/P2B1N2/1P3PPP/R1B1R1K1\n",
            "False r1b2b1r/ppp3pp/8/4pkQ1/2B5/2N1B3/PPP2PPP/R3K2R\n",
            "True 3rk1n1/ppp2ppr/2n1b3/8/8/4P1bP/1PPPKq2/2BQ1B2\n",
            "True r2r4/1pp1kppp/2nb1n2/p3p3/8/3p2Pb/PPPPPPR1/1RBQ1K1q\n",
            "Girdi\n",
            "Loaded existing validation data from validation_data.npy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 11.8976 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.9616 - value_output_loss: 2.9360 - value_output_mse: 2.9357\n",
            "Epoch 1: loss improved from inf to 11.89758, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 11.8976 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.9616 - value_output_loss: 2.9360 - value_output_mse: 2.9357  \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 6.4921 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.4921 - value_output_loss: 1.4102e-05 - value_output_mse: 1.4102e-05\n",
            "Epoch 1: loss improved from 11.89758 to 6.49209, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 6.4921 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.4921 - value_output_loss: 1.4102e-05 - value_output_mse: 1.4102e-05  \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 6.5785 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.5785 - value_output_loss: 1.4102e-05 - value_output_mse: 1.4102e-05\n",
            "Epoch 1: loss did not improve from 6.49209\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 6.5785 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 6.5785 - value_output_loss: 1.4102e-05 - value_output_mse: 1.4102e-05\n",
            "\n",
            "Ortalama Policy Output Loss: 7.344045639038086\n",
            "Ortalama Value Output Loss: 0.9786727666857283\n",
            "\n",
            "Iteration 4/7000\n",
            "True rnbq2nr/pp3kpp/4pp2/2pp3P/8/5Pb1/PPPPP1P1/RNBQKB1R\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'is_expanded'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-80a55ff86596>\u001b[0m in \u001b[0;36m<cell line: 285>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m \u001b[0mtrain_sigmachess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games_per_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-80a55ff86596>\u001b[0m in \u001b[0;36mtrain_sigmachess\u001b[0;34m(model, num_iterations, num_games_per_iteration)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration {iteration + 1}/{num_iterations}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mreplay_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games_per_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-80a55ff86596>\u001b[0m in \u001b[0;36mself_play\u001b[0;34m(model, num_games, max_workers)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Girdi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-80a55ff86596>\u001b[0m in \u001b[0;36mplay_vs_stockfish\u001b[0;34m(model, game, replay_buffer)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mmcts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mw_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-473b9f420f8d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, initial_state, temperature)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;31m# Selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_expanded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_puct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'is_expanded'"
          ]
        }
      ]
    }
  ]
}