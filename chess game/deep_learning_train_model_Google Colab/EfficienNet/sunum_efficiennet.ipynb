{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"t4jK41UULfwI"},"outputs":[],"source":["#ULTRA SUPER SIGMA CHESS AI"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gfMjit5oLfwI","outputId":"050dd4a2-d9e7-4752-cac7-7adc31fa7a6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting chess\n","  Downloading chess-1.11.1.tar.gz (156 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/156.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: chess\n","  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for chess: filename=chess-1.11.1-py3-none-any.whl size=148497 sha256=d7a6465571b1698e0bd8ce7d9cf824613386a55243395262e81b108379d0f255\n","  Stored in directory: /root/.cache/pip/wheels/2e/2d/23/1bfc95db984ed3ecbf6764167dc7526d0ab521cf9a9852544e\n","Successfully built chess\n","Installing collected packages: chess\n","Successfully installed chess-1.11.1\n"]}],"source":["!pip install chess"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RYnWlePILfwJ","outputId":"3cc96e36-73e6-4d70-ea35-edf055b9d8db"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!chmod +x \"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBlKKoQmLfwJ","outputId":"bb62b22c-eb3c-409d-b8a5-78dc26813fa4"},"outputs":[{"output_type":"stream","name":"stdout","text":["All devices:  [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import math\n","\n","import chess\n","\n","from collections import deque\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import HeNormal\n","\n","import chess.engine\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","import threading\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","class GameState:\n","    row = 8\n","    col = 8\n","    promotion_indexes = {\n","        chess.KNIGHT: 0,\n","        chess.ROOK: 1,\n","        chess.BISHOP: 2\n","    }\n","\n","    def __init__(self) -> None:\n","        self.board = chess.Board()\n","        self.repetition_count = 0\n","        self.player_color: chess.Color = chess.WHITE\n","\n","    def get_initial_state(self):\n","        self.board.reset()\n","\n","        return self.get_current_state()\n","\n","    def get_current_state(self, T=8):\n","        input_tensor = np.zeros((8, 8, 119), dtype=np.uint8)\n","\n","        for t in range(T):\n","            _t = T - t - 1\n","            if len(self.board.move_stack) < _t:\n","                continue\n","\n","            self.create_input(input_tensor, _t)\n","\n","        color = 0 if self.board.turn == chess.WHITE else 1\n","        input_tensor[:, :, 112] = color\n","\n","        input_tensor[:, :, 113] = len(self.board.move_stack) > 0\n","\n","        p1_castling = (1 * self.board.has_kingside_castling_rights(chess.WHITE)) | (2 * self.board.has_queenside_castling_rights(chess.WHITE))\n","        p1_castling_bit = format(p1_castling, \"02b\")\n","        input_tensor[:, :, 114] = int(p1_castling_bit[0])\n","        input_tensor[:, :, 115] = int(p1_castling_bit[1])\n","\n","        p2_castling = (1 * self.board.has_kingside_castling_rights(chess.BLACK)) | (2 * self.board.has_queenside_castling_rights(chess.BLACK))\n","        p2_castling_bit = format(p2_castling, \"02b\")\n","        input_tensor[:, :, 116] = int(p2_castling_bit[0])\n","        input_tensor[:, :, 117] = int(p2_castling_bit[1])\n","\n","        input_tensor[:, :, 118] = int(self.board.is_fifty_moves())\n","\n","        return np.expand_dims(input_tensor, axis=0)\n","\n","    def get_next_state(self, action: int):\n","        source_index = action // 73\n","        destination_index = 0\n","        move_type = action % 73\n","\n","        promotion = None\n","\n","        if move_type < 56:\n","            direction = move_type // 7\n","            movement = (move_type % 7) + 1\n","\n","            destination_index = source_index + (movement * 8) if direction == 0 else destination_index\n","            destination_index = source_index + (movement * 9) if direction == 1 else destination_index\n","            destination_index = source_index + movement if direction == 2 else destination_index\n","            destination_index = source_index + (movement * -7) if direction == 3 else destination_index\n","            destination_index = source_index + (movement * -8) if direction == 4 else destination_index\n","            destination_index = source_index + (movement * -9) if direction == 5 else destination_index\n","            destination_index = source_index + (-movement) if direction == 6 else destination_index\n","            destination_index = source_index + (movement * 7) if direction == 7 else destination_index\n","        elif move_type >= 56 and move_type < 64:\n","            direction = move_type - 56\n","\n","            destination_index = source_index + 17 if direction == 0 else destination_index\n","            destination_index = source_index + 10 if direction == 1 else destination_index\n","            destination_index = source_index - 6 if direction == 2 else destination_index\n","            destination_index = source_index - 15 if direction == 3 else destination_index\n","            destination_index = source_index - 17 if direction == 4 else destination_index\n","            destination_index = source_index - 10 if direction == 5 else destination_index\n","            destination_index = source_index + 6 if direction == 6 else destination_index\n","            destination_index = source_index + 15 if direction == 7 else destination_index\n","        else:\n","            direction = (move_type - 64) // 3\n","            promotion_index = (move_type - 64) % 3\n","\n","            promotion = chess.KNIGHT if promotion_index == 0 else (chess.ROOK if promotion_index == 1 else chess.BISHOP)\n","\n","            color_value = 1 if self.board.turn == chess.WHITE else -1\n","\n","            if direction == 0:\n","                destination_index = source_index + (8 * color_value)\n","            elif direction == 1:\n","                destination_index = source_index + (9 * color_value)\n","            else:\n","                destination_index = source_index + (7 * color_value)\n","\n","        from_square = chess.Square(source_index)\n","        to_square = chess.Square(destination_index)\n","\n","        promotion_rank = 7 if self.board.turn == chess.WHITE else 0\n","\n","        if promotion is None:\n","            if self.board.piece_type_at(from_square) == chess.PAWN and chess.square_rank(to_square) == promotion_rank:\n","                promotion = chess.QUEEN\n","\n","        move = chess.Move(from_square, to_square, promotion)\n","\n","        self.apply_action(move)\n","\n","        return move, self.get_current_state()\n","\n","    def apply_action(self, move: chess.Move):\n","        try:\n","            self.board.push(move)\n","        except Exception as e:\n","            print(list(self.board.legal_moves))\n","            print(self.get_valid_moves())\n","\n","            print(e)\n","\n","            raise Exception(\"Error\")\n","\n","    def create_input(self, input_tensor: np.ndarray, t: int):\n","        piece_types = {\n","            chess.PAWN: 0,\n","            chess.KNIGHT: 1,\n","            chess.BISHOP: 2,\n","            chess.ROOK: 3,\n","            chess.QUEEN: 4,\n","            chess.KING: 5\n","        }\n","\n","        board = self.board.copy()\n","        for _ in range(t):\n","            board.pop()\n","\n","        transposition_key = board._transposition_key()\n","\n","        for square in chess.SQUARES:\n","            piece = board.piece_at(square)\n","\n","            if piece is None:\n","                continue\n","\n","            piece_index = piece_types[piece.piece_type]\n","            piece_color = 0 if piece.color == chess.WHITE else 1\n","\n","            index = (t * 14) + (piece_color * 6) + piece_index\n","            input_tensor[square // 8][square % 8][index] = 1\n","\n","        repetition_count = 0\n","        index = (t * 14) + 12\n","\n","        try:\n","            while board.move_stack:\n","                move = board.pop()\n","                if board.is_irreversible(move):\n","                    break\n","\n","                if board._transposition_key() == transposition_key:\n","                    repetition_count += 1\n","\n","                if repetition_count == 3:\n","                    break\n","        finally:\n","            repetition_count = 3 if repetition_count > 3 else repetition_count\n","\n","            repetition_count_bits = [int(x) for x in format(repetition_count, \"02b\")]\n","            input_tensor[:, :, index] = repetition_count_bits[0]\n","            input_tensor[:, :, index + 1] = repetition_count_bits[1]\n","\n","    def get_valid_moves(self):\n","        legal_moves = []\n","\n","        for valid_move in self.board.legal_moves:\n","            s_row, s_col, from_square_index = self.index_of_square(valid_move.from_square)\n","            d_row, d_col, to_square_index = self.index_of_square(valid_move.to_square)\n","\n","            if valid_move.promotion:\n","                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n","\n","                if valid_move.promotion == chess.QUEEN:\n","                    index = (from_square_index * 73) + (direction * 7)\n","                    legal_moves.append(index)\n","                else:\n","                    promotion_index = self.promotion_indexes[valid_move.promotion]\n","\n","                    if direction > 2 and direction < 6:\n","                        direction = 0 if direction == 4 else (1 if direction == 5 else 2)\n","                    elif direction == 7:\n","                        direction = 2\n","\n","                    index = (from_square_index * 73) + ((direction * 3) + promotion_index + 64)\n","                    legal_moves.append(index)\n","            elif self.board.piece_type_at(valid_move.from_square) == chess.KNIGHT:\n","                direction = self.direction_of_move_for_knights(s_row, s_col, d_row, d_col)\n","\n","                index = (from_square_index * 73) + direction + 56\n","                legal_moves.append(index)\n","\n","            else:\n","                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n","                count_of_square = self.count_of_square_for_movement(s_row, s_col, d_row, d_col) - 1\n","\n","                index = (from_square_index * 73) + ((direction * 7) + count_of_square)\n","                legal_moves.append(index)\n","\n","        return legal_moves\n","\n","    def index_of_square(self, square: chess.Square):\n","        row = chess.square_rank(square)\n","        col = chess.square_file(square)\n","        index = (row * 8) + col\n","\n","        return row, col, index\n","\n","    def direction_of_move_for_ray_directions(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        if delta_x == 0:\n","            return 0 if delta_y > 0 else 4\n","\n","        if delta_y == 0:\n","            return 2 if delta_x > 0 else 6\n","\n","        if delta_x < 0:\n","            return 7 if delta_y > 0 else 5\n","\n","        return 1 if delta_y > 0 else 3\n","\n","    def direction_of_move_for_knights(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        if delta_x == 1:\n","            return 0 if delta_y > 0 else 3\n","\n","        if delta_x == 2:\n","            return 1 if delta_y > 0 else 2\n","\n","        if delta_x == -1:\n","            return 7 if delta_y > 0 else 4\n","\n","        return 6 if delta_y > 0 else 5\n","\n","    def count_of_square_for_movement(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        return max(abs(delta_x), abs(delta_y))\n","\n","    def get_winner(self):\n","        result = self.board.result()\n","\n","        if result == \"1-0\":\n","            return chess.WHITE\n","\n","        if result == \"0-1\":\n","            return chess.BLACK\n","\n","        return 2\n","\n","    def is_terminal(self):\n","        return self.board.is_game_over()\n","\n","    def clone(self):\n","        cloned_state = GameState()\n","        cloned_state.board = self.board.copy()\n","\n","        return cloned_state\n","\n","class Node:\n","    def __init__(self, state, parent=None, prior_prob=1.0):\n","        self.state = state\n","        self.parent = parent\n","        self.children = {}\n","        self.visits = 0\n","        self.value_sum = 0\n","        self.prior_prob = prior_prob\n","        self.is_expanded = False\n","\n","    @property\n","    def value(self):\n","        return self.value_sum / (self.visits + 1e-5)\n","\n","    def expand(self, action_probs):\n","        for action, prob in enumerate(action_probs):\n","            if prob > 0:\n","                next_state = self.state.clone()\n","                next_state.get_next_state(action)\n","                self.children[action] = Node(next_state, parent=self, prior_prob=prob)\n","\n","        if len(self.children) > 0:\n","            self.is_expanded = True\n","\n","    def select(self, c_puct=1.0):\n","        max_ucb = -float('inf')\n","        best_action = None\n","        best_child = None\n","\n","        for action, child in self.children.items():\n","            ucb = child.value + c_puct * child.prior_prob * (math.sqrt(self.visits) / (1 + child.visits))\n","            if ucb > max_ucb:\n","                max_ucb = ucb\n","                best_action = action\n","                best_child = child\n","        return best_action, best_child\n","\n","    def backup(self, value):\n","        self.visits += 1\n","        self.value_sum += value\n","        if self.parent:\n","            self.parent.backup(-value)\n","\n","class MCTS:\n","    def __init__(self, model, c_puct=1.0, simulations=50):\n","        self.model = model\n","        self.c_puct = c_puct\n","        self.simulations = simulations\n","\n","    def add_dirichlet_noise(self, node, valid_moves):\n","        noise = np.random.dirichlet([0.3] * len(valid_moves))\n","        for idx, action in enumerate(valid_moves):\n","            if action in node.children:\n","                node.children[action].prior_prob = \\\n","                    0.75 * node.children[action].prior_prob + 0.25 * noise[idx]\n","\n","    def run(self, initial_state, temperature=1.0):\n","        root = Node(initial_state)\n","\n","        # First evaluate and expand root\n","        action_probs, value = self.evaluate(initial_state)\n","        valid_moves = initial_state.get_valid_moves()\n","\n","        # Add Dirichlet noise to root (alpha=0.3 for chess)\n","        noise = np.random.dirichlet([0.3] * len(valid_moves))\n","\n","        # Expand with noisy priors\n","        for idx, action in enumerate(valid_moves):\n","            prob = action_probs[action]\n","            noisy_prob = 0.75 * prob + 0.25 * noise[idx]\n","            next_state = initial_state.clone()\n","            next_state.get_next_state(action)\n","            root.children[action] = Node(next_state, parent=root, prior_prob=noisy_prob)\n","\n","        for _ in range(self.simulations):\n","            node = root\n","\n","            # Selection\n","            while node.is_expanded and not node.state.is_terminal():\n","                action, node = node.select(self.c_puct)\n","\n","            # Expansion and Evaluation\n","            if not node.state.is_terminal():\n","                action_probs, value = self.evaluate(node.state)\n","                valid_moves = node.state.get_valid_moves()\n","                node.expand(action_probs)\n","            else:\n","                value = node.state.get_winner()\n","                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","\n","            # Backup\n","            node.backup(value)\n","\n","        return self.get_action_probs(root, temperature)\n","\n","    def evaluate(self, state):\n","        state_tensor = state.get_current_state()\n","        # state_tensor = np.expand_dims(state_tensor, axis=0)\n","\n","        policy, value = self.model.predict(state_tensor, verbose=0)\n","\n","        # Mask invalid moves\n","        valid_moves = state.get_valid_moves()\n","        mask = np.zeros(policy.shape[1])\n","        mask[valid_moves] = 1\n","\n","        policy = policy[0] * mask\n","\n","        # Normalize\n","        sum_policy = np.sum(policy)\n","        if sum_policy > 0:\n","            policy /= sum_policy\n","        else:\n","            # If all moves were masked, use uniform distribution over valid moves\n","            policy = mask / np.sum(mask)\n","\n","        return policy, value[0][0]\n","\n","    def get_action_probs(self, root, temperature=1.0):\n","        visits = np.array([child.visits for action, child in root.children.items()])\n","        actions = list(root.children.keys())\n","\n","        if temperature == 0:  # Pure exploitation\n","            action_idx = np.argmax(visits)\n","            probs = np.zeros_like(visits)\n","            probs[action_idx] = 1\n","        else:\n","            # Apply temperature\n","            visits = visits ** (1 / temperature)\n","            probs = visits / np.sum(visits)\n","\n","        # Convert to full move probability vector\n","        full_probs = np.zeros(4672)  # Adjust size based on your action space\n","        for action, prob in zip(actions, probs):\n","            full_probs[action] = prob\n","\n","        return full_probs\n","\n","class ReplayBuffer:\n","    def __init__(self, maxlen=500000):\n","        self.buffer = deque(maxlen=maxlen)\n","        self.current_size = 0\n","        self.lock = threading.Lock()\n","\n","    def store(self, state, policy, value):\n","        \"\"\"Store a single game state transition\"\"\"\n","        self.buffer.append({\n","            'state': state,\n","            'policy': policy,\n","            'value': value\n","        })\n","        self.current_size = len(self.buffer)\n","\n","    def store_multiple_data(self, states, policies, value):\n","        with self.lock:\n","            for s, p, v in zip(states, policies, [value]):\n","                self.store(s, p, v)\n","\n","    def sample(self, batch_size):\n","        \"\"\"Sample a batch with augmentations\"\"\"\n","        if self.current_size < batch_size:\n","            batch_size = self.current_size\n","\n","        indices = np.random.choice(self.current_size, batch_size)\n","        states, policies, values = [], [], []\n","\n","        for idx in indices:\n","            sample = self.buffer[idx]\n","            # Get augmented samples\n","            aug_states, aug_policies = self._augment_sample(\n","                sample['state'],\n","                sample['policy']\n","            )\n","\n","            # Add all augmentations\n","            states.extend(aug_states)\n","            policies.extend(aug_policies)\n","            values.extend([sample['value']] * len(aug_states))\n","\n","        return np.array(states), np.array(policies), np.array(values)\n","\n","    def _augment_sample(self, state, policy):\n","        \"\"\"Generate valid augmentations for a single sample\"\"\"\n","        # Remove batch dimension if present\n","        if len(state.shape) == 4:\n","            state = np.squeeze(state, axis=0)\n","\n","        augmented_states = [state]\n","        augmented_policies = [policy]\n","\n","        # Horizontal flip\n","        flip_h = np.flip(state, axis=1)\n","        augmented_states.append(flip_h)\n","        augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        # Vertical flip\n","        flip_v = np.flip(state, axis=0)\n","        augmented_states.append(flip_v)\n","        augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        # Diagonal flip (only if shape allows)\n","        if state.shape[0] == state.shape[1]:\n","            diag = np.transpose(state, (1, 0, 2))\n","            augmented_states.append(diag)\n","            augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        return augmented_states, augmented_policies\n","\n","    def __len__(self):\n","        return self.current_size\n","\n","# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","# tf.tpu.experimental.initialize_tpu_system(tpu)\n","# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n","\n","print(\"All devices: \", tf.config.list_logical_devices('GPU'))\n","\n","policy = tf.keras.mixed_precision.Policy('mixed_float16')\n","tf.keras.mixed_precision.set_global_policy(policy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEEbn7CxLfwL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vsoo7SmaO1tk","outputId":"f5d4d9ea-dfba-4975-f733-f4e889192038"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1/7000\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b6d27b8d870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["False rnbq2nr/1p1ppN1k/5PQb/p1p5/3P4/2NB4/PPP2PPP/R1B1K2R\n","True r3k1nr/ppp2ppp/2n5/4p2B/1b2p3/1P1qK1P1/P1b2P1P/R5NR\n","False 3Q1knr/1p3p1p/B3P2P/p1p1P3/8/4BN2/PPP2PP1/RN2K2R\n","True r1b4r/pp2kppp/2p2n2/8/P3Pb1P/1pNq1p2/1Pn2PP1/3K2NR\n","False 1n3k2/1pNb2b1/6Nn/p1pQP1Bp/4P3/8/PPP2PPP/R3KB1R\n","True r2r2k1/ppp1bppp/4b3/1P6/n3P3/P1K2PP1/1r6/7n\n","True 1nb2rk1/1pp2ppp/r2b1q2/8/1p2P3/P1n1PK1P/R1P2PP1/1Q3BNR\n","False rnbq3r/pp1pp3/5bQk/2p5/8/3B4/PPPP1PPP/RNB1K1NR\n","False rnbq1b1r/ppp2p1p/2N4n/2kPQ1p1/N7/8/PPPP1PPP/R1B1KB1R\n","False 2k2bn1/3Rp1pr/1N2B3/2p2P1p/p4B2/N7/PPP2PPP/4K2R\n","True r1b1kb1r/ppp2ppp/4p3/3p4/3nnP2/1P2P3/P2P1qPP/RNB2KNR\n","True rnb1kbnr/pppp1ppp/8/8/P6q/2P5/1P1PP1pP/RNBQKBNR\n","True r2q2n1/1bp1k1p1/np1b1p2/4p1p1/2PP2q1/P4P1r/1Pp4K/R1B5\n","True rnb2rk1/p1p2ppp/3b3n/1p1p4/1P3q2/3PK2P/P1P1P1PR/RN1Q1BN1\n","True r1q1k1nr/ppp3pp/2bb1p2/8/PP3p2/3P1KPP/2n1N3/4q1NR\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85s/step - loss: 8.7186 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.3259 - value_output_loss: 0.3928 - value_output_mse: 0.3928\n","Epoch 1: loss improved from inf to 8.71862, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 93s/step - loss: 8.7186 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.3259 - value_output_loss: 0.3928 - value_output_mse: 0.3928\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 8.6410 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.3226 - value_output_loss: 0.3184 - value_output_mse: 0.3184\n","Epoch 1: loss improved from 8.71862 to 8.64099, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 8.6410 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.3226 - value_output_loss: 0.3184 - value_output_mse: 0.3184   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 8.6412 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.3227 - value_output_loss: 0.3185 - value_output_mse: 0.3186\n","Epoch 1: loss did not improve from 8.64099\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 8.6412 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.3227 - value_output_loss: 0.3185 - value_output_mse: 0.3186\n","\n","Average Policy Output Loss: 8.323705355326334\n","Average Value Output Loss: 0.3432355026404063\n","Iteration 2/7000\n","False r1bnkbr1/p1Nppppp/7n/2P5/4P3/5N2/P1P2PPP/R1BQKB1R\n","True r3kbnr/pp3ppp/2p5/4p3/4p3/2P1n2P/PP3PP1/RN1qKBNR\n","True rn1qk2r/ppp3pp/4bn2/5p2/2p4P/P4Pb1/RPp1P3/1NB1KBNR\n","True rnb1kbnr/ppp2ppp/8/3pp3/8/2P2PqP/PP1PP3/RNBQKBNR\n","False r1bk1Qnr/p1pp1p2/8/1B2P2p/4P1P1/N4N2/PPP2PP1/R3K2R\n","False rnb2bn1/2pp2p1/8/p3P2p/2kQ4/P1N1B3/1PP2PPP/R4RK1\n","False 1r2kb1r/p2Q2pp/5q1B/1Bp1pp2/4P3/2N2N2/PPP2PPP/R3K2R\n","True r1b1kbnr/pppp1ppp/2n5/4p3/3q4/1P2KP2/P1PPP1PP/RNBQ1BNR\n","False r1bq1b1r/ppp1pQ1p/2P2k1B/5pp1/2BP4/2N5/PPP2PPP/R3K1NR\n","True rnb1r1k1/ppp2ppp/3b4/8/P1P2qnP/1P1p1K2/6PR/RN1Q1BN1\n","True r2q1r1k/ppp1bppp/8/3pP3/4n1bP/PPPK2P1/3q1P1R/R4BN1\n","True r1b1k2r/ppp2ppp/2n2n2/2b1p3/2P1P3/PP3PPP/5qK1/2r2BNR\n","False r1bq1bn1/p1pkp1p1/2Q5/1B1P4/3P4/4B3/PPP2PPP/RN2K1NR\n","False r4b2/pppb3p/1k4p1/B2Q4/2B5/8/PPP2PPP/R3K1NR\n","True rnb1k2r/ppp2pp1/3b4/3pp3/PPP5/2n1P1pP/3P1qK1/R1B2BNR\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 8.8925 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2344 - value_output_loss: 0.6582 - value_output_mse: 0.6580\n","Epoch 1: loss improved from inf to 8.89254, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step - loss: 8.8925 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2344 - value_output_loss: 0.6582 - value_output_mse: 0.6580 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 8.5969 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2769 - value_output_loss: 0.3200 - value_output_mse: 0.3200\n","Epoch 1: loss improved from 8.89254 to 8.59686, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 8.5969 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2769 - value_output_loss: 0.3200 - value_output_mse: 0.3200   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 8.6459 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2545 - value_output_loss: 0.3914 - value_output_mse: 0.3915\n","Epoch 1: loss did not improve from 8.59686\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 8.6459 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2545 - value_output_loss: 0.3914 - value_output_mse: 0.3915\n","\n","Average Policy Output Loss: 8.255235036214193\n","Average Value Output Loss: 0.4565158784389496\n","Iteration 3/7000\n","False rn3bnr/p2pq1pp/b3P3/1BpN2kQ/5N2/8/PPPP1PPP/R1B1R1K1\n","True r1b1k2r/pp3ppp/3b4/3pp3/PP1P1q2/1Q3KnP/3P2P1/RNB2BNR\n","True r1b3k1/ppp3pp/3b4/3p4/1nP1n1pP/PP2P3/4Kq2/RNB2rNR\n","False r1bq1b1r/p1p1pp1p/1pBk3p/1N1P4/3P4/8/PPP2PPP/R2QK1NR\n","False 2b5/p1p2p1p/r1p5/kB6/1P2P3/N1P1B3/P3NPPP/3RK2R\n","True r3k1nr/p1p3p1/p1n2p2/7p/P2P2q1/b3BK1N/5P1P/1q5R\n","False rn4nr/ppQp3p/3k4/2pNp1N1/1q2P3/8/PPPP1PPP/R1B1KB1R\n","False 2b2br1/4q1pp/4r3/pB1QNk2/4N1P1/4B3/PPP2P1P/R3K2R\n","False r1bqkbnr/2ppp2p/4P3/pB3pBQ/4P3/8/PPP2PPP/RN2K1NR\n","True r1b1k2r/ppp2ppp/8/2b1p3/P7/1Qpn4/3Pq1PP/R1n3KR\n","True r4rk1/1p1b1ppp/4p2n/2pp4/P2q2Pb/1p2KP1P/3NP1BR/R5N1\n","True r2qkb1r/pp3ppp/8/2p5/P3n1b1/1p1PP1p1/1Pn4P/RNB1KBNR\n","True r1b1k1nr/ppp2ppp/2n5/3p4/2PP4/4p1q1/PP2P3/RN1QKBNR\n","True 4r1k1/p1p2pp1/1rnb1n1p/p5P1/2p2K2/PP1q3P/5P2/RNB3Nb\n","True 1r2r1k1/p2n1pp1/2p2n1p/3pp2P/p1P3b1/3PP3/Pq3P2/bK4NR\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 8.7365 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.3886 - value_output_loss: 0.3478 - value_output_mse: 0.3479\n","Epoch 1: loss improved from inf to 8.73646, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 8.7365 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.3886 - value_output_loss: 0.3478 - value_output_mse: 0.3479 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 8.9443 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2606 - value_output_loss: 0.6837 - value_output_mse: 0.6842\n","Epoch 1: loss did not improve from 8.73646\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 8.9443 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.2606 - value_output_loss: 0.6837 - value_output_mse: 0.6842\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - loss: 8.1448 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.1448 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","Epoch 1: loss improved from 8.73646 to 8.14475, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 8.1448 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.1448 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00 \n","\n","Average Policy Output Loss: 8.264655113220215\n","Average Value Output Loss: 0.3438646097977956\n","Iteration 4/7000\n","False r1bqkb1N/1p1ppQ1p/2n5/p1p5/3P4/8/PPP2PPP/RNB1KBNR\n","True r3kb1r/ppp2ppp/2n1b3/1P1pp3/4n3/P2P1P1P/R1P1Pq2/1NBQKBN1\n","False 1Q2kb2/p2b1p1r/5B1n/1Bp1P2p/4P3/2N5/PPP2PPP/R3K1NR\n","False rn2k1r1/1b1pQ3/8/pBP2pBp/7P/5N2/PPPN1PP1/R2QK2R\n","False 1rb1kbnr/p1B2Q1p/2n5/1p1pN3/3pP3/8/PPP2PPP/RN2KB1R\n","True r1bqk1nr/p1pp1ppp/1pn5/4p3/P7/2P2PbP/3PP3/RNBQKBNR\n","True r2k2nr/pppnq1pp/5pb1/1P4P1/P7/R1P2PbP/3K4/1NB1q1N1\n","False 2bqk1Q1/1ppppp2/r7/p5Bp/4P3/2N5/PPP2PPP/R3KBNR\n","True r2qk2r/1pp1nppp/pb2b3/8/R7/3p1PPP/2nK4/1NB1qBNR\n","True r3kbnr/ppp2ppp/8/PP2p3/5P2/3q2P1/2nNb2P/2BK3R\n","True rnbqk1nr/pppp1ppp/8/8/1b6/6PP/P2QPP2/RNq1KBNR\n","False 2Q2b1r/4kp1p/5P2/p2pB1p1/8/5N2/PPP2PPP/R4RK1\n","False B3Qk1r/6bp/7n/p1pP1pB1/8/5N2/PPP2PPP/RN2R1K1\n","False 5k1r/3QB1p1/N7/p3pp1p/2B1P3/8/PPP2PPP/R3K1NR\n","True r5r1/pp2kp1p/n1pp1n1P/4pB2/7p/2qK1b2/3b4/8\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 7.7992 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.7992 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","Epoch 1: loss improved from inf to 7.79921, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 7.7992 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.7992 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 7.7956 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.7956 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","Epoch 1: loss improved from 7.79921 to 7.79558, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 7.7956 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.7956 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 7.7917 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.7917 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","Epoch 1: loss improved from 7.79558 to 7.79169, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 7.7917 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 7.7917 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00   \n","\n","Average Policy Output Loss: 7.795492490132649\n","Average Value Output Loss: 0.0\n","Iteration 5/7000\n","False r1bqkbnr/ppppp3/2n3B1/5p2/3PP3/8/PPP2PPP/RNBQK1NR\n","False rn1N1bnr/1p1b2pp/8/p1pk1P2/2P2Q2/8/PP1P1PPP/R1B1KB1R\n","False 4kb1r/2p1p1pp/n1B5/p2QNP2/8/4B3/PPP2PPP/RN2K2R\n","False r4k1r/2pP4/4N1Qn/pp1Npp2/4P3/3B4/PPP2PPP/R3K2R\n","False r1bqkbnr/p2ppQ1p/2n5/1pp3N1/4P3/8/PPPP1PPP/RNB1KB1R\n","False 7r/4QBkp/5B1n/p1p1P3/4P2P/2N5/PPP1NPP1/R3K2R\n","True r2qk3/1pp3p1/p3b3/P3n2r/4ppn1/1P3p1K/2pb3R/RNB2BN1\n","True r1bqkb1r/1pp2ppp/8/3pp3/3nP1nP/P2P1PP1/4K3/RNq2BNR\n","False 3Bkbn1/1p1Q1pp1/2r5/2p1N2p/4P3/8/PPP2PPP/RN2KB1R\n","True rn2k2r/ppp2ppp/5n2/4p3/P1Pbb3/1P2KP1P/R3B3/1NB1q1NR\n","False rnbq1b1r/1p2p1p1/3P3n/p1P1kpBQ/1PB5/2N2N2/P1P2PPP/R3K2R\n","True r3k1nr/ppp3pp/2n1b3/4pp2/2P2P1P/P2pb1P1/7R/1q1K1BN1\n","True rnbqk2r/ppp2ppp/8/3pp2n/5P2/PP4b1/2PPP3/RNBQKBNR\n","False rnbq1bnr/ppp2Q2/3k4/1N1pNPpp/3P4/8/PPP2PPP/R1B1KB1R\n","False 5bkr/5Q2/4B2p/p1p1NPp1/8/2NP3P/PPP2PP1/1RB1R1K1\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - loss: 8.2019 - policy_output_accuracy: 0.2667 - policy_output_loss: 8.2019 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","Epoch 1: loss improved from inf to 8.20187, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step - loss: 8.2019 - policy_output_accuracy: 0.2667 - policy_output_loss: 8.2019 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00 \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - loss: 8.2043 - policy_output_accuracy: 0.2833 - policy_output_loss: 8.2043 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","Epoch 1: loss did not improve from 8.20187\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - loss: 8.2043 - policy_output_accuracy: 0.2833 - policy_output_loss: 8.2043 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 8.2115 - policy_output_accuracy: 0.3000 - policy_output_loss: 8.2115 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","Epoch 1: loss did not improve from 8.20187\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 8.2115 - policy_output_accuracy: 0.3000 - policy_output_loss: 8.2115 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","\n","Average Policy Output Loss: 8.205875078837076\n","Average Value Output Loss: 0.0\n","\n","[After 5 Iterations]\n","Average Policy Output Loss: 8.168992614746093\n","Average Value Output Loss: 0.2287231981754303\n","Iteration 6/7000\n"]}],"source":["import numpy as np\n","import threading\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","\n","def EfficientNet_block(inputs, filters=256, expansion_factor=6, stride=1, se_ratio=0.25):\n","    \"\"\"\n","    EfficientNet tarzı bir MBConv bloğu.\n","\n","    Args:\n","        inputs: Giriş tensörü.\n","        filters: Çıkış filtrelerinin sayısı.\n","        expansion_factor: Genişletme oranı (Bottleneck genişletme için).\n","        stride: Adım boyutu (ör. 1 veya 2).\n","        se_ratio: Squeeze-and-Excitation (SE) oranı.\n","\n","    Returns:\n","        EfficientNet MBConv bloğunun çıktısı.\n","    \"\"\"\n","    input_channels = inputs.shape[-1]  # Giriş kanal sayısı\n","\n","    # 1. Expand (Genişletme)\n","    x = layers.Conv2D(input_channels * expansion_factor, (1, 1), strides=1, padding=\"same\",\n","                      kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.ReLU()(x)\n","\n","    # 2. Depthwise Convolution (Derinlikçe Ayrık Evrişim)\n","    x = layers.DepthwiseConv2D((3, 3), strides=stride, padding=\"same\",\n","                               depthwise_initializer=\"he_normal\", use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.ReLU()(x)\n","\n","    # 3. Squeeze-and-Excitation (SE Blok)\n","    if se_ratio:\n","        se = layers.GlobalAveragePooling2D()(x)\n","        se = layers.Reshape((1, 1, x.shape[-1]))(se)\n","        se = layers.Conv2D(int(input_channels * se_ratio), (1, 1), activation=\"relu\", kernel_initializer=\"he_normal\")(se)\n","        se = layers.Conv2D(x.shape[-1], (1, 1), activation=\"sigmoid\", kernel_initializer=\"he_normal\")(se)\n","        x = layers.Multiply()([x, se])\n","\n","    # 4. Project (Daraltma)\n","    x = layers.Conv2D(filters, (1, 1), strides=1, padding=\"same\",\n","                      kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","\n","    # 5. Shortcut Connection (Skip Connection)\n","    if stride == 1 and inputs.shape[-1] == filters:\n","        x = layers.Add()([inputs, x])\n","\n","    return x\n","\n","\n","\n","def sigmachess_network(input_shape=(8, 8, 119)):\n","    inputs = layers.Input(shape=input_shape)\n","\n","    x = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.ReLU()(x)\n","\n","    for _ in range(19):\n","        x = EfficientNet_block(x)\n","\n","    policy = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    policy = layers.BatchNormalization()(policy)\n","    policy = layers.ReLU()(policy)\n","    policy = layers.Conv2D(73, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(policy)\n","    policy = layers.Flatten()(policy)\n","    policy = layers.Softmax(name=\"policy_output\")(policy)\n","\n","    value = layers.Conv2D(1, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    value = layers.BatchNormalization()(value)\n","    value = layers.ReLU()(value)\n","    value = layers.Flatten()(value)\n","    value = layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(value)\n","    value = layers.Dense(1, activation=\"tanh\", name=\"value_output\", kernel_initializer=\"he_normal\")(value)\n","\n","    model = models.Model(inputs=inputs, outputs=[policy, value])\n","\n","    return model\n","\n","def create_model():\n","    model = sigmachess_network()\n","\n","    model.compile(\n","        optimizer=Adam(learning_rate=0.02),\n","        loss={\n","            \"policy_output\": \"categorical_crossentropy\",\n","            \"value_output\": \"mean_squared_error\"\n","        },\n","        metrics={\n","            \"policy_output\": \"accuracy\",\n","            \"value_output\": \"mse\"\n","        }\n","    )\n","\n","    return model\n","\n","def play_vs_stockfish(model, game, replay_buffer):\n","    state = GameState()\n","    temperature = 1.0 if game < 5 else 0.1\n","\n","    w_states, w_policies, w_rewards = [], [], []\n","    player = np.random.choice([chess.WHITE, chess.BLACK])\n","\n","    engine = chess.engine.SimpleEngine.popen_uci(r\"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\")\n","\n","    while not state.is_terminal():\n","        if state.board.turn == player:\n","            mcts = MCTS(model, 1.0, 10)\n","            action_probs = mcts.run(state, temperature)\n","\n","            w_states.append(state.get_current_state())\n","            w_policies.append(action_probs)\n","\n","            action = np.random.choice(len(action_probs), p=action_probs)\n","            state.get_next_state(action)\n","        else:\n","            result = engine.play(state.board, chess.engine.Limit(0.04))\n","            state.apply_action(result.move)\n","\n","    engine.close()\n","\n","    winner = state.get_winner()\n","    w_value = 1 if winner == player else (0 if winner == 2 else -1)\n","\n","    print(player, state.board.board_fen())\n","\n","    replay_buffer.store_multiple_data(w_states, w_policies, w_value)\n","\n","def self_play(model, num_games=100, max_workers=5):\n","    replay_buffer = ReplayBuffer(maxlen=500000)\n","\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = [\n","            executor.submit(play_vs_stockfish, model, i, replay_buffer)\n","            for i in range(num_games)\n","        ]\n","\n","        for future in as_completed(futures):\n","            future.result()\n","\n","    print(\"Self-play completed\")\n","\n","    return replay_buffer\n","\n","def prepare_validation_data(replay_buffer=None, filename=\"validation_data.npy\"):\n","    try:\n","        # First, try to load existing validation data\n","        data = np.load(filename, allow_pickle=True).item()\n","        print(f\"Loaded existing validation data from {filename}\")\n","        return data['x_val'], {\n","            \"policy_output\": data['y_val_policy'],\n","            \"value_output\": data['y_val_value']\n","        }\n","    except (FileNotFoundError, IOError):\n","        # If no existing data, create new validation dataset\n","        if replay_buffer is None:\n","            # If no replay buffer provided, generate synthetic data\n","            x_val = np.random.random((100, 8, 8, 119))\n","            y_val_policy = np.random.randint(0, 73, size=(100, 1))\n","            y_val_value = np.random.random((100, 1)) * 2 - 1  # Values between -1 and 1\n","        else:\n","            # Use replay buffer to generate validation data\n","            states, policies, rewards = replay_buffer.sample(100)\n","            x_val = np.squeeze(states)\n","            y_val_policy = policies\n","            y_val_value = np.array(rewards).reshape(-1, 1)\n","\n","        # One-hot encode policy output\n","        y_val_policy_onehot = np.eye(73)[y_val_policy.flatten()]\n","\n","        # Prepare data dictionary\n","        validation_data = {\n","            \"x_val\": x_val,\n","            \"y_val_policy\": y_val_policy_onehot,\n","            \"y_val_value\": y_val_value\n","        }\n","\n","        # Save the validation data\n","        np.save(filename, validation_data)\n","        print(f\"Created and saved new validation data to {filename}\")\n","\n","        return x_val, {\n","            \"policy_output\": y_val_policy_onehot,\n","            \"value_output\": y_val_value\n","        }\n","\n","def create_callbacks(checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n","    checkpoint = ModelCheckpoint(\n","        filepath=checkpoint_path,\n","        save_weights_only=True,\n","        monitor=\"loss\",\n","        mode=\"min\",\n","        save_best_only=True,\n","        save_freq=\"epoch\",\n","        verbose=1\n","    )\n","    return [checkpoint]\n","\n","def train_model(model, replay_buffer: ReplayBuffer, batch_size=256, epochs=3, checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n","    x_val, y_val = prepare_validation_data()\n","\n","    callbacks = create_callbacks(checkpoint_path)\n","\n","    total_policy_loss = 0\n","    total_value_loss = 0\n","    epoch_count = 0\n","\n","    for epoch in range(epochs):\n","        states, policies, values = replay_buffer.sample(batch_size)\n","\n","        states = np.squeeze(states)\n","        if len(states.shape) == 3:\n","            states = np.expand_dims(states, -1)\n","\n","        values = np.array(values).reshape(-1, 1)\n","\n","        history = model.fit(\n","            states,\n","            { \"policy_output\": policies, \"value_output\": values },\n","            batch_size=batch_size,\n","            epochs=1,\n","            callbacks=callbacks,\n","            verbose=1\n","        )\n","\n","        total_policy_loss += history.history['policy_output_loss'][0]\n","        total_value_loss += history.history['value_output_loss'][0]\n","        epoch_count += 1\n","\n","    avg_policy_loss = total_policy_loss / epoch_count\n","    avg_value_loss = total_value_loss / epoch_count\n","\n","    print(f\"\\nAverage Policy Output Loss: {avg_policy_loss}\")\n","    print(f\"Average Value Output Loss: {avg_value_loss}\")\n","\n","    return avg_policy_loss, avg_value_loss\n","\n","is_stop = False\n","\n","def train_sigmachess(model, num_iterations=100, num_games_per_iteration=100):\n","    global is_stop\n","\n","    # Değişkenleri başlatın\n","    total_policy_loss = 0\n","    total_value_loss = 0\n","    iteration_count = 0\n","\n","    for iteration in range(num_iterations):\n","        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n","\n","        replay_buffer = self_play(model, num_games_per_iteration)\n","        policy_loss, value_loss = train_model(model, replay_buffer)\n","\n","        # Kayıpları biriktir\n","        total_policy_loss += policy_loss\n","        total_value_loss += value_loss\n","        iteration_count += 1\n","\n","        # Her 5 iteration'da bir ortalamaları yazdır\n","        if iteration_count % 5 == 0:\n","            avg_policy_loss = total_policy_loss / iteration_count\n","            avg_value_loss = total_value_loss / iteration_count\n","            print(f\"\\n[After {iteration_count} Iterations]\")\n","            print(f\"Average Policy Output Loss: {avg_policy_loss}\")\n","            print(f\"Average Value Output Loss: {avg_value_loss}\")\n","\n","        if is_stop:\n","            break\n","\n","    model.save(\"/content/drive/My Drive/full_model.keras\")\n","\n","\n","def stop():\n","    global is_stop\n","\n","    while True:\n","        inp = input(\"\")\n","        if inp == \"stop\":\n","            is_stop = True\n","            print(\"After the iteration is completed, the training will be stopped and the model will be saved!\")\n","\n","            break\n","\n","t = threading.Thread(target=stop, daemon=True)\n","t.start()\n","\n","model = create_model()\n","train_sigmachess(model, num_iterations=7000, num_games_per_iteration=15)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDw_Ru4EPXCm"},"outputs":[],"source":["'''\n","def run(self, initial_state, temperature=1.0):\n","    root = Node(initial_state)\n","\n","    # First evaluate and expand root\n","    action_probs, value = self.evaluate(initial_state)\n","    valid_moves = initial_state.get_valid_moves()\n","\n","    # Add Dirichlet noise to root (alpha=0.3 for chess)\n","    noise = np.random.dirichlet([0.3] * len(valid_moves))\n","\n","    # Expand with noisy priors\n","    for idx, action in enumerate(valid_moves):\n","        prob = action_probs[action]\n","        noisy_prob = 0.75 * prob + 0.25 * noise[idx]\n","        next_state = initial_state.clone()\n","        next_state.get_next_state(action)\n","        root.children[action] = Node(next_state, parent=root, prior_prob=noisy_prob)\n","\n","    for _ in range(self.simulations):\n","        node = root\n","\n","        # Selection with additional safety checks\n","        while node.is_expanded and not node.state.is_terminal():\n","            action, child_node = node.select(self.c_puct)\n","\n","            # Safety check to prevent NoneType errors\n","            if child_node is None:\n","                break\n","\n","            node = child_node\n","\n","        # Expansion and Evaluation\n","        if not node.state.is_terminal():\n","            # Ensure node has valid children before expansion\n","            if not node.is_expanded and len(node.children) == 0:\n","                action_probs, value = self.evaluate(node.state)\n","                valid_moves = node.state.get_valid_moves()\n","                node.expand(action_probs)\n","\n","            if node.is_expanded and len(node.children) > 0:\n","                # Randomly select an unexpanded child if possible\n","                unexpanded_children = [child for child in node.children.values() if not child.is_expanded]\n","                if unexpanded_children:\n","                    node = np.random.choice(unexpanded_children)\n","\n","                action_probs, value = self.evaluate(node.state)\n","                valid_moves = node.state.get_valid_moves()\n","                node.expand(action_probs)\n","            else:\n","                # Fallback: re-evaluate the current node\n","                value = node.state.get_winner()\n","                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","        else:\n","            value = node.state.get_winner()\n","            value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","\n","        # Backup\n","        node.backup(value)\n","\n","    return self.get_action_probs(root, temperature)\n","'''\n","#run revize edilmiş"]},{"cell_type":"code","source":["#!!!   üstteki hata önleme etkili değilse yapmamız gereken Eğer mevcut düğüm üzerinde genişletme veya seçim yapılması mümkün değilse, başka bir alt düğüme geçiş yapılır. Bu, unexpanded_children listesinden rastgele bir çocuk seçilerek yapılır.\n","'''\n","for _ in range(self.simulations):\n","    node = root\n","\n","    # Seçim aşamasında NoneType kontrolü\n","    while node.is_expanded and not node.state.is_terminal():\n","        action, child_node = node.select(self.c_puct)\n","\n","        # Eğer child_node None ise, bu düğüm üzerinde işlem yapılmaz, döngü devam eder\n","        if child_node is None:\n","            break\n","\n","        node = child_node\n","\n","    # Genişletme ve Değerlendirme\n","    if not node.state.is_terminal():\n","        # Düğümün çocukları yoksa ve genişletilemiyorsa\n","        if not node.is_expanded and len(node.children) == 0:\n","            action_probs, value = self.evaluate(node.state)\n","            valid_moves = node.state.get_valid_moves()\n","\n","            # Eğer geçerli hareketler varsa, genişletme işlemi yapılır\n","            if valid_moves:\n","                node.expand(action_probs)\n","            else:\n","                # Geçerli hareket yoksa, yedekleme yap\n","                value = node.state.get_winner()\n","                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","                node.backup(value)\n","                continue  # Diğer bir düğüme geçmek için döngüye devam et\n","\n","        # Çocuk düğümlerinin varlığı kontrol ediliyor\n","        if node.is_expanded and len(node.children) > 0:\n","            unexpanded_children = [child for child in node.children.values() if not child.is_expanded]\n","            if unexpanded_children:\n","                # Rastgele bir genişletilmemiş çocuk seç\n","                node = np.random.choice(unexpanded_children)\n","\n","            action_probs, value = self.evaluate(node.state)\n","            valid_moves = node.state.get_valid_moves()\n","            node.expand(action_probs)\n","        else:\n","            # Eğer hiçbir şey yapılamazsa, fallback değeriyle yedekleme yap\n","            value = node.state.get_winner()\n","            value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","            node.backup(value)\n","            continue  # Geçerli düğüm üzerinde işlem yapamıyorsak, başka bir düğüme geç\n","    else:\n","        # Eğer düğüm terminalse, kazananı belirle\n","        value = node.state.get_winner()\n","        value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","\n","    # Değeri yedekle\n","    node.backup(value)\n","'''"],"metadata":{"id":"za1kYZLOlMC0"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":6219321,"sourceId":10086949,"sourceType":"datasetVersion"},{"datasetId":6227771,"sourceId":10097989,"sourceType":"datasetVersion"}],"dockerImageVersionId":30805,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}