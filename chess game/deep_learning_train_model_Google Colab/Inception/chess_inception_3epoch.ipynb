{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10086949,"sourceType":"datasetVersion","datasetId":6219321}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["#ULTRA SUPER SIGMA CHESS AI"],"metadata":{"trusted":true,"id":"RUlmIButCyE6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!pip install chess"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:37:19.019974Z","iopub.execute_input":"2024-12-03T18:37:19.020302Z","iopub.status.idle":"2024-12-03T18:37:30.406565Z","shell.execute_reply.started":"2024-12-03T18:37:19.020275Z","shell.execute_reply":"2024-12-03T18:37:30.405717Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"6j5hDBiZCyE7","outputId":"330fcd31-9927-4f1e-b633-f2028ae7fdef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: chess in /usr/local/lib/python3.10/dist-packages (1.11.1)\n"]}],"execution_count":null},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9yiWlrkxC03v","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f250df97-f9eb-4608-da5f-acd85a50bfa5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["\n","\n","!chmod +x \"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\"\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:37:30.408309Z","iopub.execute_input":"2024-12-03T18:37:30.408566Z","iopub.status.idle":"2024-12-03T18:37:33.926613Z","shell.execute_reply.started":"2024-12-03T18:37:30.408541Z","shell.execute_reply":"2024-12-03T18:37:33.925393Z"},"id":"DVYV0gxkCyE8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import math\n","\n","import chess\n","\n","from collections import deque\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import HeNormal\n","\n","import chess.engine\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","import threading\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","class GameState:\n","    row = 8\n","    col = 8\n","    promotion_indexes = {\n","        chess.KNIGHT: 0,\n","        chess.ROOK: 1,\n","        chess.BISHOP: 2\n","    }\n","\n","    def __init__(self) -> None:\n","        self.board = chess.Board()\n","        self.repetition_count = 0\n","        self.player_color: chess.Color = chess.WHITE\n","\n","    def get_initial_state(self):\n","        self.board.reset()\n","\n","        return self.get_current_state()\n","\n","    def get_current_state(self, T=8):\n","        input_tensor = np.zeros((8, 8, 119), dtype=np.uint8)\n","\n","        for t in range(T):\n","            _t = T - t - 1\n","            if len(self.board.move_stack) < _t:\n","                continue\n","\n","            self.create_input(input_tensor, _t)\n","\n","        color = 0 if self.board.turn == chess.WHITE else 1\n","        input_tensor[:, :, 112] = color\n","\n","        input_tensor[:, :, 113] = len(self.board.move_stack) > 0\n","\n","        p1_castling = (1 * self.board.has_kingside_castling_rights(chess.WHITE)) | (2 * self.board.has_queenside_castling_rights(chess.WHITE))\n","        p1_castling_bit = format(p1_castling, \"02b\")\n","        input_tensor[:, :, 114] = int(p1_castling_bit[0])\n","        input_tensor[:, :, 115] = int(p1_castling_bit[1])\n","\n","        p2_castling = (1 * self.board.has_kingside_castling_rights(chess.BLACK)) | (2 * self.board.has_queenside_castling_rights(chess.BLACK))\n","        p2_castling_bit = format(p2_castling, \"02b\")\n","        input_tensor[:, :, 116] = int(p2_castling_bit[0])\n","        input_tensor[:, :, 117] = int(p2_castling_bit[1])\n","\n","        input_tensor[:, :, 118] = int(self.board.is_fifty_moves())\n","\n","        return np.expand_dims(input_tensor, axis=0)\n","\n","    def get_next_state(self, action: int):\n","        source_index = action // 73\n","        destination_index = 0\n","        move_type = action % 73\n","\n","        promotion = None\n","\n","        if move_type < 56:\n","            direction = move_type // 7\n","            movement = (move_type % 7) + 1\n","\n","            destination_index = source_index + (movement * 8) if direction == 0 else destination_index\n","            destination_index = source_index + (movement * 9) if direction == 1 else destination_index\n","            destination_index = source_index + movement if direction == 2 else destination_index\n","            destination_index = source_index + (movement * -7) if direction == 3 else destination_index\n","            destination_index = source_index + (movement * -8) if direction == 4 else destination_index\n","            destination_index = source_index + (movement * -9) if direction == 5 else destination_index\n","            destination_index = source_index + (-movement) if direction == 6 else destination_index\n","            destination_index = source_index + (movement * 7) if direction == 7 else destination_index\n","        elif move_type >= 56 and move_type < 64:\n","            direction = move_type - 56\n","\n","            destination_index = source_index + 17 if direction == 0 else destination_index\n","            destination_index = source_index + 10 if direction == 1 else destination_index\n","            destination_index = source_index - 6 if direction == 2 else destination_index\n","            destination_index = source_index - 15 if direction == 3 else destination_index\n","            destination_index = source_index - 17 if direction == 4 else destination_index\n","            destination_index = source_index - 10 if direction == 5 else destination_index\n","            destination_index = source_index + 6 if direction == 6 else destination_index\n","            destination_index = source_index + 15 if direction == 7 else destination_index\n","        else:\n","            direction = (move_type - 64) // 3\n","            promotion_index = (move_type - 64) % 3\n","\n","            promotion = chess.KNIGHT if promotion_index == 0 else (chess.ROOK if promotion_index == 1 else chess.BISHOP)\n","\n","            color_value = 1 if self.board.turn == chess.WHITE else -1\n","\n","            if direction == 0:\n","                destination_index = source_index + (8 * color_value)\n","            elif direction == 1:\n","                destination_index = source_index + (9 * color_value)\n","            else:\n","                destination_index = source_index + (7 * color_value)\n","\n","        from_square = chess.Square(source_index)\n","        to_square = chess.Square(destination_index)\n","\n","        promotion_rank = 7 if self.board.turn == chess.WHITE else 0\n","\n","        if promotion is None:\n","            if self.board.piece_type_at(from_square) == chess.PAWN and chess.square_rank(to_square) == promotion_rank:\n","                promotion = chess.QUEEN\n","\n","        move = chess.Move(from_square, to_square, promotion)\n","\n","        self.apply_action(move)\n","\n","        return move, self.get_current_state()\n","\n","    def apply_action(self, move: chess.Move):\n","        try:\n","            self.board.push(move)\n","        except Exception as e:\n","            print(list(self.board.legal_moves))\n","            print(self.get_valid_moves())\n","\n","            print(e)\n","\n","            raise Exception(\"Error\")\n","\n","    def create_input(self, input_tensor: np.ndarray, t: int):\n","        piece_types = {\n","            chess.PAWN: 0,\n","            chess.KNIGHT: 1,\n","            chess.BISHOP: 2,\n","            chess.ROOK: 3,\n","            chess.QUEEN: 4,\n","            chess.KING: 5\n","        }\n","\n","        board = self.board.copy()\n","        for _ in range(t):\n","            board.pop()\n","\n","        transposition_key = board._transposition_key()\n","\n","        for square in chess.SQUARES:\n","            piece = board.piece_at(square)\n","\n","            if piece is None:\n","                continue\n","\n","            piece_index = piece_types[piece.piece_type]\n","            piece_color = 0 if piece.color == chess.WHITE else 1\n","\n","            index = (t * 14) + (piece_color * 6) + piece_index\n","            input_tensor[square // 8][square % 8][index] = 1\n","\n","        repetition_count = 0\n","        index = (t * 14) + 12\n","\n","        try:\n","            while board.move_stack:\n","                move = board.pop()\n","                if board.is_irreversible(move):\n","                    break\n","\n","                if board._transposition_key() == transposition_key:\n","                    repetition_count += 1\n","\n","                if repetition_count == 3:\n","                    break\n","        finally:\n","            repetition_count = 3 if repetition_count > 3 else repetition_count\n","\n","            repetition_count_bits = [int(x) for x in format(repetition_count, \"02b\")]\n","            input_tensor[:, :, index] = repetition_count_bits[0]\n","            input_tensor[:, :, index + 1] = repetition_count_bits[1]\n","\n","    def get_valid_moves(self):\n","        legal_moves = []\n","\n","        for valid_move in self.board.legal_moves:\n","            s_row, s_col, from_square_index = self.index_of_square(valid_move.from_square)\n","            d_row, d_col, to_square_index = self.index_of_square(valid_move.to_square)\n","\n","            if valid_move.promotion:\n","                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n","\n","                if valid_move.promotion == chess.QUEEN:\n","                    index = (from_square_index * 73) + (direction * 7)\n","                    legal_moves.append(index)\n","                else:\n","                    promotion_index = self.promotion_indexes[valid_move.promotion]\n","\n","                    if direction > 2 and direction < 6:\n","                        direction = 0 if direction == 4 else (1 if direction == 5 else 2)\n","                    elif direction == 7:\n","                        direction = 2\n","\n","                    index = (from_square_index * 73) + ((direction * 3) + promotion_index + 64)\n","                    legal_moves.append(index)\n","            elif self.board.piece_type_at(valid_move.from_square) == chess.KNIGHT:\n","                direction = self.direction_of_move_for_knights(s_row, s_col, d_row, d_col)\n","\n","                index = (from_square_index * 73) + direction + 56\n","                legal_moves.append(index)\n","\n","            else:\n","                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n","                count_of_square = self.count_of_square_for_movement(s_row, s_col, d_row, d_col) - 1\n","\n","                index = (from_square_index * 73) + ((direction * 7) + count_of_square)\n","                legal_moves.append(index)\n","\n","        return legal_moves\n","\n","    def index_of_square(self, square: chess.Square):\n","        row = chess.square_rank(square)\n","        col = chess.square_file(square)\n","        index = (row * 8) + col\n","\n","        return row, col, index\n","\n","    def direction_of_move_for_ray_directions(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        if delta_x == 0:\n","            return 0 if delta_y > 0 else 4\n","\n","        if delta_y == 0:\n","            return 2 if delta_x > 0 else 6\n","\n","        if delta_x < 0:\n","            return 7 if delta_y > 0 else 5\n","\n","        return 1 if delta_y > 0 else 3\n","\n","    def direction_of_move_for_knights(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        if delta_x == 1:\n","            return 0 if delta_y > 0 else 3\n","\n","        if delta_x == 2:\n","            return 1 if delta_y > 0 else 2\n","\n","        if delta_x == -1:\n","            return 7 if delta_y > 0 else 4\n","\n","        return 6 if delta_y > 0 else 5\n","\n","    def count_of_square_for_movement(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        return max(abs(delta_x), abs(delta_y))\n","\n","    def get_winner(self):\n","        result = self.board.result()\n","\n","        if result == \"1-0\":\n","            return chess.WHITE\n","\n","        if result == \"0-1\":\n","            return chess.BLACK\n","\n","        return 2\n","\n","    def is_terminal(self):\n","        return self.board.is_game_over()\n","\n","    def clone(self):\n","        cloned_state = GameState()\n","        cloned_state.board = self.board.copy()\n","\n","        return cloned_state\n","\n","class Node:\n","    def __init__(self, state, parent=None, prior_prob=1.0):\n","        self.state = state\n","        self.parent = parent\n","        self.children = {}\n","        self.visits = 0\n","        self.value_sum = 0\n","        self.prior_prob = prior_prob\n","        self.is_expanded = False\n","\n","    @property\n","    def value(self):\n","        return self.value_sum / (self.visits + 1e-5)\n","\n","    def expand(self, action_probs):\n","        for action, prob in enumerate(action_probs):\n","            if prob > 0:\n","                next_state = self.state.clone()\n","                next_state.get_next_state(action)\n","                self.children[action] = Node(next_state, parent=self, prior_prob=prob)\n","\n","        if len(self.children) > 0:\n","            self.is_expanded = True\n","\n","    def select(self, c_puct=1.0):\n","        max_ucb = -float('inf')\n","        best_action = None\n","        best_child = None\n","\n","        for action, child in self.children.items():\n","            ucb = child.value + c_puct * child.prior_prob * (math.sqrt(self.visits) / (1 + child.visits))\n","            if ucb > max_ucb:\n","                max_ucb = ucb\n","                best_action = action\n","                best_child = child\n","        return best_action, best_child\n","\n","    def backup(self, value):\n","        self.visits += 1\n","        self.value_sum += value\n","        if self.parent:\n","            self.parent.backup(-value)\n","\n","class MCTS:\n","    def __init__(self, model, c_puct=1.0, simulations=50):\n","        self.model = model\n","        self.c_puct = c_puct\n","        self.simulations = simulations\n","\n","    def add_dirichlet_noise(self, node, valid_moves):\n","        noise = np.random.dirichlet([0.3] * len(valid_moves))\n","        for idx, action in enumerate(valid_moves):\n","            if action in node.children:\n","                node.children[action].prior_prob = \\\n","                    0.75 * node.children[action].prior_prob + 0.25 * noise[idx]\n","\n","    def run(self, initial_state, temperature=1.0):\n","        root = Node(initial_state)\n","\n","        # First evaluate and expand root\n","        action_probs, value = self.evaluate(initial_state)\n","        valid_moves = initial_state.get_valid_moves()\n","\n","        # Add Dirichlet noise to root (alpha=0.3 for chess)\n","        noise = np.random.dirichlet([0.3] * len(valid_moves))\n","\n","        # Expand with noisy priors\n","        for idx, action in enumerate(valid_moves):\n","            prob = action_probs[action]\n","            noisy_prob = 0.75 * prob + 0.25 * noise[idx]\n","            next_state = initial_state.clone()\n","            next_state.get_next_state(action)\n","            root.children[action] = Node(next_state, parent=root, prior_prob=noisy_prob)\n","\n","        for _ in range(self.simulations):\n","            node = root\n","\n","            # Selection\n","            while node.is_expanded and not node.state.is_terminal():\n","                action, node = node.select(self.c_puct)\n","\n","            # Expansion and Evaluation\n","            if not node.state.is_terminal():\n","                action_probs, value = self.evaluate(node.state)\n","                valid_moves = node.state.get_valid_moves()\n","                node.expand(action_probs)\n","            else:\n","                value = node.state.get_winner()\n","                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","\n","            # Backup\n","            node.backup(value)\n","\n","        return self.get_action_probs(root, temperature)\n","\n","    def evaluate(self, state):\n","        state_tensor = state.get_current_state()\n","        # state_tensor = np.expand_dims(state_tensor, axis=0)\n","\n","        policy, value = self.model.predict(state_tensor, verbose=0)\n","\n","        # Mask invalid moves\n","        valid_moves = state.get_valid_moves()\n","        mask = np.zeros(policy.shape[1])\n","        mask[valid_moves] = 1\n","\n","        policy = policy[0] * mask\n","\n","        # Normalize\n","        sum_policy = np.sum(policy)\n","        if sum_policy > 0:\n","            policy /= sum_policy\n","        else:\n","            # If all moves were masked, use uniform distribution over valid moves\n","            policy = mask / np.sum(mask)\n","\n","        return policy, value[0][0]\n","\n","    def get_action_probs(self, root, temperature=1.0):\n","        visits = np.array([child.visits for action, child in root.children.items()])\n","        actions = list(root.children.keys())\n","\n","        if temperature == 0:  # Pure exploitation\n","            action_idx = np.argmax(visits)\n","            probs = np.zeros_like(visits)\n","            probs[action_idx] = 1\n","        else:\n","            # Apply temperature\n","            visits = visits ** (1 / temperature)\n","            probs = visits / np.sum(visits)\n","\n","        # Convert to full move probability vector\n","        full_probs = np.zeros(4672)  # Adjust size based on your action space\n","        for action, prob in zip(actions, probs):\n","            full_probs[action] = prob\n","\n","        return full_probs\n","\n","class ReplayBuffer:\n","    def __init__(self, maxlen=500000):\n","        self.buffer = deque(maxlen=maxlen)\n","        self.current_size = 0\n","        self.lock = threading.Lock()\n","\n","    def store(self, state, policy, value):\n","        \"\"\"Store a single game state transition\"\"\"\n","        self.buffer.append({\n","            'state': state,\n","            'policy': policy,\n","            'value': value\n","        })\n","        self.current_size = len(self.buffer)\n","\n","    def store_multiple_data(self, states, policies, value):\n","        with self.lock:\n","            for s, p, v in zip(states, policies, [value]):\n","                self.store(s, p, v)\n","\n","    def sample(self, batch_size):\n","        \"\"\"Sample a batch with augmentations\"\"\"\n","        if self.current_size < batch_size:\n","            batch_size = self.current_size\n","\n","        indices = np.random.choice(self.current_size, batch_size)\n","        states, policies, values = [], [], []\n","\n","        for idx in indices:\n","            sample = self.buffer[idx]\n","            # Get augmented samples\n","            aug_states, aug_policies = self._augment_sample(\n","                sample['state'],\n","                sample['policy']\n","            )\n","\n","            # Add all augmentations\n","            states.extend(aug_states)\n","            policies.extend(aug_policies)\n","            values.extend([sample['value']] * len(aug_states))\n","\n","        return np.array(states), np.array(policies), np.array(values)\n","\n","    def _augment_sample(self, state, policy):\n","        \"\"\"Generate valid augmentations for a single sample\"\"\"\n","        # Remove batch dimension if present\n","        if len(state.shape) == 4:\n","            state = np.squeeze(state, axis=0)\n","\n","        augmented_states = [state]\n","        augmented_policies = [policy]\n","\n","        # Horizontal flip\n","        flip_h = np.flip(state, axis=1)\n","        augmented_states.append(flip_h)\n","        augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        # Vertical flip\n","        flip_v = np.flip(state, axis=0)\n","        augmented_states.append(flip_v)\n","        augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        # Diagonal flip (only if shape allows)\n","        if state.shape[0] == state.shape[1]:\n","            diag = np.transpose(state, (1, 0, 2))\n","            augmented_states.append(diag)\n","            augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        return augmented_states, augmented_policies\n","\n","    def __len__(self):\n","        return self.current_size\n","\n","# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","# tf.tpu.experimental.initialize_tpu_system(tpu)\n","# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n","\n","print(\"All devices: \", tf.config.list_logical_devices('GPU'))\n","\n","policy = tf.keras.mixed_precision.Policy('mixed_float16')\n","tf.keras.mixed_precision.set_global_policy(policy)"],"metadata":{"_uuid":"d7c89aa5-8449-4c9b-b220-63cf5c8ab98c","_cell_guid":"e45be06e-fe38-4b84-b26e-6ead59ee098e","trusted":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-03T18:40:52.940405Z","iopub.execute_input":"2024-12-03T18:40:52.940740Z"},"id":"py_fPedFCyE8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b80e43c6-4b7f-455a-a9e0-004f91b9fe71"},"outputs":[{"output_type":"stream","name":"stdout","text":["All devices:  [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"]}],"execution_count":null},{"cell_type":"code","source":["import numpy as np\n","import threading\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","def inception_block(inputs, filters=256, growth_rate=32, bottleneck=True, stride=1):\n","    # 1x1 Convolution (Bottleneck)\n","    branch_1x1 = layers.Conv2D(filters, (1, 1), strides=stride, padding=\"same\",\n","                                kernel_initializer=HeNormal(), use_bias=False)(inputs)\n","    branch_1x1 = layers.BatchNormalization()(branch_1x1)\n","    branch_1x1 = layers.ReLU()(branch_1x1)\n","\n","    # 3x3 Convolution\n","    branch_3x3 = layers.Conv2D(growth_rate, (1, 1), strides=stride, padding=\"same\",\n","                                kernel_initializer=HeNormal(), use_bias=False)(inputs)\n","    branch_3x3 = layers.BatchNormalization()(branch_3x3)\n","    branch_3x3 = layers.ReLU()(branch_3x3)\n","    branch_3x3 = layers.Conv2D(growth_rate, (3, 3), strides=1, padding=\"same\",\n","                                kernel_initializer=HeNormal(), use_bias=False)(branch_3x3)\n","    branch_3x3 = layers.BatchNormalization()(branch_3x3)\n","    branch_3x3 = layers.ReLU()(branch_3x3)\n","\n","    # 5x5 Convolution\n","    branch_5x5 = layers.Conv2D(growth_rate, (1, 1), strides=stride, padding=\"same\",\n","                                kernel_initializer=HeNormal(), use_bias=False)(inputs)\n","    branch_5x5 = layers.BatchNormalization()(branch_5x5)\n","    branch_5x5 = layers.ReLU()(branch_5x5)\n","    branch_5x5 = layers.Conv2D(growth_rate, (5, 5), strides=1, padding=\"same\",\n","                                kernel_initializer=HeNormal(), use_bias=False)(branch_5x5)\n","    branch_5x5 = layers.BatchNormalization()(branch_5x5)\n","    branch_5x5 = layers.ReLU()(branch_5x5)\n","\n","    # MaxPooling + 1x1 Convolution\n","    branch_pool = layers.MaxPooling2D((3, 3), strides=1, padding=\"same\")(inputs)\n","    branch_pool = layers.Conv2D(growth_rate, (1, 1), strides=1, padding=\"same\",\n","                                 kernel_initializer=HeNormal(), use_bias=False)(branch_pool)\n","    branch_pool = layers.BatchNormalization()(branch_pool)\n","    branch_pool = layers.ReLU()(branch_pool)\n","\n","    # Concatenate all branches\n","    x = layers.Concatenate()([branch_1x1, branch_3x3, branch_5x5, branch_pool])\n","\n","    return x\n","\n","def sigmachess_network(input_shape=(8, 8, 119)):\n","    inputs = layers.Input(shape=input_shape)\n","\n","    x = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.ReLU()(x)\n","\n","    for _ in range(19):\n","        x = inception_block(x)\n","\n","    policy = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    policy = layers.BatchNormalization()(policy)\n","    policy = layers.ReLU()(policy)\n","    policy = layers.Conv2D(73, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(policy)\n","    policy = layers.Flatten()(policy)\n","    policy = layers.Softmax(name=\"policy_output\")(policy)\n","\n","    value = layers.Conv2D(1, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    value = layers.BatchNormalization()(value)\n","    value = layers.ReLU()(value)\n","    value = layers.Flatten()(value)\n","    value = layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(value)\n","    value = layers.Dense(1, activation=\"tanh\", name=\"value_output\", kernel_initializer=\"he_normal\")(value)\n","\n","    model = models.Model(inputs=inputs, outputs=[policy, value])\n","\n","    return model\n","\n","def create_model():\n","    model = sigmachess_network()\n","\n","    model.compile(\n","        optimizer=Adam(learning_rate=0.02),\n","        loss={\n","            \"policy_output\": \"categorical_crossentropy\",\n","            \"value_output\": \"mean_squared_error\"\n","        },\n","        metrics={\n","            \"policy_output\": \"accuracy\",\n","            \"value_output\": \"mse\"\n","        }\n","    )\n","\n","    return model\n","\n","def play_vs_stockfish(model, game, replay_buffer):\n","    state = GameState()\n","    temperature = 1.0 if game < 5 else 0.1\n","\n","    w_states, w_policies, w_rewards = [], [], []\n","    player = np.random.choice([chess.WHITE, chess.BLACK])\n","\n","    engine = chess.engine.SimpleEngine.popen_uci(r\"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\")\n","\n","    while not state.is_terminal():\n","        if state.board.turn == player:\n","            mcts = MCTS(model, 1.0, 10)\n","            action_probs = mcts.run(state, temperature)\n","\n","            w_states.append(state.get_current_state())\n","            w_policies.append(action_probs)\n","\n","            action = np.random.choice(len(action_probs), p=action_probs)\n","            state.get_next_state(action)\n","        else:\n","            result = engine.play(state.board, chess.engine.Limit(0.04))\n","            state.apply_action(result.move)\n","\n","    engine.close()\n","\n","    winner = state.get_winner()\n","    w_value = 1 if winner == player else (0 if winner == 2 else -1)\n","\n","    print(player, state.board.board_fen())\n","\n","    replay_buffer.store_multiple_data(w_states, w_policies, w_value)\n","\n","def self_play(model, num_games=100, max_workers=5):\n","    replay_buffer = ReplayBuffer(maxlen=500000)\n","\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = [\n","            executor.submit(play_vs_stockfish, model, i, replay_buffer)\n","            for i in range(num_games)\n","        ]\n","\n","        for future in as_completed(futures):\n","            future.result()\n","\n","    print(\"Self-play completed\")\n","\n","    return replay_buffer\n","\n","def prepare_validation_data(replay_buffer=None, filename=\"validation_data.npy\"):\n","    try:\n","        # First, try to load existing validation data\n","        data = np.load(filename, allow_pickle=True).item()\n","        print(f\"Loaded existing validation data from {filename}\")\n","        return data['x_val'], {\n","            \"policy_output\": data['y_val_policy'],\n","            \"value_output\": data['y_val_value']\n","        }\n","    except (FileNotFoundError, IOError):\n","        # If no existing data, create new validation dataset\n","        if replay_buffer is None:\n","            # If no replay buffer provided, generate synthetic data\n","            x_val = np.random.random((100, 8, 8, 119))\n","            y_val_policy = np.random.randint(0, 73, size=(100, 1))\n","            y_val_value = np.random.random((100, 1)) * 2 - 1  # Values between -1 and 1\n","        else:\n","            # Use replay buffer to generate validation data\n","            states, policies, rewards = replay_buffer.sample(100)\n","            x_val = np.squeeze(states)\n","            y_val_policy = policies\n","            y_val_value = np.array(rewards).reshape(-1, 1)\n","\n","        # One-hot encode policy output\n","        y_val_policy_onehot = np.eye(73)[y_val_policy.flatten()]\n","\n","        # Prepare data dictionary\n","        validation_data = {\n","            \"x_val\": x_val,\n","            \"y_val_policy\": y_val_policy_onehot,\n","            \"y_val_value\": y_val_value\n","        }\n","\n","        # Save the validation data\n","        np.save(filename, validation_data)\n","        print(f\"Created and saved new validation data to {filename}\")\n","\n","        return x_val, {\n","            \"policy_output\": y_val_policy_onehot,\n","            \"value_output\": y_val_value\n","        }\n","\n","def create_callbacks(checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n","    checkpoint = ModelCheckpoint(\n","        filepath=checkpoint_path,\n","        save_weights_only=True,\n","        monitor=\"loss\",\n","        mode=\"min\",\n","        save_best_only=True,\n","        save_freq=\"epoch\",\n","        verbose=1\n","    )\n","    return [checkpoint]\n","\n","def train_model(model, replay_buffer: ReplayBuffer, batch_size=256, epochs=3, checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n","    x_val, y_val = prepare_validation_data()\n","\n","    callbacks = create_callbacks(checkpoint_path)\n","\n","    total_policy_loss = 0\n","    total_value_loss = 0\n","    epoch_count = 0\n","\n","    for epoch in range(epochs):\n","        states, policies, values = replay_buffer.sample(batch_size)\n","\n","        states = np.squeeze(states)\n","        if len(states.shape) == 3:\n","            states = np.expand_dims(states, -1)\n","\n","        values = np.array(values).reshape(-1, 1)\n","\n","        history = model.fit(\n","            states,\n","            { \"policy_output\": policies, \"value_output\": values },\n","            batch_size=batch_size,\n","            epochs=1,\n","            callbacks=callbacks,\n","            verbose=1\n","        )\n","\n","        total_policy_loss += history.history['policy_output_loss'][0]\n","        total_value_loss += history.history['value_output_loss'][0]\n","        epoch_count += 1\n","\n","    avg_policy_loss = total_policy_loss / epoch_count\n","    avg_value_loss = total_value_loss / epoch_count\n","\n","    print(f\"\\nAverage Policy Output Loss: {avg_policy_loss}\")\n","    print(f\"Average Value Output Loss: {avg_value_loss}\")\n","\n","    return avg_policy_loss, avg_value_loss\n","\n","is_stop = False\n","\n","def train_sigmachess(model, num_iterations=100, num_games_per_iteration=100):\n","    global is_stop\n","\n","    # Değişkenleri başlatın\n","    total_policy_loss = 0\n","    total_value_loss = 0\n","    iteration_count = 0\n","\n","    for iteration in range(num_iterations):\n","        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n","\n","        replay_buffer = self_play(model, num_games_per_iteration)\n","        policy_loss, value_loss = train_model(model, replay_buffer)\n","\n","        # Kayıpları biriktir\n","        total_policy_loss += policy_loss\n","        total_value_loss += value_loss\n","        iteration_count += 1\n","\n","        # Her 5 iteration'da bir ortalamaları yazdır\n","        if iteration_count % 5 == 0:\n","            avg_policy_loss = total_policy_loss / iteration_count\n","            avg_value_loss = total_value_loss / iteration_count\n","            print(f\"\\n[After {iteration_count} Iterations]\")\n","            print(f\"Average Policy Output Loss: {avg_policy_loss}\")\n","            print(f\"Average Value Output Loss: {avg_value_loss}\")\n","\n","        if is_stop:\n","            break\n","\n","    model.save(\"/content/drive/My Drive/full_model.keras\")\n","\n","\n","def stop():\n","    global is_stop\n","\n","    while True:\n","        inp = input(\"\")\n","        if inp == \"stop\":\n","            is_stop = True\n","            print(\"After the iteration is completed, the training will be stopped and the model will be saved!\")\n","\n","            break\n","\n","t = threading.Thread(target=stop, daemon=True)\n","t.start()\n","\n","model = create_model()\n","train_sigmachess(model, num_iterations=7000, num_games_per_iteration=15)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zmJtM_9MHZcr","outputId":"80c4ca1d-4ef6-4eba-dd75-a46fa66f3d3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1/7000\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x77fc209428c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["False rn1q1bnr/1pp2kpp/4Q3/p2B1P2/8/8/PPPP1PPP/RNB1K1NR\n","True r3k2r/ppp2ppp/2n1p2n/2bp4/2P2Pb1/8/PP1PPqBP/RNBQ2KR\n","False 1nbqk1Q1/1p1ppp2/7B/p1p5/3P4/3B4/PPP2PPP/RN2K1NR\n","True r3k2r/ppp2ppp/2n5/4p3/4nPbP/2P5/P1P2b2/R1B1KBNR\n","False rnbqkbnr/3pp3/6Qp/pBp1Pp2/3P3N/P7/1PP2PPP/RNB1K2R\n","True r3k2r/pp3ppp/2n2n2/4p3/2P3bP/4p2N/P2qP3/1R1K1B1R\n","True r3k2r/pp1n1ppp/2p1pn2/8/3PbPPP/2b3K1/6q1/7R\n","True rn2k1nr/p1pq1ppp/1p6/1K6/1PP1pPb1/7N/P6P/R1B4R\n","False rnb2bnr/pp2kpp1/3Q4/1Bp1P2p/8/2N5/PPP2PPP/R1B1K1NR\n","False 1nbq1bnr/rp1p1B1p/p7/k1pN2p1/P1N1P3/8/1PPP1PPP/R1BQK2R\n","False 8/4Q1p1/1P2k3/5pB1/p3p3/P1N4R/1PP2PP1/R3KB2\n","False 5bkr/4p1pp/4Qn2/pB4N1/8/2N5/PPPP1PPP/R1BQK2R\n","False 1nb2bnr/3p2p1/r7/pBp2QBk/4P3/P1N2NP1/1PP2P2/R3K2R\n","False 1nb2bnr/3pqBpp/r2k4/p1QNN3/3PP3/8/PPP2PPP/R1B2RK1\n","False 3k4/4Bp2/pNB5/2pNP1p1/2P3Pr/7p/PP3P1P/2KRR3\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119s/step - loss: 10.5332 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8098 - value_output_loss: 1.7234 - value_output_mse: 1.7237\n","Epoch 1: loss improved from inf to 10.53317, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 122s/step - loss: 10.5332 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8098 - value_output_loss: 1.7234 - value_output_mse: 1.7237\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 10.3789 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.9240 - value_output_loss: 1.4549 - value_output_mse: 1.4550\n","Epoch 1: loss improved from 10.53317 to 10.37892, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 10.3789 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.9240 - value_output_loss: 1.4549 - value_output_mse: 1.4550  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 9.9655 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.7817 - value_output_loss: 1.1838 - value_output_mse: 1.1835\n","Epoch 1: loss improved from 10.37892 to 9.96550, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 9.9655 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.7817 - value_output_loss: 1.1838 - value_output_mse: 1.1835  \n","\n","Average Policy Output Loss: 8.83852481842041\n","Average Value Output Loss: 1.4540056387583415\n","Iteration 2/7000\n","True rnb1k1nr/pppp1ppp/8/8/2P2pPq/8/PP1PP3/RNBQKBN1\n","False rnbqkbnr/ppppp3/6Qp/5P2/8/8/PPPP1PPP/RNB1KBNR\n","False r7/p3Qp1p/n3k3/3pN1B1/4P3/8/PPP2PPP/RN2K2R\n","False rnbq1bnr/pp1pk1p1/7p/2pNPP1Q/8/3B4/PPP2PPP/R1B1K1NR\n","True rn1qk2r/ppp3pp/8/4pp2/2pbK1b1/1P6/1P1P3P/R1B2B1R\n","True rn1r4/ppp1kppp/5n2/4pb2/6P1/N7/PP1q1P1P/1R2KBNR\n","False rnbqkbnr/ppppp3/5pQp/8/3PP3/8/PPP2PPP/RNB1KBNR\n","True r3kbnr/p1p2ppp/2n5/1q6/K2p4/4P3/PP1Bb2P/RN5R\n","True rn2k2r/ppp1nppp/8/4p1B1/2P1p3/2Nb1Pb1/PPq5/R2K4\n","True r3kb1r/pp2pppp/2p5/P2N4/1P2pPqK/8/5n1P/1b5n\n","False 3k1b1r/p1pQ2pp/8/8/8/1PN1BN1B/1PP2PPP/R3K2R\n","False 1nbk4/rp1pB3/p5Q1/2pNPp2/4P2N/P6P/1PPK1P1P/R4B1R\n","True r3kb1r/pp2pppp/2n5/3P4/2P1nPb1/8/PP3q1P/RNB1KBNR\n","False 2b2bQ1/r2pN2k/n2P4/ppp4p/8/2N1B3/PPP1BP1P/2KR2R1\n","True rn2k2r/1pp2pp1/4p3/p2p3p/Pb1PnPP1/5K1B/2bBPq1P/1R4NR\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 10.5292 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.7948 - value_output_loss: 1.7344 - value_output_mse: 1.7342\n","Epoch 1: loss improved from inf to 10.52916, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 10.5292 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.7948 - value_output_loss: 1.7344 - value_output_mse: 1.7342  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 10.3142 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8865 - value_output_loss: 1.4277 - value_output_mse: 1.4280\n","Epoch 1: loss improved from 10.52916 to 10.31423, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 10.3142 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8865 - value_output_loss: 1.4277 - value_output_mse: 1.4280  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 11.0797 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8439 - value_output_loss: 2.2359 - value_output_mse: 2.2358\n","Epoch 1: loss did not improve from 10.31423\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 11.0797 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8439 - value_output_loss: 2.2359 - value_output_mse: 2.2358\n","\n","Average Policy Output Loss: 8.841728846232096\n","Average Value Output Loss: 1.7993100484212239\n","Iteration 3/7000\n","True r1b1k1nr/ppp2ppp/2n5/3q4/P7/1PP3b1/R2PP3/2BQKBNR\n","False 1nb2k1Q/3pnp1p/p2B4/8/p3P3/N4N2/PPP2PPP/R4RK1\n","False r1b1k2Q/p2p1p2/3Bp1pp/p3P3/3N4/2N5/PPP2PPP/R3K2R\n","False N1bk4/p3nN2/3p1Bp1/1B6/3PP2p/P4P2/P1P3PP/R2Q1RK1\n","False r1bqkbnr/1pppp2P/n7/7Q/p2P4/8/PPP2PPP/RNB1KBNR\n","False 1r2kb1r/pp1Qpppp/8/1B6/4P3/5N1P/PPP2PP1/RNB1K2R\n","False rnbqkbnr/ppppp3/6Qp/5P2/8/8/PPPP1PPP/RNB1KBNR\n","False rnb1k2r/p1ppQppp/8/5pB1/3P4/2NB1N2/PPP2PPP/R3K2R\n","False rnbqkbnr/1p1pp2P/2p5/p6Q/8/8/PPPP1PPP/RNB1KBNR\n","True r3r1k1/2pp1ppp/p7/8/1PbPnp2/4n1q1/R3P3/4K3\n","False 2r2bnN/p1p1p1pp/3p2k1/2p5/4P3/5Q2/PPP2PPP/RNB1K2R\n","False q4b1r/1PpBkppp/1NN2n2/6B1/4P3/p7/PPP2PPP/R2QK2R\n","False r1b5/1p6/p2p1pQk/4p3/2BPP2N/8/PPP2PPP/RN2R1K1\n","False 1n3bnr/2B4p/B7/2pN4/3kP1Q1/2P4P/PP3PP1/R3K2R\n","False 3R3r/5k2/2p1pBQ1/2P2P2/B6p/2N5/PPP2PPN/4R1K1\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 10.6008 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0365 - value_output_loss: 1.5643 - value_output_mse: 1.5639\n","Epoch 1: loss improved from inf to 10.60075, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - loss: 10.6008 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0365 - value_output_loss: 1.5643 - value_output_mse: 1.5639  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - loss: 9.8295 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8852 - value_output_loss: 0.9442 - value_output_mse: 0.9442\n","Epoch 1: loss improved from 10.60075 to 9.82947, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 9.8295 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8852 - value_output_loss: 0.9442 - value_output_mse: 0.9442  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 10.6081 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0335 - value_output_loss: 1.5746 - value_output_mse: 1.5752\n","Epoch 1: loss did not improve from 9.82947\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 10.6081 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0335 - value_output_loss: 1.5746 - value_output_mse: 1.5752\n","\n","Average Policy Output Loss: 8.985073725382486\n","Average Value Output Loss: 1.3610451817512512\n","Iteration 4/7000\n","False 1rbqkb1r/1pppp2P/7B/p6Q/1P1P4/8/1PP2PPP/RN2KBNR\n","True r3k2r/ppp2ppp/2n5/3p4/Pb2np2/2N3qb/2PPP3/R1BQKBN1\n","False r1b1kbnr/3ppQ2/1B6/p3N1pp/4P3/2N5/PPP2PPP/R3KB1R\n","False 3q2nr/2pB2p1/b5kp/5Q2/4P3/2N1B3/PPP2PPP/R4RK1\n","False 5bn1/2pQppp1/2rk1r2/p3N3/2PP3P/2NBB3/PP1K1PP1/RQ5R\n","True rnb1kb1r/ppp2ppp/8/4p3/6nP/P5PN/1PP1PPBK/3q3R\n","False rnbq1bnr/1pppp1pp/5pk1/p4P2/2BPP3/2N5/PPP3PP/R1BQK1NR\n","True rn2k1nr/1pp2ppp/1p6/5b2/2P2p2/b2q4/5pPP/3K2NR\n","True r1b1k2r/1pp2ppp/5n2/p3p3/2p4P/bP1q2PN/P4P2/RnK4R\n","False rn1q1bnr/1pN1pkp1/4P3/p5Qp/2BP4/8/PPP2PPP/R1B1K1NR\n","False 1nb1k1nr/1p1p1Qpp/r7/p3NpB1/4P3/P1N3P1/1PP2PP1/R3KB1R\n","False 4kbnr/1pB2Q1p/r1n5/pB2p1N1/3PN3/2P5/PP3PPP/R3K2R\n","True r3k2r/pp3ppp/2pb4/4n3/3n4/P1Kq2Pb/1P3p1P/RN4R1\n","True r3k1nr/1pp3pp/p1nb1p2/P3p3/2b5/2P1P1PP/1P1KqP2/2B5\n","True r5r1/1bpqk1p1/2nb3p/P2p1p2/2PP1P1P/4p2N/4P1P1/2q1K2R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - loss: 10.2374 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8194 - value_output_loss: 1.4180 - value_output_mse: 1.4184\n","Epoch 1: loss improved from inf to 10.23744, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 10.2374 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8194 - value_output_loss: 1.4180 - value_output_mse: 1.4184  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - loss: 7.1479 - policy_output_accuracy: 0.2500 - policy_output_loss: 7.1388 - value_output_loss: 0.0091 - value_output_mse: 0.0091\n","Epoch 1: loss improved from 10.23744 to 7.14793, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 7.1479 - policy_output_accuracy: 0.2500 - policy_output_loss: 7.1388 - value_output_loss: 0.0091 - value_output_mse: 0.0091   \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 6.8310 - policy_output_accuracy: 0.3000 - policy_output_loss: 6.8126 - value_output_loss: 0.0184 - value_output_mse: 0.0184\n","Epoch 1: loss improved from 7.14793 to 6.83102, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 6.8310 - policy_output_accuracy: 0.3000 - policy_output_loss: 6.8126 - value_output_loss: 0.0184 - value_output_mse: 0.0184   \n","\n","Average Policy Output Loss: 7.590274333953857\n","Average Value Output Loss: 0.4818552313372493\n","Iteration 5/7000\n"]}]}]}