{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t4jK41UULfwI"
      },
      "outputs": [],
      "source": [
        "#ULTRA SUPER SIGMA CHESS AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfMjit5oLfwI",
        "outputId": "e1894f7b-7121-412b-d17e-a7bad521e28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chess in /usr/local/lib/python3.10/dist-packages (1.11.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install chess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYnWlePILfwJ",
        "outputId": "e15232ae-1c2d-4dc2-ebc7-d66caeea50d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!chmod +x \"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rBlKKoQmLfwJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d0b07d-eddf-473e-d96b-3f0fb11efd42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All devices:  [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import math\n",
        "\n",
        "import chess\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "\n",
        "import chess.engine\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
        "\n",
        "class GameState:\n",
        "    row = 8\n",
        "    col = 8\n",
        "    promotion_indexes = {\n",
        "        chess.KNIGHT: 0,\n",
        "        chess.ROOK: 1,\n",
        "        chess.BISHOP: 2\n",
        "    }\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.board = chess.Board()\n",
        "        self.repetition_count = 0\n",
        "        self.player_color: chess.Color = chess.WHITE\n",
        "\n",
        "    def get_initial_state(self):\n",
        "        self.board.reset()\n",
        "\n",
        "        return self.get_current_state()\n",
        "\n",
        "    def get_current_state(self, T=8):\n",
        "        input_tensor = np.zeros((8, 8, 119), dtype=np.uint8)\n",
        "\n",
        "        for t in range(T):\n",
        "            _t = T - t - 1\n",
        "            if len(self.board.move_stack) < _t:\n",
        "                continue\n",
        "\n",
        "            self.create_input(input_tensor, _t)\n",
        "\n",
        "        color = 0 if self.board.turn == chess.WHITE else 1\n",
        "        input_tensor[:, :, 112] = color\n",
        "\n",
        "        input_tensor[:, :, 113] = len(self.board.move_stack) > 0\n",
        "\n",
        "        p1_castling = (1 * self.board.has_kingside_castling_rights(chess.WHITE)) | (2 * self.board.has_queenside_castling_rights(chess.WHITE))\n",
        "        p1_castling_bit = format(p1_castling, \"02b\")\n",
        "        input_tensor[:, :, 114] = int(p1_castling_bit[0])\n",
        "        input_tensor[:, :, 115] = int(p1_castling_bit[1])\n",
        "\n",
        "        p2_castling = (1 * self.board.has_kingside_castling_rights(chess.BLACK)) | (2 * self.board.has_queenside_castling_rights(chess.BLACK))\n",
        "        p2_castling_bit = format(p2_castling, \"02b\")\n",
        "        input_tensor[:, :, 116] = int(p2_castling_bit[0])\n",
        "        input_tensor[:, :, 117] = int(p2_castling_bit[1])\n",
        "\n",
        "        input_tensor[:, :, 118] = int(self.board.is_fifty_moves())\n",
        "\n",
        "        return np.expand_dims(input_tensor, axis=0)\n",
        "\n",
        "    def get_next_state(self, action: int):\n",
        "        source_index = action // 73\n",
        "        destination_index = 0\n",
        "        move_type = action % 73\n",
        "\n",
        "        promotion = None\n",
        "\n",
        "        if move_type < 56:\n",
        "            direction = move_type // 7\n",
        "            movement = (move_type % 7) + 1\n",
        "\n",
        "            destination_index = source_index + (movement * 8) if direction == 0 else destination_index\n",
        "            destination_index = source_index + (movement * 9) if direction == 1 else destination_index\n",
        "            destination_index = source_index + movement if direction == 2 else destination_index\n",
        "            destination_index = source_index + (movement * -7) if direction == 3 else destination_index\n",
        "            destination_index = source_index + (movement * -8) if direction == 4 else destination_index\n",
        "            destination_index = source_index + (movement * -9) if direction == 5 else destination_index\n",
        "            destination_index = source_index + (-movement) if direction == 6 else destination_index\n",
        "            destination_index = source_index + (movement * 7) if direction == 7 else destination_index\n",
        "        elif move_type >= 56 and move_type < 64:\n",
        "            direction = move_type - 56\n",
        "\n",
        "            destination_index = source_index + 17 if direction == 0 else destination_index\n",
        "            destination_index = source_index + 10 if direction == 1 else destination_index\n",
        "            destination_index = source_index - 6 if direction == 2 else destination_index\n",
        "            destination_index = source_index - 15 if direction == 3 else destination_index\n",
        "            destination_index = source_index - 17 if direction == 4 else destination_index\n",
        "            destination_index = source_index - 10 if direction == 5 else destination_index\n",
        "            destination_index = source_index + 6 if direction == 6 else destination_index\n",
        "            destination_index = source_index + 15 if direction == 7 else destination_index\n",
        "        else:\n",
        "            direction = (move_type - 64) // 3\n",
        "            promotion_index = (move_type - 64) % 3\n",
        "\n",
        "            promotion = chess.KNIGHT if promotion_index == 0 else (chess.ROOK if promotion_index == 1 else chess.BISHOP)\n",
        "\n",
        "            color_value = 1 if self.board.turn == chess.WHITE else -1\n",
        "\n",
        "            if direction == 0:\n",
        "                destination_index = source_index + (8 * color_value)\n",
        "            elif direction == 1:\n",
        "                destination_index = source_index + (9 * color_value)\n",
        "            else:\n",
        "                destination_index = source_index + (7 * color_value)\n",
        "\n",
        "        from_square = chess.Square(source_index)\n",
        "        to_square = chess.Square(destination_index)\n",
        "\n",
        "        promotion_rank = 7 if self.board.turn == chess.WHITE else 0\n",
        "\n",
        "        if promotion is None:\n",
        "            if self.board.piece_type_at(from_square) == chess.PAWN and chess.square_rank(to_square) == promotion_rank:\n",
        "                promotion = chess.QUEEN\n",
        "\n",
        "        move = chess.Move(from_square, to_square, promotion)\n",
        "\n",
        "        self.apply_action(move)\n",
        "\n",
        "        return move, self.get_current_state()\n",
        "\n",
        "    def apply_action(self, move: chess.Move):\n",
        "        try:\n",
        "            self.board.push(move)\n",
        "        except Exception as e:\n",
        "            print(list(self.board.legal_moves))\n",
        "            print(self.get_valid_moves())\n",
        "\n",
        "            print(e)\n",
        "\n",
        "            raise Exception(\"Error\")\n",
        "\n",
        "    def create_input(self, input_tensor: np.ndarray, t: int):\n",
        "        piece_types = {\n",
        "            chess.PAWN: 0,\n",
        "            chess.KNIGHT: 1,\n",
        "            chess.BISHOP: 2,\n",
        "            chess.ROOK: 3,\n",
        "            chess.QUEEN: 4,\n",
        "            chess.KING: 5\n",
        "        }\n",
        "\n",
        "        board = self.board.copy()\n",
        "        for _ in range(t):\n",
        "            board.pop()\n",
        "\n",
        "        transposition_key = board._transposition_key()\n",
        "\n",
        "        for square in chess.SQUARES:\n",
        "            piece = board.piece_at(square)\n",
        "\n",
        "            if piece is None:\n",
        "                continue\n",
        "\n",
        "            piece_index = piece_types[piece.piece_type]\n",
        "            piece_color = 0 if piece.color == chess.WHITE else 1\n",
        "\n",
        "            index = (t * 14) + (piece_color * 6) + piece_index\n",
        "            input_tensor[square // 8][square % 8][index] = 1\n",
        "\n",
        "        repetition_count = 0\n",
        "        index = (t * 14) + 12\n",
        "\n",
        "        try:\n",
        "            while board.move_stack:\n",
        "                move = board.pop()\n",
        "                if board.is_irreversible(move):\n",
        "                    break\n",
        "\n",
        "                if board._transposition_key() == transposition_key:\n",
        "                    repetition_count += 1\n",
        "\n",
        "                if repetition_count == 3:\n",
        "                    break\n",
        "        finally:\n",
        "            repetition_count = 3 if repetition_count > 3 else repetition_count\n",
        "\n",
        "            repetition_count_bits = [int(x) for x in format(repetition_count, \"02b\")]\n",
        "            input_tensor[:, :, index] = repetition_count_bits[0]\n",
        "            input_tensor[:, :, index + 1] = repetition_count_bits[1]\n",
        "\n",
        "    def get_valid_moves(self):\n",
        "        legal_moves = []\n",
        "\n",
        "        for valid_move in self.board.legal_moves:\n",
        "            s_row, s_col, from_square_index = self.index_of_square(valid_move.from_square)\n",
        "            d_row, d_col, to_square_index = self.index_of_square(valid_move.to_square)\n",
        "\n",
        "            if valid_move.promotion:\n",
        "                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n",
        "\n",
        "                if valid_move.promotion == chess.QUEEN:\n",
        "                    index = (from_square_index * 73) + (direction * 7)\n",
        "                    legal_moves.append(index)\n",
        "                else:\n",
        "                    promotion_index = self.promotion_indexes[valid_move.promotion]\n",
        "\n",
        "                    if direction > 2 and direction < 6:\n",
        "                        direction = 0 if direction == 4 else (1 if direction == 5 else 2)\n",
        "                    elif direction == 7:\n",
        "                        direction = 2\n",
        "\n",
        "                    index = (from_square_index * 73) + ((direction * 3) + promotion_index + 64)\n",
        "                    legal_moves.append(index)\n",
        "            elif self.board.piece_type_at(valid_move.from_square) == chess.KNIGHT:\n",
        "                direction = self.direction_of_move_for_knights(s_row, s_col, d_row, d_col)\n",
        "\n",
        "                index = (from_square_index * 73) + direction + 56\n",
        "                legal_moves.append(index)\n",
        "\n",
        "            else:\n",
        "                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n",
        "                count_of_square = self.count_of_square_for_movement(s_row, s_col, d_row, d_col) - 1\n",
        "\n",
        "                index = (from_square_index * 73) + ((direction * 7) + count_of_square)\n",
        "                legal_moves.append(index)\n",
        "\n",
        "        return legal_moves\n",
        "\n",
        "    def index_of_square(self, square: chess.Square):\n",
        "        row = chess.square_rank(square)\n",
        "        col = chess.square_file(square)\n",
        "        index = (row * 8) + col\n",
        "\n",
        "        return row, col, index\n",
        "\n",
        "    def direction_of_move_for_ray_directions(self, s_row: int, s_col: int, d_row: int, d_col: int):\n",
        "        delta_x = d_col - s_col\n",
        "        delta_y = d_row - s_row\n",
        "\n",
        "        if delta_x == 0:\n",
        "            return 0 if delta_y > 0 else 4\n",
        "\n",
        "        if delta_y == 0:\n",
        "            return 2 if delta_x > 0 else 6\n",
        "\n",
        "        if delta_x < 0:\n",
        "            return 7 if delta_y > 0 else 5\n",
        "\n",
        "        return 1 if delta_y > 0 else 3\n",
        "\n",
        "    def direction_of_move_for_knights(self, s_row: int, s_col: int, d_row: int, d_col: int):\n",
        "        delta_x = d_col - s_col\n",
        "        delta_y = d_row - s_row\n",
        "\n",
        "        if delta_x == 1:\n",
        "            return 0 if delta_y > 0 else 3\n",
        "\n",
        "        if delta_x == 2:\n",
        "            return 1 if delta_y > 0 else 2\n",
        "\n",
        "        if delta_x == -1:\n",
        "            return 7 if delta_y > 0 else 4\n",
        "\n",
        "        return 6 if delta_y > 0 else 5\n",
        "\n",
        "    def count_of_square_for_movement(self, s_row: int, s_col: int, d_row: int, d_col: int):\n",
        "        delta_x = d_col - s_col\n",
        "        delta_y = d_row - s_row\n",
        "\n",
        "        return max(abs(delta_x), abs(delta_y))\n",
        "\n",
        "    def get_winner(self):\n",
        "        result = self.board.result()\n",
        "\n",
        "        if result == \"1-0\":\n",
        "            return chess.WHITE\n",
        "\n",
        "        if result == \"0-1\":\n",
        "            return chess.BLACK\n",
        "\n",
        "        return 2\n",
        "\n",
        "    def is_terminal(self):\n",
        "        return self.board.is_game_over()\n",
        "\n",
        "    def clone(self):\n",
        "        cloned_state = GameState()\n",
        "        cloned_state.board = self.board.copy()\n",
        "\n",
        "        return cloned_state\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, state, parent=None, prior_prob=1.0):\n",
        "        self.state = state\n",
        "        self.parent = parent\n",
        "        self.children = {}\n",
        "        self.visits = 0\n",
        "        self.value_sum = 0\n",
        "        self.prior_prob = prior_prob\n",
        "        self.is_expanded = False\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.value_sum / (self.visits + 1e-5)\n",
        "\n",
        "    def expand(self, action_probs):\n",
        "        if not self.children:\n",
        "            for action, prob in enumerate(action_probs):\n",
        "                if prob > 0 and action not in self.children:\n",
        "                    next_state = self.state.clone()\n",
        "                    next_state.get_next_state(action)\n",
        "                    self.children[action] = Node(next_state, parent=self, prior_prob=prob)\n",
        "\n",
        "            self.is_expanded = len(self.children) > 0\n",
        "\n",
        "    def select(self, c_puct=1.0):\n",
        "      if not self.children:  # If no children exist\n",
        "          return None, None\n",
        "\n",
        "      max_ucb = -float('inf')\n",
        "      best_action = None\n",
        "      best_child = None\n",
        "\n",
        "      for action, child in self.children.items():\n",
        "          ucb = child.value + c_puct * child.prior_prob * (math.sqrt(self.visits) / (1 + child.visits))\n",
        "          if ucb > max_ucb:\n",
        "              max_ucb = ucb\n",
        "              best_action = action\n",
        "              best_child = child\n",
        "\n",
        "      return best_action, best_child\n",
        "\n",
        "    def backup(self, value):\n",
        "        self.visits += 1\n",
        "        self.value_sum += value\n",
        "        if self.parent:\n",
        "            self.parent.backup(-value)\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self, model, c_puct=1.0, simulations=300):\n",
        "        self.model = model\n",
        "        self.c_puct = c_puct\n",
        "        self.simulations = simulations\n",
        "\n",
        "    def add_dirichlet_noise(self, node, valid_moves):\n",
        "        noise = np.random.dirichlet([0.3] * len(valid_moves))\n",
        "        for idx, action in enumerate(valid_moves):\n",
        "            if action in node.children:\n",
        "                node.children[action].prior_prob = \\\n",
        "                    0.75 * node.children[action].prior_prob + 0.25 * noise[idx]\n",
        "\n",
        "    def run(self, initial_state, temperature=1.0):\n",
        "        root = Node(initial_state)\n",
        "\n",
        "        action_probs, value = self.evaluate(initial_state)\n",
        "        valid_moves = initial_state.get_valid_moves()\n",
        "\n",
        "        noise = np.random.dirichlet([0.3] * len(valid_moves))\n",
        "\n",
        "        for idx, action in enumerate(valid_moves):\n",
        "            prob = action_probs[action]\n",
        "            noisy_prob = 0.75 * prob + 0.25 * noise[idx]\n",
        "            next_state = initial_state.clone()\n",
        "            next_state.get_next_state(action)\n",
        "            root.children[action] = Node(next_state, parent=root, prior_prob=noisy_prob)\n",
        "\n",
        "        for _ in range(self.simulations):\n",
        "            node = root\n",
        "\n",
        "            while node.children and not node.state.is_terminal():\n",
        "                action, child_node = node.select(self.c_puct)\n",
        "\n",
        "                if child_node is None:\n",
        "                    break\n",
        "\n",
        "                node = child_node\n",
        "\n",
        "            if not node.state.is_terminal():\n",
        "                action_probs, value = self.evaluate(node.state)\n",
        "                valid_moves = node.state.get_valid_moves()\n",
        "                node.expand(action_probs)\n",
        "            else:\n",
        "                value = node.state.get_winner()\n",
        "                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n",
        "\n",
        "            node.backup(value)\n",
        "\n",
        "        return self.get_action_probs(root, temperature)\n",
        "\n",
        "    def evaluate(self, state):\n",
        "        state_tensor = state.get_current_state()\n",
        "        # state_tensor = np.expand_dims(state_tensor, axis=0)\n",
        "\n",
        "        policy, value = self.model.predict(state_tensor, verbose=0)\n",
        "\n",
        "        # Mask invalid moves\n",
        "        valid_moves = state.get_valid_moves()\n",
        "        mask = np.zeros(policy.shape[1])\n",
        "        mask[valid_moves] = 1\n",
        "\n",
        "        policy = policy[0] * mask\n",
        "\n",
        "        # Normalize\n",
        "        sum_policy = np.sum(policy)\n",
        "        if sum_policy > 0:\n",
        "            policy /= sum_policy\n",
        "        else:\n",
        "            # If all moves were masked, use uniform distribution over valid moves\n",
        "            policy = mask / np.sum(mask)\n",
        "\n",
        "        return policy, value[0][0]\n",
        "\n",
        "    def get_action_probs(self, root, temperature=1.0):\n",
        "        visits = np.array([child.visits for action, child in root.children.items()])\n",
        "        actions = list(root.children.keys())\n",
        "\n",
        "        if temperature == 0:  # Pure exploitation\n",
        "            action_idx = np.argmax(visits)\n",
        "            probs = np.zeros_like(visits)\n",
        "            probs[action_idx] = 1\n",
        "        else:\n",
        "            # Apply temperature\n",
        "            visits = visits ** (1 / temperature)\n",
        "            probs = visits / np.sum(visits)\n",
        "\n",
        "        # Convert to full move probability vector\n",
        "        full_probs = np.zeros(4672)  # Adjust size based on your action space\n",
        "        for action, prob in zip(actions, probs):\n",
        "            full_probs[action] = prob\n",
        "\n",
        "        return full_probs\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, maxlen=500000):\n",
        "        self.buffer = deque(maxlen=maxlen)\n",
        "        self.current_size = 0\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def store(self, state, policy, value):\n",
        "        \"\"\"Store a single game state transition\"\"\"\n",
        "        self.buffer.append({\n",
        "            'state': state,\n",
        "            'policy': policy,\n",
        "            'value': value\n",
        "        })\n",
        "        self.current_size = len(self.buffer)\n",
        "\n",
        "    def store_multiple_data(self, states, policies, value):\n",
        "        with self.lock:\n",
        "            for s, p, v in zip(states, policies, [value]):\n",
        "                self.store(s, p, v)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample a batch with augmentations\"\"\"\n",
        "        if self.current_size < batch_size:\n",
        "            batch_size = self.current_size\n",
        "\n",
        "        indices = np.random.choice(self.current_size, batch_size)\n",
        "        states, policies, values = [], [], []\n",
        "\n",
        "        for idx in indices:\n",
        "            sample = self.buffer[idx]\n",
        "            # Get augmented samples\n",
        "            aug_states, aug_policies = self._augment_sample(\n",
        "                sample['state'],\n",
        "                sample['policy']\n",
        "            )\n",
        "\n",
        "            # Add all augmentations\n",
        "            states.extend(aug_states)\n",
        "            policies.extend(aug_policies)\n",
        "            values.extend([sample['value']] * len(aug_states))\n",
        "\n",
        "        return np.array(states), np.array(policies), np.array(values)\n",
        "\n",
        "    def _augment_sample(self, state, policy):\n",
        "        \"\"\"Generate valid augmentations for a single sample\"\"\"\n",
        "        # Remove batch dimension if present\n",
        "        if len(state.shape) == 4:\n",
        "            state = np.squeeze(state, axis=0)\n",
        "\n",
        "        augmented_states = [state]\n",
        "        augmented_policies = [policy]\n",
        "\n",
        "        # Horizontal flip\n",
        "        flip_h = np.flip(state, axis=1)\n",
        "        augmented_states.append(flip_h)\n",
        "        augmented_policies.append(policy)  # Policy needs game-specific mapping\n",
        "\n",
        "        # Vertical flip\n",
        "        flip_v = np.flip(state, axis=0)\n",
        "        augmented_states.append(flip_v)\n",
        "        augmented_policies.append(policy)  # Policy needs game-specific mapping\n",
        "\n",
        "        # Diagonal flip (only if shape allows)\n",
        "        if state.shape[0] == state.shape[1]:\n",
        "            diag = np.transpose(state, (1, 0, 2))\n",
        "            augmented_states.append(diag)\n",
        "            augmented_policies.append(policy)  # Policy needs game-specific mapping\n",
        "\n",
        "        return augmented_states, augmented_policies\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.current_size\n",
        "\n",
        "# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "\n",
        "# tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n",
        "\n",
        "print(\"All devices: \", tf.config.list_logical_devices('GPU'))\n",
        "\n",
        "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "tf.keras.mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yEEbn7CxLfwL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsoo7SmaO1tk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5897e973-806a-4e28-95de-091cdfb20576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1/7000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7c396011fd90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False r1bQ1knr/ppp2ppp/8/4p3/4P3/N4N2/PPP2PPP/R1B1KB1R\n",
            "True r3k1nr/pppb1ppp/2n5/2b1p2q/8/PP5P/2PPB3/RNBK3q\n",
            "True rn2k2r/pp2bppp/6b1/1P2p3/2ppn3/P1P1P3/1R1PKqPP/2BQ1BNR\n",
            "True r3kb1r/1p3ppp/p7/3pp3/PP1n2n1/6Pb/2qK3P/RNB1Q2R\n",
            "True r4rk1/1p3pbp/2n2np1/p1p5/P1pp4/1P4Pb/RBP2P1P/5qKR\n",
            "True r4b1r/pp1k1ppp/2n1b3/2p2q2/1P2K1n1/P2PP1P1/2P2P1P/R1B2BR1\n",
            "False 3k1bnr/3Q1p2/2B1p2p/p4bp1/8/2N5/PPPB1PPP/R3K1NR\n",
            "True rnb1kb1r/ppp1pppp/8/8/P5n1/6P1/1PPP1p1P/RNBKqBNR\n",
            "True rnb1k1nr/pppp1ppp/8/4p3/8/2P2Pq1/PP1PP3/R1BQKBN1\n",
            "False rn1k1Qnr/1b1p3p/1p1P1Np1/p4p2/2p1P3/2B2N2/PPP1BPPP/R3R1K1\n",
            "False r1bqkQ2/pppp1p2/7B/4n3/3PP3/8/PPP2PPP/RN2KBNR\n",
            "True 4rbnr/pp1k1ppp/8/3p4/1p3PbP/P3P1P1/2n1qK2/RNB3R1\n",
            "False 2r2k1Q/5p1p/1pp3p1/p1P3B1/8/2N2N1P/PPP2PBP/R3K2R\n",
            "False r2k1bnr/1b1Qp3/p1p3B1/1pP1N2p/1P6/2N5/1PP2PPP/R1B1R1K1\n",
            "False 1n1Q3R/1b1k4/2pp4/8/3PPN2/4B3/PPP2PPK/4R2R\n",
            "Self-play completed\n",
            "Loaded existing validation data from validation_data.npy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94s/step - loss: 12.3022 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3201 - value_output_loss: 2.9821 - value_output_mse: 2.9823\n",
            "Epoch 1: loss improved from inf to 12.30221, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 105s/step - loss: 12.3022 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3201 - value_output_loss: 2.9821 - value_output_mse: 2.9823\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 12.2009 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3204 - value_output_loss: 2.8805 - value_output_mse: 2.8796\n",
            "Epoch 1: loss improved from 12.30221 to 12.20093, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 12.2009 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3204 - value_output_loss: 2.8805 - value_output_mse: 2.8796   \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 12.0395 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1547 - value_output_loss: 2.8848 - value_output_mse: 2.8843\n",
            "Epoch 1: loss improved from 12.20093 to 12.03947, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 12.0395 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1547 - value_output_loss: 2.8848 - value_output_mse: 2.8843   \n",
            "\n",
            "Average Policy Output Loss: 9.265071233113607\n",
            "Average Value Output Loss: 2.9158002535502114\n",
            "Iteration 2/7000\n",
            "False rnbqkbr1/1pppnBpp/8/p3N3/4P3/8/PPPP1PPP/RNBQK2R\n",
            "False 1nb4k/1pppn2Q/6B1/8/p3P3/b7/PPPP1PPP/RNB1K2R\n",
            "True 3rk2r/1pp1nppp/2n5/p2b4/P2P4/bPp3PP/2Pq1P1R/1Q2KB2\n",
            "False 3R1k2/3p2rp/p6B/2PB1pp1/8/5N2/P1P1QPPP/4K2R\n",
            "False Bnk1Q3/2p2p2/3p2pB/p1P5/7p/2N5/PPP2PPP/R3K1NR\n",
            "False rn1kQ3/p6p/P1p2Np1/3p4/3P1BP1/1PNB3P/1P3P2/R3K2R\n",
            "True r1b1kb1r/ppp2ppp/2n2n2/3qp3/2K4P/1P3NP1/P4P2/RNBn1B1R\n",
            "True r2qk1nr/ppp2pp1/2nbb3/P3p3/4P2p/2P5/5PPP/3q2K1\n",
            "True r1b1k2r/1p1n1pp1/2p4p/2bpp3/2P5/P1P2P2/3BPq2/R2Q1KNn\n",
            "False 4R3/r1pp1ppp/p3k2n/4P3/4P3/P1NB1N2/P1P2PPP/2BQK2R\n",
            "False 1rbq1b1r/1pp1nP2/n3kQp1/pN2N1Bp/3PP3/8/PP3PPP/R3KB1R\n",
            "False rn4Q1/p2pk1pp/b1p1p3/1pP1NpB1/4P3/2PB4/PPN2PPP/R4RK1\n",
            "False 1nbk3r/p1NpBp1p/1ppQ2pn/4P3/4P3/2P5/PP3PPP/R3KBNR\n",
            "True rn1qkbnr/ppp2ppp/8/3p4/P1P5/4P2b/RP1P1P1P/1NBQKq2\n",
            "False r3kbnr/p2B4/3pP3/2p3Bp/3P4/2N2N2/PPP2PPP/R2QK2R\n",
            "Self-play completed\n",
            "Loaded existing validation data from validation_data.npy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - loss: 12.0526 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0686 - value_output_loss: 2.9841 - value_output_mse: 2.9844\n",
            "Epoch 1: loss improved from inf to 12.05264, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 12.0526 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0686 - value_output_loss: 2.9841 - value_output_mse: 2.9844   \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - loss: 12.1110 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1858 - value_output_loss: 2.9252 - value_output_mse: 2.9265\n",
            "Epoch 1: loss did not improve from 12.05264\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 12.1110 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1858 - value_output_loss: 2.9252 - value_output_mse: 2.9265\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 12.2637 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1988 - value_output_loss: 3.0649 - value_output_mse: 3.0651\n",
            "Epoch 1: loss did not improve from 12.05264\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 12.2637 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1988 - value_output_loss: 3.0649 - value_output_mse: 3.0651\n",
            "\n",
            "Average Policy Output Loss: 9.151068369547525\n",
            "Average Value Output Loss: 2.9913904666900635\n",
            "Iteration 3/7000\n",
            "False rnbqkb1r/ppppnBpp/8/4N3/4P3/8/PPPP1PPP/RNBQK2R\n",
            "False r1bq1kr1/2ppnQpp/1pn5/1N2NP2/p2P4/8/PPP2PPP/R1B1RBK1\n",
            "False rnb1k2r/2p1Q2p/p2p1PP1/1p6/8/3B1N2/PPPB1PPP/RN2K2R\n",
            "False rn1q1br1/p3kPpp/1pp1Q3/3p4/3N1B2/2N5/PPP2PPP/R3R1K1\n",
            "False r5rb/1bNp1pkp/p6Q/n1P1p1P1/B3PB2/8/PPP1NPP1/1R2K2R\n",
            "True r2qk2r/1pp2ppp/2n3P1/5n2/P7/1bpppP1N/1b5P/1NK2B1R\n",
            "False r3kbnr/pb1Qpp1p/1p4p1/1Bp5/4P2P/5N2/PPP2PP1/RNB1K2R\n",
            "True r3k1nr/ppp2pp1/4b3/2b4p/1n1p2P1/2K2P1P/PPPP4/RNB1qBNR\n",
            "True rnb1k1nr/ppp2ppp/8/3p4/5P1q/8/PPPPP3/RNBQKBN1\n",
            "True rn2kbnr/pp3ppp/4b3/2ppp3/8/PPP2PqP/3PP3/RNBQKBNR\n",
            "False r1bk4/p2pQ3/n2Bp1pp/1Bp5/4P3/2N2N1P/PPP2PP1/R3K2R\n",
            "True r2qk2r/ppp2ppp/3b4/8/1Pp1n3/P3Pb1P/2nP1K2/RNB2B1q\n",
            "False 1Q1k1br1/4p2p/3p1p2/NB6/4P1p1/2N1B3/PPP2PPP/R3K2R\n",
            "False 1Q1k3r/p7/P2p2p1/1B1N1p2/2pP3p/2P5/1P1N1PPP/R1B1K2R\n",
            "False r2qk3/4bQ2/3p4/pN2N2p/4P3/3BB3/PPP2PPP/R3K2R\n",
            "Self-play completed\n",
            "Loaded existing validation data from validation_data.npy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 12.3577 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.4161 - value_output_loss: 2.9416 - value_output_mse: 2.9424\n",
            "Epoch 1: loss improved from inf to 12.35772, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - loss: 12.3577 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.4161 - value_output_loss: 2.9416 - value_output_mse: 2.9424 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - loss: 12.2855 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2393 - value_output_loss: 3.0462 - value_output_mse: 3.0472\n",
            "Epoch 1: loss improved from 12.35772 to 12.28550, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - loss: 12.2855 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2393 - value_output_loss: 3.0462 - value_output_mse: 3.0472   \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - loss: 12.3631 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2981 - value_output_loss: 3.0650 - value_output_mse: 3.0654\n",
            "Epoch 1: loss did not improve from 12.28550\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - loss: 12.3631 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2981 - value_output_loss: 3.0650 - value_output_mse: 3.0654\n",
            "\n",
            "Average Policy Output Loss: 9.317826271057129\n",
            "Average Value Output Loss: 3.0176193714141846\n",
            "Iteration 4/7000\n",
            "False rnbqkbr1/p2ppBPp/8/7Q/8/2N5/PPPP1PPP/R1B1K1NR\n",
            "False 2Q2kr1/p2p1pp1/8/3Np2p/3P4/P7/P1P2KPP/R1B2BNR\n",
            "False 4Q1k1/p1p3pp/8/1p6/8/8/PPPP1PPP/RNB1KBNR\n",
            "False 1r2kQ2/pp2p3/4B2B/P2P4/3p4/2N3P1/1PP2PP1/R3K2R\n",
            "True 4kbnr/1pp2ppp/p1n1b3/4p3/2P2P2/P3P1PN/3r1K1P/3q3R\n",
            "True 1r1q1r1k/p1p2ppp/Pp4b1/3p4/3P4/bPn1P2P/4BnP1/RK6\n",
            "False rn4nr/pNp3p1/3k1p1p/1B1P4/8/P1N5/P1P2PPP/R1BQR1K1\n",
            "True 3rk2r/ppp2ppp/2n5/1n2p3/1P6/K1PP1P2/P1qN4/R7\n",
            "False 3k1b1r/1rpQ1p2/pp2p1p1/4N2p/3P4/2N5/PPP2PPP/R1B1K1R1\n",
            "True rn1qk2r/ppp2pp1/8/4p3/2p5/PP1P1N1b/R3PbKP/1N1n1B1R\n",
            "True rnb1k1nr/ppp2ppp/8/3pp3/8/1P3Pq1/P1PPP3/RNBQKBN1\n",
            "True 2kr3r/1p2bpp1/1qn2n1p/p3p3/P1b1P1P1/5PRP/2P3B1/2r1K3\n",
            "True rn3rk1/ppp2ppp/8/4p3/P6P/4BbP1/7R/b3q1K1\n",
            "True r3k2r/1pp2pp1/p2bb2p/3p3n/P2n4/R4Pp1/1PP1P1R1/1Nq1KB2\n",
            "True r3k2r/pp3ppp/3b4/2p2b2/2Pn4/P3p3/1P3qPP/R4KNR\n",
            "Self-play completed\n",
            "Loaded existing validation data from validation_data.npy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 12.0247 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0426 - value_output_loss: 2.9821 - value_output_mse: 2.9813\n",
            "Epoch 1: loss improved from inf to 12.02469, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 12.0247 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0426 - value_output_loss: 2.9821 - value_output_mse: 2.9813 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 11.3939 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8322 - value_output_loss: 2.5617 - value_output_mse: 2.5620\n",
            "Epoch 1: loss improved from 12.02469 to 11.39394, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 11.3939 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8322 - value_output_loss: 2.5617 - value_output_mse: 2.5620   \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - loss: 10.5600 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8155 - value_output_loss: 1.7445 - value_output_mse: 1.7444\n",
            "Epoch 1: loss improved from 11.39394 to 10.56003, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step - loss: 10.5600 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.8155 - value_output_loss: 1.7445 - value_output_mse: 1.7444   \n",
            "\n",
            "Average Policy Output Loss: 8.89680004119873\n",
            "Average Value Output Loss: 2.429417689641317\n",
            "Iteration 5/7000\n",
            "False 1nbk2r1/r1p1Q3/4p1p1/p5Bp/3P4/1P1B4/1PP2PPP/RN2K2R\n",
            "False r1b5/P2p3p/1p2p2n/2k5/1n2PB2/P1Q2P2/1PP3PP/R3KBNR\n",
            "True rnb1k2r/ppp2ppp/8/4p3/P3PbPq/3p3P/1PP5/R1BQKB2\n",
            "True r3kr2/p1pb3p/p3ppnp/4b3/6P1/1p1PKP2/P7/4q3\n",
            "False r3R2r/2Q2P2/1kn5/p1p4p/2B2B2/5N2/PPP2PPP/2R3K1\n",
            "True rnb1k1nr/ppp2pp1/6p1/8/1b1P1p2/5P1P/3KP3/1q3B1R\n",
            "True 1r2r1k1/ppp1n1pp/4bp2/1P2b3/P2p2PP/Kq1PnB2/8/RN6\n",
            "False r1b2knr/2Bp1Q2/1pn4p/p1P1Np2/4P3/8/PPP2PPP/RN2KB1R\n",
            "False rn3b1r/1b1pPp2/pP5p/6p1/1BNk4/2PB1Q2/PP3PPP/R4R1K\n",
            "False rnbq3r/ppp1bP1p/2k3p1/1Q6/3P1B2/8/PPP2PPP/RN2KBNR\n",
            "True rnb1kbnr/pppp1ppp/8/4p3/5P2/6qP/PPPPP3/RNBQKBNR\n",
            "True rn2k2r/1p2bppp/p3bn2/2p5/2q3P1/2K3p1/1P1PP2P/2B1QB2\n",
            "False 1B3b2/3p1n1B/1p1P1pN1/p1pPp1p1/7k/8/PPP2PPP/R2QR1K1\n",
            "False 2Q2kn1/3p1pp1/2pP3r/p1P1p1Bp/8/2NB1N2/1PP2PPP/R2QK2R\n",
            "False 1Q6/kp6/2r1p3/pBPpB3/8/2N5/PPP1NPPP/R2QK2R\n",
            "Self-play completed\n",
            "Loaded existing validation data from validation_data.npy\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - loss: 11.7927 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2374 - value_output_loss: 2.5553 - value_output_mse: 2.5552\n",
            "Epoch 1: loss improved from inf to 11.79270, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step - loss: 11.7927 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2374 - value_output_loss: 2.5553 - value_output_mse: 2.5552 \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - loss: 10.3441 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0321 - value_output_loss: 1.3120 - value_output_mse: 1.3122\n",
            "Epoch 1: loss improved from 11.79270 to 10.34415, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 10.3441 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0321 - value_output_loss: 1.3120 - value_output_mse: 1.3122   \n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - loss: 9.3838 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.9291 - value_output_loss: 0.4547 - value_output_mse: 0.4547\n",
            "Epoch 1: loss improved from 10.34415 to 9.38380, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - loss: 9.3838 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 8.9291 - value_output_loss: 0.4547 - value_output_mse: 0.4547   \n",
            "\n",
            "Average Policy Output Loss: 9.066213289896647\n",
            "Average Value Output Loss: 1.4406685133775075\n",
            "\n",
            "[After 5 Iterations]\n",
            "Average Policy Output Loss: 9.139395840962727\n",
            "Average Value Output Loss: 2.5589792589346567\n",
            "Iteration 6/7000\n",
            "False 1nb2knr/1ppp1Q1p/r3p3/p1P2PN1/8/2NB4/PPP2PPP/R3K2R\n",
            "False r4b1r/pbBBk2p/3Ppp1n/4N3/4P3/2N5/PPPQ1PPP/R3K2R\n",
            "True r1b1k1nr/ppp2ppp/P1q5/3K4/2P4P/1P1pP3/1bnP1PP1/1N3BNR\n",
            "True r3k2r/ppp2ppp/2np4/4p3/P5n1/BP1bb3/2P3PP/1NQ1KqNR\n",
            "False r1bq4/1pp1nP1N/8/k1BBP3/8/1PN2Q2/2P2PPP/R4RK1\n",
            "True r1b1k2r/ppp2ppp/2n2n2/2b1p3/8/B4PP1/4NqKP/R1r2B1R\n",
            "False rnbk1Qnr/ppqp3p/8/2p1p1N1/4P3/8/PPPP1PPP/RNB1KB1R\n",
            "False r2k1b1r/p1pBp1pp/2PpNp2/8/4P3/1P2B3/1PP2PPP/RN1QK2R\n",
            "True r3kbnr/1pp2ppp/2n5/1B2p3/4P1b1/2P2PP1/P2P3q/q3K3\n",
            "True r3kb1r/ppp1ppp1/5n2/nN5p/8/3b1PqP/PP6/R4K2\n",
            "False rnbk1Qnr/ppqpp2p/8/2p4Q/8/8/PPPP1PPP/RNB1KBNR\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import threading\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "\n",
        "def EfficientNet_block(inputs, filters=256, expansion_factor=6, stride=1, se_ratio=0.25):\n",
        "    \"\"\"\n",
        "    EfficientNet tarzı bir MBConv bloğu.\n",
        "\n",
        "    Args:\n",
        "        inputs: Giriş tensörü.\n",
        "        filters: Çıkış filtrelerinin sayısı.\n",
        "        expansion_factor: Genişletme oranı (Bottleneck genişletme için).\n",
        "        stride: Adım boyutu (ör. 1 veya 2).\n",
        "        se_ratio: Squeeze-and-Excitation (SE) oranı.\n",
        "\n",
        "    Returns:\n",
        "        EfficientNet MBConv bloğunun çıktısı.\n",
        "    \"\"\"\n",
        "    input_channels = inputs.shape[-1]  # Giriş kanal sayısı\n",
        "\n",
        "    # 1. Expand (Genişletme)\n",
        "    x = layers.Conv2D(input_channels * expansion_factor, (1, 1), strides=1, padding=\"same\",\n",
        "                      kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # 2. Depthwise Convolution (Derinlikçe Ayrık Evrişim)\n",
        "    x = layers.DepthwiseConv2D((3, 3), strides=stride, padding=\"same\",\n",
        "                               depthwise_initializer=\"he_normal\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # 3. Squeeze-and-Excitation (SE Blok)\n",
        "    if se_ratio:\n",
        "        se = layers.GlobalAveragePooling2D()(x)\n",
        "        se = layers.Reshape((1, 1, x.shape[-1]))(se)\n",
        "        se = layers.Conv2D(int(input_channels * se_ratio), (1, 1), activation=\"relu\", kernel_initializer=\"he_normal\")(se)\n",
        "        se = layers.Conv2D(x.shape[-1], (1, 1), activation=\"sigmoid\", kernel_initializer=\"he_normal\")(se)\n",
        "        x = layers.Multiply()([x, se])\n",
        "\n",
        "    # 4. Project (Daraltma)\n",
        "    x = layers.Conv2D(filters, (1, 1), strides=1, padding=\"same\",\n",
        "                      kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # 5. Shortcut Connection (Skip Connection)\n",
        "    if stride == 1 and inputs.shape[-1] == filters:\n",
        "        x = layers.Add()([inputs, x])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "def sigmachess_network(input_shape=(8, 8, 119)):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    for _ in range(19):\n",
        "        x = EfficientNet_block(x)\n",
        "\n",
        "    policy = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
        "    policy = layers.BatchNormalization()(policy)\n",
        "    policy = layers.ReLU()(policy)\n",
        "    policy = layers.Conv2D(73, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(policy)\n",
        "    policy = layers.Flatten()(policy)\n",
        "    policy = layers.Softmax(name=\"policy_output\")(policy)\n",
        "\n",
        "    value = layers.Conv2D(1, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
        "    value = layers.BatchNormalization()(value)\n",
        "    value = layers.ReLU()(value)\n",
        "    value = layers.Flatten()(value)\n",
        "    value = layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(value)\n",
        "    value = layers.Dense(1, activation=\"tanh\", name=\"value_output\", kernel_initializer=\"he_normal\")(value)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=[policy, value])\n",
        "\n",
        "    return model\n",
        "\n",
        "def create_model():\n",
        "    model = sigmachess_network()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.00001),\n",
        "        loss={\n",
        "            \"policy_output\": \"categorical_crossentropy\",\n",
        "            \"value_output\": \"mean_squared_error\"\n",
        "        },\n",
        "        metrics={\n",
        "            \"policy_output\": \"accuracy\",\n",
        "            \"value_output\": \"mse\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def play_vs_stockfish(model, game, replay_buffer):\n",
        "    state = GameState()\n",
        "    temperature = 1.0 if game < 5 else 0.1\n",
        "\n",
        "    w_states, w_policies, w_rewards = [], [], []\n",
        "    player = np.random.choice([chess.WHITE, chess.BLACK])\n",
        "\n",
        "    engine = chess.engine.SimpleEngine.popen_uci(r\"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\")\n",
        "\n",
        "    while not state.is_terminal():\n",
        "        if state.board.turn == player:\n",
        "            mcts = MCTS(model, 1.0, 10)\n",
        "            action_probs = mcts.run(state, temperature)\n",
        "\n",
        "            w_states.append(state.get_current_state())\n",
        "            w_policies.append(action_probs)\n",
        "\n",
        "            action = np.random.choice(len(action_probs), p=action_probs)\n",
        "            state.get_next_state(action)\n",
        "        else:\n",
        "            result = engine.play(state.board, chess.engine.Limit(0.04))\n",
        "            state.apply_action(result.move)\n",
        "\n",
        "    engine.close()\n",
        "\n",
        "    winner = state.get_winner()\n",
        "    w_value = 1 if winner == player else (0 if winner == 2 else -1)\n",
        "\n",
        "    print(player, state.board.board_fen())\n",
        "\n",
        "    replay_buffer.store_multiple_data(w_states, w_policies, w_value)\n",
        "\n",
        "def self_play(model, num_games=100, max_workers=5):\n",
        "    replay_buffer = ReplayBuffer(maxlen=500000)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [\n",
        "            executor.submit(play_vs_stockfish, model, i, replay_buffer)\n",
        "            for i in range(num_games)\n",
        "        ]\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            future.result()\n",
        "\n",
        "    print(\"Self-play completed\")\n",
        "\n",
        "    return replay_buffer\n",
        "\n",
        "def prepare_validation_data(replay_buffer=None, filename=\"validation_data.npy\"):\n",
        "    try:\n",
        "        # First, try to load existing validation data\n",
        "        data = np.load(filename, allow_pickle=True).item()\n",
        "        print(f\"Loaded existing validation data from {filename}\")\n",
        "        return data['x_val'], {\n",
        "            \"policy_output\": data['y_val_policy'],\n",
        "            \"value_output\": data['y_val_value']\n",
        "        }\n",
        "    except (FileNotFoundError, IOError):\n",
        "        # If no existing data, create new validation dataset\n",
        "        if replay_buffer is None:\n",
        "            # If no replay buffer provided, generate synthetic data\n",
        "            x_val = np.random.random((100, 8, 8, 119))\n",
        "            y_val_policy = np.random.randint(0, 73, size=(100, 1))\n",
        "            y_val_value = np.random.random((100, 1)) * 2 - 1  # Values between -1 and 1\n",
        "        else:\n",
        "            # Use replay buffer to generate validation data\n",
        "            states, policies, rewards = replay_buffer.sample(100)\n",
        "            x_val = np.squeeze(states)\n",
        "            y_val_policy = policies\n",
        "            y_val_value = np.array(rewards).reshape(-1, 1)\n",
        "\n",
        "        # One-hot encode policy output\n",
        "        y_val_policy_onehot = np.eye(73)[y_val_policy.flatten()]\n",
        "\n",
        "        # Prepare data dictionary\n",
        "        validation_data = {\n",
        "            \"x_val\": x_val,\n",
        "            \"y_val_policy\": y_val_policy_onehot,\n",
        "            \"y_val_value\": y_val_value\n",
        "        }\n",
        "\n",
        "        # Save the validation data\n",
        "        np.save(filename, validation_data)\n",
        "        print(f\"Created and saved new validation data to {filename}\")\n",
        "\n",
        "        return x_val, {\n",
        "            \"policy_output\": y_val_policy_onehot,\n",
        "            \"value_output\": y_val_value\n",
        "        }\n",
        "\n",
        "def create_callbacks(checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=checkpoint_path,\n",
        "        save_weights_only=True,\n",
        "        monitor=\"loss\",\n",
        "        mode=\"min\",\n",
        "        save_best_only=True,\n",
        "        save_freq=\"epoch\",\n",
        "        verbose=1\n",
        "    )\n",
        "    return [checkpoint]\n",
        "\n",
        "def train_model(model, replay_buffer: ReplayBuffer, batch_size=256, epochs=3, checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n",
        "    x_val, y_val = prepare_validation_data()\n",
        "\n",
        "    callbacks = create_callbacks(checkpoint_path)\n",
        "\n",
        "    total_policy_loss = 0\n",
        "    total_value_loss = 0\n",
        "    epoch_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        states, policies, values = replay_buffer.sample(batch_size)\n",
        "\n",
        "        states = np.squeeze(states)\n",
        "        if len(states.shape) == 3:\n",
        "            states = np.expand_dims(states, -1)\n",
        "\n",
        "        values = np.array(values).reshape(-1, 1)\n",
        "\n",
        "        history = model.fit(\n",
        "            states,\n",
        "            { \"policy_output\": policies, \"value_output\": values },\n",
        "            batch_size=batch_size,\n",
        "            epochs=1,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        total_policy_loss += history.history['policy_output_loss'][0]\n",
        "        total_value_loss += history.history['value_output_loss'][0]\n",
        "        epoch_count += 1\n",
        "\n",
        "    avg_policy_loss = total_policy_loss / epoch_count\n",
        "    avg_value_loss = total_value_loss / epoch_count\n",
        "\n",
        "    print(f\"\\nAverage Policy Output Loss: {avg_policy_loss}\")\n",
        "    print(f\"Average Value Output Loss: {avg_value_loss}\")\n",
        "\n",
        "    return avg_policy_loss, avg_value_loss\n",
        "\n",
        "is_stop = False\n",
        "\n",
        "def train_sigmachess(model, num_iterations=100, num_games_per_iteration=100):\n",
        "    global is_stop\n",
        "\n",
        "    # Değişkenleri başlatın\n",
        "    total_policy_loss = 0\n",
        "    total_value_loss = 0\n",
        "    iteration_count = 0\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n",
        "\n",
        "        replay_buffer = self_play(model, num_games_per_iteration)\n",
        "        policy_loss, value_loss = train_model(model, replay_buffer)\n",
        "\n",
        "        # Kayıpları biriktir\n",
        "        total_policy_loss += policy_loss\n",
        "        total_value_loss += value_loss\n",
        "        iteration_count += 1\n",
        "\n",
        "        # Her 5 iteration'da bir ortalamaları yazdır\n",
        "        if iteration_count % 5 == 0:\n",
        "            avg_policy_loss = total_policy_loss / iteration_count\n",
        "            avg_value_loss = total_value_loss / iteration_count\n",
        "            print(f\"\\n[After {iteration_count} Iterations]\")\n",
        "            print(f\"Average Policy Output Loss: {avg_policy_loss}\")\n",
        "            print(f\"Average Value Output Loss: {avg_value_loss}\")\n",
        "\n",
        "        if is_stop:\n",
        "            break\n",
        "\n",
        "    model.save(\"/content/drive/My Drive/full_model.keras\")\n",
        "\n",
        "\n",
        "def stop():\n",
        "    global is_stop\n",
        "\n",
        "    while True:\n",
        "        inp = input(\"\")\n",
        "        if inp == \"stop\":\n",
        "            is_stop = True\n",
        "            print(\"After the iteration is completed, the training will be stopped and the model will be saved!\")\n",
        "\n",
        "            break\n",
        "\n",
        "t = threading.Thread(target=stop, daemon=True)\n",
        "t.start()\n",
        "\n",
        "model = create_model()\n",
        "train_sigmachess(model, num_iterations=7000, num_games_per_iteration=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDw_Ru4EPXCm"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "def run(self, initial_state, temperature=1.0):\n",
        "    root = Node(initial_state)\n",
        "\n",
        "    # First evaluate and expand root\n",
        "    action_probs, value = self.evaluate(initial_state)\n",
        "    valid_moves = initial_state.get_valid_moves()\n",
        "\n",
        "    # Add Dirichlet noise to root (alpha=0.3 for chess)\n",
        "    noise = np.random.dirichlet([0.3] * len(valid_moves))\n",
        "\n",
        "    # Expand with noisy priors\n",
        "    for idx, action in enumerate(valid_moves):\n",
        "        prob = action_probs[action]\n",
        "        noisy_prob = 0.75 * prob + 0.25 * noise[idx]\n",
        "        next_state = initial_state.clone()\n",
        "        next_state.get_next_state(action)\n",
        "        root.children[action] = Node(next_state, parent=root, prior_prob=noisy_prob)\n",
        "\n",
        "    for _ in range(self.simulations):\n",
        "        node = root\n",
        "\n",
        "        # Selection with additional safety checks\n",
        "        while node.is_expanded and not node.state.is_terminal():\n",
        "            action, child_node = node.select(self.c_puct)\n",
        "\n",
        "            # Safety check to prevent NoneType errors\n",
        "            if child_node is None:\n",
        "                break\n",
        "\n",
        "            node = child_node\n",
        "\n",
        "        # Expansion and Evaluation\n",
        "        if not node.state.is_terminal():\n",
        "            # Ensure node has valid children before expansion\n",
        "            if not node.is_expanded and len(node.children) == 0:\n",
        "                action_probs, value = self.evaluate(node.state)\n",
        "                valid_moves = node.state.get_valid_moves()\n",
        "                node.expand(action_probs)\n",
        "\n",
        "            if node.is_expanded and len(node.children) > 0:\n",
        "                # Randomly select an unexpanded child if possible\n",
        "                unexpanded_children = [child for child in node.children.values() if not child.is_expanded]\n",
        "                if unexpanded_children:\n",
        "                    node = np.random.choice(unexpanded_children)\n",
        "\n",
        "                action_probs, value = self.evaluate(node.state)\n",
        "                valid_moves = node.state.get_valid_moves()\n",
        "                node.expand(action_probs)\n",
        "            else:\n",
        "                # Fallback: re-evaluate the current node\n",
        "                value = node.state.get_winner()\n",
        "                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n",
        "        else:\n",
        "            value = node.state.get_winner()\n",
        "            value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n",
        "\n",
        "        # Backup\n",
        "        node.backup(value)\n",
        "\n",
        "    return self.get_action_probs(root, temperature)\n",
        "'''\n",
        "#run revize edilmiş"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za1kYZLOlMC0"
      },
      "outputs": [],
      "source": [
        "#!!!   üstteki hata önleme etkili değilse yapmamız gereken Eğer mevcut düğüm üzerinde genişletme veya seçim yapılması mümkün değilse, başka bir alt düğüme geçiş yapılır. Bu, unexpanded_children listesinden rastgele bir çocuk seçilerek yapılır.\n",
        "'''\n",
        "for _ in range(self.simulations):\n",
        "    node = root\n",
        "\n",
        "    # Seçim aşamasında NoneType kontrolü\n",
        "    while node.is_expanded and not node.state.is_terminal():\n",
        "        action, child_node = node.select(self.c_puct)\n",
        "\n",
        "        # Eğer child_node None ise, bu düğüm üzerinde işlem yapılmaz, döngü devam eder\n",
        "        if child_node is None:\n",
        "            break\n",
        "\n",
        "        node = child_node\n",
        "\n",
        "    # Genişletme ve Değerlendirme\n",
        "    if not node.state.is_terminal():\n",
        "        # Düğümün çocukları yoksa ve genişletilemiyorsa\n",
        "        if not node.is_expanded and len(node.children) == 0:\n",
        "            action_probs, value = self.evaluate(node.state)\n",
        "            valid_moves = node.state.get_valid_moves()\n",
        "\n",
        "            # Eğer geçerli hareketler varsa, genişletme işlemi yapılır\n",
        "            if valid_moves:\n",
        "                node.expand(action_probs)\n",
        "            else:\n",
        "                # Geçerli hareket yoksa, yedekleme yap\n",
        "                value = node.state.get_winner()\n",
        "                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n",
        "                node.backup(value)\n",
        "                continue  # Diğer bir düğüme geçmek için döngüye devam et\n",
        "\n",
        "        # Çocuk düğümlerinin varlığı kontrol ediliyor\n",
        "        if node.is_expanded and len(node.children) > 0:\n",
        "            unexpanded_children = [child for child in node.children.values() if not child.is_expanded]\n",
        "            if unexpanded_children:\n",
        "                # Rastgele bir genişletilmemiş çocuk seç\n",
        "                node = np.random.choice(unexpanded_children)\n",
        "\n",
        "            action_probs, value = self.evaluate(node.state)\n",
        "            valid_moves = node.state.get_valid_moves()\n",
        "            node.expand(action_probs)\n",
        "        else:\n",
        "            # Eğer hiçbir şey yapılamazsa, fallback değeriyle yedekleme yap\n",
        "            value = node.state.get_winner()\n",
        "            value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n",
        "            node.backup(value)\n",
        "            continue  # Geçerli düğüm üzerinde işlem yapamıyorsak, başka bir düğüme geç\n",
        "    else:\n",
        "        # Eğer düğüm terminalse, kazananı belirle\n",
        "        value = node.state.get_winner()\n",
        "        value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n",
        "\n",
        "    # Değeri yedekle\n",
        "    node.backup(value)\n",
        "'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 6219321,
          "sourceId": 10086949,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6227771,
          "sourceId": 10097989,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}