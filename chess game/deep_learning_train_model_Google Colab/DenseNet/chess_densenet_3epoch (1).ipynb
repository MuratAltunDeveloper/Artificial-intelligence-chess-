{"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10086949,"sourceType":"datasetVersion","datasetId":6219321}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["#ULTRA SUPER SIGMA CHESS AI"],"metadata":{"trusted":true,"id":"RUlmIButCyE6"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!pip install chess"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:37:19.019974Z","iopub.execute_input":"2024-12-03T18:37:19.020302Z","iopub.status.idle":"2024-12-03T18:37:30.406565Z","shell.execute_reply.started":"2024-12-03T18:37:19.020275Z","shell.execute_reply":"2024-12-03T18:37:30.405717Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"6j5hDBiZCyE7","outputId":"fe0b592e-5d91-4251-e7ec-886611804676"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: chess in /usr/local/lib/python3.10/dist-packages (1.11.1)\n"]}],"execution_count":null},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9yiWlrkxC03v","colab":{"base_uri":"https://localhost:8080/"},"outputId":"11396612-5c4d-405f-9ff3-5ce640c878bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["\n","\n","!chmod +x \"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\"\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T18:37:30.408309Z","iopub.execute_input":"2024-12-03T18:37:30.408566Z","iopub.status.idle":"2024-12-03T18:37:33.926613Z","shell.execute_reply.started":"2024-12-03T18:37:30.408541Z","shell.execute_reply":"2024-12-03T18:37:33.925393Z"},"id":"DVYV0gxkCyE8"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import math\n","\n","import chess\n","\n","from collections import deque\n","\n","import tensorflow as tf\n","\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import HeNormal\n","\n","import chess.engine\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","\n","import threading\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","\n","class GameState:\n","    row = 8\n","    col = 8\n","    promotion_indexes = {\n","        chess.KNIGHT: 0,\n","        chess.ROOK: 1,\n","        chess.BISHOP: 2\n","    }\n","\n","    def __init__(self) -> None:\n","        self.board = chess.Board()\n","        self.repetition_count = 0\n","        self.player_color: chess.Color = chess.WHITE\n","\n","    def get_initial_state(self):\n","        self.board.reset()\n","\n","        return self.get_current_state()\n","\n","    def get_current_state(self, T=8):\n","        input_tensor = np.zeros((8, 8, 119), dtype=np.uint8)\n","\n","        for t in range(T):\n","            _t = T - t - 1\n","            if len(self.board.move_stack) < _t:\n","                continue\n","\n","            self.create_input(input_tensor, _t)\n","\n","        color = 0 if self.board.turn == chess.WHITE else 1\n","        input_tensor[:, :, 112] = color\n","\n","        input_tensor[:, :, 113] = len(self.board.move_stack) > 0\n","\n","        p1_castling = (1 * self.board.has_kingside_castling_rights(chess.WHITE)) | (2 * self.board.has_queenside_castling_rights(chess.WHITE))\n","        p1_castling_bit = format(p1_castling, \"02b\")\n","        input_tensor[:, :, 114] = int(p1_castling_bit[0])\n","        input_tensor[:, :, 115] = int(p1_castling_bit[1])\n","\n","        p2_castling = (1 * self.board.has_kingside_castling_rights(chess.BLACK)) | (2 * self.board.has_queenside_castling_rights(chess.BLACK))\n","        p2_castling_bit = format(p2_castling, \"02b\")\n","        input_tensor[:, :, 116] = int(p2_castling_bit[0])\n","        input_tensor[:, :, 117] = int(p2_castling_bit[1])\n","\n","        input_tensor[:, :, 118] = int(self.board.is_fifty_moves())\n","\n","        return np.expand_dims(input_tensor, axis=0)\n","\n","    def get_next_state(self, action: int):\n","        source_index = action // 73\n","        destination_index = 0\n","        move_type = action % 73\n","\n","        promotion = None\n","\n","        if move_type < 56:\n","            direction = move_type // 7\n","            movement = (move_type % 7) + 1\n","\n","            destination_index = source_index + (movement * 8) if direction == 0 else destination_index\n","            destination_index = source_index + (movement * 9) if direction == 1 else destination_index\n","            destination_index = source_index + movement if direction == 2 else destination_index\n","            destination_index = source_index + (movement * -7) if direction == 3 else destination_index\n","            destination_index = source_index + (movement * -8) if direction == 4 else destination_index\n","            destination_index = source_index + (movement * -9) if direction == 5 else destination_index\n","            destination_index = source_index + (-movement) if direction == 6 else destination_index\n","            destination_index = source_index + (movement * 7) if direction == 7 else destination_index\n","        elif move_type >= 56 and move_type < 64:\n","            direction = move_type - 56\n","\n","            destination_index = source_index + 17 if direction == 0 else destination_index\n","            destination_index = source_index + 10 if direction == 1 else destination_index\n","            destination_index = source_index - 6 if direction == 2 else destination_index\n","            destination_index = source_index - 15 if direction == 3 else destination_index\n","            destination_index = source_index - 17 if direction == 4 else destination_index\n","            destination_index = source_index - 10 if direction == 5 else destination_index\n","            destination_index = source_index + 6 if direction == 6 else destination_index\n","            destination_index = source_index + 15 if direction == 7 else destination_index\n","        else:\n","            direction = (move_type - 64) // 3\n","            promotion_index = (move_type - 64) % 3\n","\n","            promotion = chess.KNIGHT if promotion_index == 0 else (chess.ROOK if promotion_index == 1 else chess.BISHOP)\n","\n","            color_value = 1 if self.board.turn == chess.WHITE else -1\n","\n","            if direction == 0:\n","                destination_index = source_index + (8 * color_value)\n","            elif direction == 1:\n","                destination_index = source_index + (9 * color_value)\n","            else:\n","                destination_index = source_index + (7 * color_value)\n","\n","        from_square = chess.Square(source_index)\n","        to_square = chess.Square(destination_index)\n","\n","        promotion_rank = 7 if self.board.turn == chess.WHITE else 0\n","\n","        if promotion is None:\n","            if self.board.piece_type_at(from_square) == chess.PAWN and chess.square_rank(to_square) == promotion_rank:\n","                promotion = chess.QUEEN\n","\n","        move = chess.Move(from_square, to_square, promotion)\n","\n","        self.apply_action(move)\n","\n","        return move, self.get_current_state()\n","\n","    def apply_action(self, move: chess.Move):\n","        try:\n","            self.board.push(move)\n","        except Exception as e:\n","            print(list(self.board.legal_moves))\n","            print(self.get_valid_moves())\n","\n","            print(e)\n","\n","            raise Exception(\"Error\")\n","\n","    def create_input(self, input_tensor: np.ndarray, t: int):\n","        piece_types = {\n","            chess.PAWN: 0,\n","            chess.KNIGHT: 1,\n","            chess.BISHOP: 2,\n","            chess.ROOK: 3,\n","            chess.QUEEN: 4,\n","            chess.KING: 5\n","        }\n","\n","        board = self.board.copy()\n","        for _ in range(t):\n","            board.pop()\n","\n","        transposition_key = board._transposition_key()\n","\n","        for square in chess.SQUARES:\n","            piece = board.piece_at(square)\n","\n","            if piece is None:\n","                continue\n","\n","            piece_index = piece_types[piece.piece_type]\n","            piece_color = 0 if piece.color == chess.WHITE else 1\n","\n","            index = (t * 14) + (piece_color * 6) + piece_index\n","            input_tensor[square // 8][square % 8][index] = 1\n","\n","        repetition_count = 0\n","        index = (t * 14) + 12\n","\n","        try:\n","            while board.move_stack:\n","                move = board.pop()\n","                if board.is_irreversible(move):\n","                    break\n","\n","                if board._transposition_key() == transposition_key:\n","                    repetition_count += 1\n","\n","                if repetition_count == 3:\n","                    break\n","        finally:\n","            repetition_count = 3 if repetition_count > 3 else repetition_count\n","\n","            repetition_count_bits = [int(x) for x in format(repetition_count, \"02b\")]\n","            input_tensor[:, :, index] = repetition_count_bits[0]\n","            input_tensor[:, :, index + 1] = repetition_count_bits[1]\n","\n","    def get_valid_moves(self):\n","        legal_moves = []\n","\n","        for valid_move in self.board.legal_moves:\n","            s_row, s_col, from_square_index = self.index_of_square(valid_move.from_square)\n","            d_row, d_col, to_square_index = self.index_of_square(valid_move.to_square)\n","\n","            if valid_move.promotion:\n","                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n","\n","                if valid_move.promotion == chess.QUEEN:\n","                    index = (from_square_index * 73) + (direction * 7)\n","                    legal_moves.append(index)\n","                else:\n","                    promotion_index = self.promotion_indexes[valid_move.promotion]\n","\n","                    if direction > 2 and direction < 6:\n","                        direction = 0 if direction == 4 else (1 if direction == 5 else 2)\n","                    elif direction == 7:\n","                        direction = 2\n","\n","                    index = (from_square_index * 73) + ((direction * 3) + promotion_index + 64)\n","                    legal_moves.append(index)\n","            elif self.board.piece_type_at(valid_move.from_square) == chess.KNIGHT:\n","                direction = self.direction_of_move_for_knights(s_row, s_col, d_row, d_col)\n","\n","                index = (from_square_index * 73) + direction + 56\n","                legal_moves.append(index)\n","\n","            else:\n","                direction = self.direction_of_move_for_ray_directions(s_row, s_col, d_row, d_col)\n","                count_of_square = self.count_of_square_for_movement(s_row, s_col, d_row, d_col) - 1\n","\n","                index = (from_square_index * 73) + ((direction * 7) + count_of_square)\n","                legal_moves.append(index)\n","\n","        return legal_moves\n","\n","    def index_of_square(self, square: chess.Square):\n","        row = chess.square_rank(square)\n","        col = chess.square_file(square)\n","        index = (row * 8) + col\n","\n","        return row, col, index\n","\n","    def direction_of_move_for_ray_directions(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        if delta_x == 0:\n","            return 0 if delta_y > 0 else 4\n","\n","        if delta_y == 0:\n","            return 2 if delta_x > 0 else 6\n","\n","        if delta_x < 0:\n","            return 7 if delta_y > 0 else 5\n","\n","        return 1 if delta_y > 0 else 3\n","\n","    def direction_of_move_for_knights(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        if delta_x == 1:\n","            return 0 if delta_y > 0 else 3\n","\n","        if delta_x == 2:\n","            return 1 if delta_y > 0 else 2\n","\n","        if delta_x == -1:\n","            return 7 if delta_y > 0 else 4\n","\n","        return 6 if delta_y > 0 else 5\n","\n","    def count_of_square_for_movement(self, s_row: int, s_col: int, d_row: int, d_col: int):\n","        delta_x = d_col - s_col\n","        delta_y = d_row - s_row\n","\n","        return max(abs(delta_x), abs(delta_y))\n","\n","    def get_winner(self):\n","        result = self.board.result()\n","\n","        if result == \"1-0\":\n","            return chess.WHITE\n","\n","        if result == \"0-1\":\n","            return chess.BLACK\n","\n","        return 2\n","\n","    def is_terminal(self):\n","        return self.board.is_game_over()\n","\n","    def clone(self):\n","        cloned_state = GameState()\n","        cloned_state.board = self.board.copy()\n","\n","        return cloned_state\n","\n","class Node:\n","    def __init__(self, state, parent=None, prior_prob=1.0):\n","        self.state = state\n","        self.parent = parent\n","        self.children = {}\n","        self.visits = 0\n","        self.value_sum = 0\n","        self.prior_prob = prior_prob\n","        self.is_expanded = False\n","\n","    @property\n","    def value(self):\n","        return self.value_sum / (self.visits + 1e-5)\n","\n","    def expand(self, action_probs):\n","        for action, prob in enumerate(action_probs):\n","            if prob > 0:\n","                next_state = self.state.clone()\n","                next_state.get_next_state(action)\n","                self.children[action] = Node(next_state, parent=self, prior_prob=prob)\n","\n","        if len(self.children) > 0:\n","            self.is_expanded = True\n","\n","    def select(self, c_puct=1.0):\n","        max_ucb = -float('inf')\n","        best_action = None\n","        best_child = None\n","\n","        for action, child in self.children.items():\n","            ucb = child.value + c_puct * child.prior_prob * (math.sqrt(self.visits) / (1 + child.visits))\n","            if ucb > max_ucb:\n","                max_ucb = ucb\n","                best_action = action\n","                best_child = child\n","        return best_action, best_child\n","\n","    def backup(self, value):\n","        self.visits += 1\n","        self.value_sum += value\n","        if self.parent:\n","            self.parent.backup(-value)\n","\n","class MCTS:\n","    def __init__(self, model, c_puct=1.0, simulations=50):\n","        self.model = model\n","        self.c_puct = c_puct\n","        self.simulations = simulations\n","\n","    def add_dirichlet_noise(self, node, valid_moves):\n","        noise = np.random.dirichlet([0.3] * len(valid_moves))\n","        for idx, action in enumerate(valid_moves):\n","            if action in node.children:\n","                node.children[action].prior_prob = \\\n","                    0.75 * node.children[action].prior_prob + 0.25 * noise[idx]\n","\n","    def run(self, initial_state, temperature=1.0):\n","        root = Node(initial_state)\n","\n","        # First evaluate and expand root\n","        action_probs, value = self.evaluate(initial_state)\n","        valid_moves = initial_state.get_valid_moves()\n","\n","        # Add Dirichlet noise to root (alpha=0.3 for chess)\n","        noise = np.random.dirichlet([0.3] * len(valid_moves))\n","\n","        # Expand with noisy priors\n","        for idx, action in enumerate(valid_moves):\n","            prob = action_probs[action]\n","            noisy_prob = 0.75 * prob + 0.25 * noise[idx]\n","            next_state = initial_state.clone()\n","            next_state.get_next_state(action)\n","            root.children[action] = Node(next_state, parent=root, prior_prob=noisy_prob)\n","\n","        for _ in range(self.simulations):\n","            node = root\n","\n","            # Selection\n","            while node.is_expanded and not node.state.is_terminal():\n","                action, node = node.select(self.c_puct)\n","\n","            # Expansion and Evaluation\n","            if not node.state.is_terminal():\n","                action_probs, value = self.evaluate(node.state)\n","                valid_moves = node.state.get_valid_moves()\n","                node.expand(action_probs)\n","            else:\n","                value = node.state.get_winner()\n","                value = 1 if value == node.state.player_color else (0 if value == 2 else -1)\n","\n","            # Backup\n","            node.backup(value)\n","\n","        return self.get_action_probs(root, temperature)\n","\n","    def evaluate(self, state):\n","        state_tensor = state.get_current_state()\n","        # state_tensor = np.expand_dims(state_tensor, axis=0)\n","\n","        policy, value = self.model.predict(state_tensor, verbose=0)\n","\n","        # Mask invalid moves\n","        valid_moves = state.get_valid_moves()\n","        mask = np.zeros(policy.shape[1])\n","        mask[valid_moves] = 1\n","\n","        policy = policy[0] * mask\n","\n","        # Normalize\n","        sum_policy = np.sum(policy)\n","        if sum_policy > 0:\n","            policy /= sum_policy\n","        else:\n","            # If all moves were masked, use uniform distribution over valid moves\n","            policy = mask / np.sum(mask)\n","\n","        return policy, value[0][0]\n","\n","    def get_action_probs(self, root, temperature=1.0):\n","        visits = np.array([child.visits for action, child in root.children.items()])\n","        actions = list(root.children.keys())\n","\n","        if temperature == 0:  # Pure exploitation\n","            action_idx = np.argmax(visits)\n","            probs = np.zeros_like(visits)\n","            probs[action_idx] = 1\n","        else:\n","            # Apply temperature\n","            visits = visits ** (1 / temperature)\n","            probs = visits / np.sum(visits)\n","\n","        # Convert to full move probability vector\n","        full_probs = np.zeros(4672)  # Adjust size based on your action space\n","        for action, prob in zip(actions, probs):\n","            full_probs[action] = prob\n","\n","        return full_probs\n","\n","class ReplayBuffer:\n","    def __init__(self, maxlen=500000):\n","        self.buffer = deque(maxlen=maxlen)\n","        self.current_size = 0\n","        self.lock = threading.Lock()\n","\n","    def store(self, state, policy, value):\n","        \"\"\"Store a single game state transition\"\"\"\n","        self.buffer.append({\n","            'state': state,\n","            'policy': policy,\n","            'value': value\n","        })\n","        self.current_size = len(self.buffer)\n","\n","    def store_multiple_data(self, states, policies, value):\n","        with self.lock:\n","            for s, p, v in zip(states, policies, [value]):\n","                self.store(s, p, v)\n","\n","    def sample(self, batch_size):\n","        \"\"\"Sample a batch with augmentations\"\"\"\n","        if self.current_size < batch_size:\n","            batch_size = self.current_size\n","\n","        indices = np.random.choice(self.current_size, batch_size)\n","        states, policies, values = [], [], []\n","\n","        for idx in indices:\n","            sample = self.buffer[idx]\n","            # Get augmented samples\n","            aug_states, aug_policies = self._augment_sample(\n","                sample['state'],\n","                sample['policy']\n","            )\n","\n","            # Add all augmentations\n","            states.extend(aug_states)\n","            policies.extend(aug_policies)\n","            values.extend([sample['value']] * len(aug_states))\n","\n","        return np.array(states), np.array(policies), np.array(values)\n","\n","    def _augment_sample(self, state, policy):\n","        \"\"\"Generate valid augmentations for a single sample\"\"\"\n","        # Remove batch dimension if present\n","        if len(state.shape) == 4:\n","            state = np.squeeze(state, axis=0)\n","\n","        augmented_states = [state]\n","        augmented_policies = [policy]\n","\n","        # Horizontal flip\n","        flip_h = np.flip(state, axis=1)\n","        augmented_states.append(flip_h)\n","        augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        # Vertical flip\n","        flip_v = np.flip(state, axis=0)\n","        augmented_states.append(flip_v)\n","        augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        # Diagonal flip (only if shape allows)\n","        if state.shape[0] == state.shape[1]:\n","            diag = np.transpose(state, (1, 0, 2))\n","            augmented_states.append(diag)\n","            augmented_policies.append(policy)  # Policy needs game-specific mapping\n","\n","        return augmented_states, augmented_policies\n","\n","    def __len__(self):\n","        return self.current_size\n","\n","# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n","\n","# tf.tpu.experimental.initialize_tpu_system(tpu)\n","# tpu_strategy = tf.distribute.TPUStrategy(tpu)\n","\n","print(\"All devices: \", tf.config.list_logical_devices('GPU'))\n","\n","policy = tf.keras.mixed_precision.Policy('mixed_float16')\n","tf.keras.mixed_precision.set_global_policy(policy)"],"metadata":{"_uuid":"d7c89aa5-8449-4c9b-b220-63cf5c8ab98c","_cell_guid":"e45be06e-fe38-4b84-b26e-6ead59ee098e","trusted":true,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-03T18:40:52.940405Z","iopub.execute_input":"2024-12-03T18:40:52.940740Z"},"id":"py_fPedFCyE8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7599a16a-0ab9-4f34-9621-3d2536c55488"},"outputs":[{"output_type":"stream","name":"stdout","text":["All devices:  [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"]}],"execution_count":null},{"cell_type":"code","source":["import numpy as np\n","import threading\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","def dense_block(inputs, filters=256, growth_rate=32, bottleneck=True, stride=1):\n","    if bottleneck:\n","        x = layers.Conv2D(4 * growth_rate, (1, 1), strides=stride, padding=\"same\",\n","                          kernel_initializer=HeNormal(), use_bias=False)(inputs)\n","        x = layers.BatchNormalization()(x)\n","        x = layers.ReLU()(x)\n","    else:\n","        x = inputs\n","\n","    x = layers.Conv2D(growth_rate, (3, 3), strides=stride, padding=\"same\",\n","                      kernel_initializer=HeNormal(), use_bias=False)(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.ReLU()(x)\n","\n","    x = layers.Concatenate()([inputs, x])\n","    return x\n","\n","def sigmachess_network(input_shape=(8, 8, 119)):\n","    inputs = layers.Input(shape=input_shape)\n","\n","    x = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(inputs)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.ReLU()(x)\n","\n","    for _ in range(19):\n","        x = dense_block(x)\n","\n","    policy = layers.Conv2D(256, (3, 3), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    policy = layers.BatchNormalization()(policy)\n","    policy = layers.ReLU()(policy)\n","    policy = layers.Conv2D(73, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\")(policy)\n","    policy = layers.Flatten()(policy)\n","    policy = layers.Softmax(name=\"policy_output\")(policy)\n","\n","    value = layers.Conv2D(1, (1, 1), strides=1, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n","    value = layers.BatchNormalization()(value)\n","    value = layers.ReLU()(value)\n","    value = layers.Flatten()(value)\n","    value = layers.Dense(256, activation=\"relu\", kernel_initializer=\"he_normal\")(value)\n","    value = layers.Dense(1, activation=\"tanh\", name=\"value_output\", kernel_initializer=\"he_normal\")(value)\n","\n","    model = models.Model(inputs=inputs, outputs=[policy, value])\n","\n","    return model\n","\n","def create_model():\n","    model = sigmachess_network()\n","\n","    model.compile(\n","        optimizer=Adam(learning_rate=0.02),\n","        loss={\n","            \"policy_output\": \"categorical_crossentropy\",\n","            \"value_output\": \"mean_squared_error\"\n","        },\n","        metrics={\n","            \"policy_output\": \"accuracy\",\n","            \"value_output\": \"mse\"\n","        }\n","    )\n","\n","    return model\n","\n","def play_vs_stockfish(model, game, replay_buffer):\n","    state = GameState()\n","    temperature = 1.0 if game < 5 else 0.1\n","\n","    w_states, w_policies, w_rewards = [], [], []\n","    player = np.random.choice([chess.WHITE, chess.BLACK])\n","\n","    engine = chess.engine.SimpleEngine.popen_uci(r\"/content/drive/My Drive/stockfish-ubuntu-x86-64-avx2\")\n","\n","    while not state.is_terminal():\n","        if state.board.turn == player:\n","            mcts = MCTS(model, 1.0, 10)\n","            action_probs = mcts.run(state, temperature)\n","\n","            w_states.append(state.get_current_state())\n","            w_policies.append(action_probs)\n","\n","            action = np.random.choice(len(action_probs), p=action_probs)\n","            state.get_next_state(action)\n","        else:\n","            result = engine.play(state.board, chess.engine.Limit(0.04))\n","            state.apply_action(result.move)\n","\n","    engine.close()\n","\n","    winner = state.get_winner()\n","    w_value = 1 if winner == player else (0 if winner == 2 else -1)\n","\n","    print(player, state.board.board_fen())\n","\n","    replay_buffer.store_multiple_data(w_states, w_policies, w_value)\n","\n","def self_play(model, num_games=100, max_workers=5):\n","    replay_buffer = ReplayBuffer(maxlen=500000)\n","\n","    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n","        futures = [\n","            executor.submit(play_vs_stockfish, model, i, replay_buffer)\n","            for i in range(num_games)\n","        ]\n","\n","        for future in as_completed(futures):\n","            future.result()\n","\n","    print(\"Self-play completed\")\n","\n","    return replay_buffer\n","\n","def prepare_validation_data(replay_buffer=None, filename=\"validation_data.npy\"):\n","    try:\n","        # First, try to load existing validation data\n","        data = np.load(filename, allow_pickle=True).item()\n","        print(f\"Loaded existing validation data from {filename}\")\n","        return data['x_val'], {\n","            \"policy_output\": data['y_val_policy'],\n","            \"value_output\": data['y_val_value']\n","        }\n","    except (FileNotFoundError, IOError):\n","        # If no existing data, create new validation dataset\n","        if replay_buffer is None:\n","            # If no replay buffer provided, generate synthetic data\n","            x_val = np.random.random((100, 8, 8, 119))\n","            y_val_policy = np.random.randint(0, 73, size=(100, 1))\n","            y_val_value = np.random.random((100, 1)) * 2 - 1  # Values between -1 and 1\n","        else:\n","            # Use replay buffer to generate validation data\n","            states, policies, rewards = replay_buffer.sample(100)\n","            x_val = np.squeeze(states)\n","            y_val_policy = policies\n","            y_val_value = np.array(rewards).reshape(-1, 1)\n","\n","        # One-hot encode policy output\n","        y_val_policy_onehot = np.eye(73)[y_val_policy.flatten()]\n","\n","        # Prepare data dictionary\n","        validation_data = {\n","            \"x_val\": x_val,\n","            \"y_val_policy\": y_val_policy_onehot,\n","            \"y_val_value\": y_val_value\n","        }\n","\n","        # Save the validation data\n","        np.save(filename, validation_data)\n","        print(f\"Created and saved new validation data to {filename}\")\n","\n","        return x_val, {\n","            \"policy_output\": y_val_policy_onehot,\n","            \"value_output\": y_val_value\n","        }\n","\n","def create_callbacks(checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n","    checkpoint = ModelCheckpoint(\n","        filepath=checkpoint_path,\n","        save_weights_only=True,\n","        monitor=\"loss\",\n","        mode=\"min\",\n","        save_best_only=True,\n","        save_freq=\"epoch\",\n","        verbose=1\n","    )\n","    return [checkpoint]\n","\n","def train_model(model, replay_buffer: ReplayBuffer, batch_size=256, epochs=3, checkpoint_path=\"/content/drive/My Drive/sigma_checkpoint.weights.h5\"):\n","    x_val, y_val = prepare_validation_data()\n","\n","    callbacks = create_callbacks(checkpoint_path)\n","\n","    total_policy_loss = 0\n","    total_value_loss = 0\n","    epoch_count = 0\n","\n","    for epoch in range(epochs):\n","        states, policies, values = replay_buffer.sample(batch_size)\n","\n","        states = np.squeeze(states)\n","        if len(states.shape) == 3:\n","            states = np.expand_dims(states, -1)\n","\n","        values = np.array(values).reshape(-1, 1)\n","\n","        history = model.fit(\n","            states,\n","            { \"policy_output\": policies, \"value_output\": values },\n","            batch_size=batch_size,\n","            epochs=1,\n","            callbacks=callbacks,\n","            verbose=1\n","        )\n","\n","        total_policy_loss += history.history['policy_output_loss'][0]\n","        total_value_loss += history.history['value_output_loss'][0]\n","        epoch_count += 1\n","\n","    avg_policy_loss = total_policy_loss / epoch_count\n","    avg_value_loss = total_value_loss / epoch_count\n","\n","    print(f\"\\nAverage Policy Output Loss: {avg_policy_loss}\")\n","    print(f\"Average Value Output Loss: {avg_value_loss}\")\n","\n","    return avg_policy_loss, avg_value_loss\n","\n","is_stop = False\n","\n","def train_sigmachess(model, num_iterations=100, num_games_per_iteration=100):\n","    global is_stop\n","\n","    # Değişkenleri başlatın\n","    total_policy_loss = 0\n","    total_value_loss = 0\n","    iteration_count = 0\n","\n","    for iteration in range(num_iterations):\n","        print(f\"Iteration {iteration + 1}/{num_iterations}\")\n","\n","        replay_buffer = self_play(model, num_games_per_iteration)\n","        policy_loss, value_loss = train_model(model, replay_buffer)\n","\n","        # Kayıpları biriktir\n","        total_policy_loss += policy_loss\n","        total_value_loss += value_loss\n","        iteration_count += 1\n","\n","        # Her 5 iteration'da bir ortalamaları yazdır\n","        if iteration_count % 5 == 0:\n","            avg_policy_loss = total_policy_loss / iteration_count\n","            avg_value_loss = total_value_loss / iteration_count\n","            print(f\"\\n[After {iteration_count} Iterations]\")\n","            print(f\"Average Policy Output Loss: {avg_policy_loss}\")\n","            print(f\"Average Value Output Loss: {avg_value_loss}\")\n","\n","        if is_stop:\n","            break\n","\n","    model.save(\"/content/drive/My Drive/full_model.keras\")\n","\n","\n","def stop():\n","    global is_stop\n","\n","    while True:\n","        inp = input(\"\")\n","        if inp == \"stop\":\n","            is_stop = True\n","            print(\"After the iteration is completed, the training will be stopped and the model will be saved!\")\n","\n","            break\n","\n","t = threading.Thread(target=stop, daemon=True)\n","t.start()\n","\n","model = create_model()\n","train_sigmachess(model, num_iterations=7000, num_games_per_iteration=15)\n"],"metadata":{"id":"zmJtM_9MHZcr","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"caf2f9c0-8099-404c-a823-e7ba7013f980"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Iteration 1/7000\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x780bb04aff40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["False rnb2Bnr/1pN2pp1/2kQ4/4P3/p3P2p/8/PPP2PPP/R3KBNR\n","True r1b1kb1r/pp3pp1/2n2n2/3pp2p/Pp4Pq/2P2P2/1P1PP3/1RBQKB2\n","False rnbN1b2/1pNpn1p1/8/p6B/k1Q1PB1p/8/PPP2PPP/R3K2R\n","False k2R1bnr/4p1pp/2P5/1N3B2/p7/4B2N/PPP2PPP/4K2R\n","True rnb1k1nr/ppp2ppp/8/3p4/3P4/b3p3/PPPQPqPP/R4BKR\n","True r3r1k1/ppp2pp1/b1n2n2/P2p4/5pp1/2P5/1P2qPBP/2B1K1R1\n","False rnbk4/p1pp4/5B2/1B3p2/3p3p/2N2N1P/PPP1QPP1/R3K2R\n","False rn1q3r/1bpp1p1p/3k1p2/pB6/P2P1BpP/P1N5/2P1QPP1/R4KNR\n","True r1b1kb1r/pp1ppppp/8/7B/1p6/8/PPnP1nPP/RNB1qKNR\n","True 2b1k1nr/1ppp1ppp/8/4p3/1n6/b3PP2/Pqr3PP/1K3BNR\n","True r3k2r/ppp2ppp/2n1bn2/2b1p3/3q4/P1K2P2/1PPPP1PP/R1BQ1BR1\n","True r1b1r1k1/1pp2pp1/p1n4p/8/PP3pnb/8/2PPP2P/RNQKqBR1\n","False 3n4/8/3Q4/2p3pp/p2k4/2N5/PPPP1PPP/R1B1KB1R\n","False 2r4r/1Qpqp2p/3p3N/k7/p2P4/6N1/PPPB1PPP/R3R1K1\n","True 3r1rk1/ppp2pp1/2n1bn2/P1b1q2p/7P/2P2P2/1P3KP1/RN2BB1R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54s/step - loss: 9.4690 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2847 - value_output_loss: 0.1842 - value_output_mse: 0.1842\n","Epoch 1: loss improved from inf to 9.46897, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 57s/step - loss: 9.4690 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2847 - value_output_loss: 0.1842 - value_output_mse: 0.1842\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 9.5367 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2049 - value_output_loss: 0.3319 - value_output_mse: 0.3319\n","Epoch 1: loss did not improve from 9.46897\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 9.5367 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.2049 - value_output_loss: 0.3319 - value_output_mse: 0.3319\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 9.4662 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3210 - value_output_loss: 0.1452 - value_output_mse: 0.1452\n","Epoch 1: loss improved from 9.46897 to 9.46622, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 9.4662 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3210 - value_output_loss: 0.1452 - value_output_mse: 0.1452  \n","\n","Average Policy Output Loss: 9.270186424255371\n","Average Value Output Loss: 0.2204501579205195\n","Iteration 2/7000\n","True r1bqk2r/ppp2ppp/B1n2n2/4p3/3b4/5P2/P1P2qPP/1R1Q1KNR\n","False rr5k/1pp4p/p1n2Q2/4pN2/2B5/8/PPP2PPP/RNB1K2R\n","False r2k1Rn1/2p5/2B1P1N1/p2p3p/3P2p1/2N5/PPP3PP/R2Q2K1\n","True r3kb1r/pp2p1p1/n4p2/1p1p3p/3Pb3/PPP5/2q4P/R1K5\n","True 1r2k2r/p1p1nppp/p2b4/8/P1p1b3/2Pn2PP/1P1P1q2/2QR3K\n","True 3qk2r/1pp2ppp/1p5n/n1bp4/b3p2P/2PK4/1P1PPPP1/rNB2B1R\n","False rnbq1bnr/p1p1pNpp/8/1p1p1P1Q/k2P4/1P6/P1PB1PPP/RN2KB1R\n","False 1n1N1bn1/2p5/3p1k1r/p2PpPQp/p1P5/8/PP3PPP/RNB1K2R\n","False 4Q2r/r2pR1kp/n4pp1/pBpP4/P6N/8/1PPB1PPP/R5K1\n","True rnb1k1nr/pppp1ppp/8/4p2P/8/5Pq1/PPPPP3/RNBQKBN1\n","False 1Qbk1b2/1pN1ppp1/8/p2Q1P1r/7p/5N2/PPP1BPPP/R1B1K2R\n","True 2b1kbnr/1ppp1ppp/8/8/1pP5/1n1P1N1N/1qK1PPPP/1r3B1R\n","True rn2k3/ppp2pp1/4b3/3pprq1/Pb4P1/3PK3/1PP1P1pP/RNQ2B1R\n","False 1n2k2r/1p2Qp1p/1N2bP2/p1p1P1N1/8/8/PPP1BPPP/R3K2R\n","True 1r1qr1k1/2p2ppn/2n2b1p/1p5P/q3p3/1KPP2P1/1P2PR2/2B2B2\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 9.4785 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3333 - value_output_loss: 0.1452 - value_output_mse: 0.1452\n","Epoch 1: loss improved from inf to 9.47850, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 9.4785 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3333 - value_output_loss: 0.1452 - value_output_mse: 0.1452  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 9.4882 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3040 - value_output_loss: 0.1842 - value_output_mse: 0.1842\n","Epoch 1: loss did not improve from 9.47850\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 9.4882 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3040 - value_output_loss: 0.1842 - value_output_mse: 0.1842\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 9.4881 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3042 - value_output_loss: 0.1839 - value_output_mse: 0.1839\n","Epoch 1: loss did not improve from 9.47850\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 9.4881 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.3042 - value_output_loss: 0.1839 - value_output_mse: 0.1839\n","\n","Average Policy Output Loss: 9.3138427734375\n","Average Value Output Loss: 0.17107833425203958\n","Iteration 3/7000\n","True rnb1kbnr/ppp2ppp/8/3pp3/8/P4PqP/1PPPP3/RNBQKBNR\n","True rn2kbnr/ppp2ppp/4b3/3pp3/6Pq/P4P2/RPPPP2P/1NBQKBNR\n","False rnbqkbnr/ppppp2p/8/5ppQ/4P3/2N5/PPPP1PPP/R1B1KBNR\n","False rnbqkbnr/2ppp2P/8/pp5Q/8/8/PPPP1PPP/RNB1KBNR\n","True rnb1k1nr/ppp2ppp/8/8/7P/2b5/2P1PPP1/q1K2B1R\n","False N1b2bnr/pp4pp/n3k3/5pB1/2B5/5N2/PPP2PPP/R2QK2R\n","True r2qk2r/ppp2ppp/2n5/7P/6b1/P1bP4/2P1P1P1/3KqB2\n","False r1b1kQ2/pp1p1p2/n6B/q1pP4/4P3/2N5/PPP2PPP/R3KBNR\n","True r3r3/1ppk1pp1/4b3/1Pb4p/q7/P1P3P1/2nPPn1P/2BK1B1R\n","True r3k1r1/ppp1qp1p/2n5/6p1/6b1/2bPP1P1/n4P1P/4KB1R\n","False r1bqkQ2/p1pp4/7B/nB2P2p/8/5N2/PPP2PPP/RN1QK2R\n","True rnbqk2r/ppp2ppp/4pn2/5P2/8/P2Pq1BN/3b1KPP/5B1R\n","True r3kb2/pp3pp1/5n2/3qp2r/P2p2pP/K3P3/1PnP1P2/R1B3R1\n","True rn2k2r/pp3ppp/8/2b1pb2/P3q3/2P5/1P1P1nPP/RNBK1B2\n","False 6n1/3N2p1/p7/2R3B1/k1RpP2p/7P/PP1N1PP1/6K1\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 9.2021 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1244 - value_output_loss: 0.0776 - value_output_mse: 0.0777\n","Epoch 1: loss improved from inf to 9.20209, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 9.2021 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1244 - value_output_loss: 0.0776 - value_output_mse: 0.0777  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 9.2048 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1077 - value_output_loss: 0.0971 - value_output_mse: 0.0971\n","Epoch 1: loss did not improve from 9.20209\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 9.2048 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.1077 - value_output_loss: 0.0971 - value_output_mse: 0.0971\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 9.2612 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0770 - value_output_loss: 0.1842 - value_output_mse: 0.1842\n","Epoch 1: loss did not improve from 9.20209\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 9.2612 - policy_output_accuracy: 0.0000e+00 - policy_output_loss: 9.0770 - value_output_loss: 0.1842 - value_output_mse: 0.1842\n","\n","Average Policy Output Loss: 9.103047053019205\n","Average Value Output Loss: 0.11965920776128769\n","Iteration 4/7000\n","False rnbq1br1/pppp1Qpp/3k4/1N2N3/2BPP3/8/PPP2PPP/R1B1K2R\n","True r1b1k2r/pppp1ppp/2nb1n2/8/6qP/N6K/PPPP1PP1/R1r2B1R\n","True r3k2r/pppb1ppp/5n2/4p3/8/2b3PN/Pn2PP1P/1R2KB1R\n","True r2k1b1r/pppn1p1p/5n2/3p2p1/2P1p2K/1P2q2b/7P/7R\n","True rn2kb1r/ppp2ppp/8/8/2p1p1nq/2P2P1b/PP1PP3/R1BQK3\n","True r4rk1/ppp3pp/2n1b2n/3p4/5q1P/2PPK3/PP2P1Pb/R7\n","False r1bk1Q2/pppp4/2n4p/3Q4/1P2P3/5N2/PP3PPP/RNB1KB1R\n","False rnb2k1r/ppppnQpp/8/4p1N1/1b2P3/2P5/PP1P1PPP/RNB1KB1R\n","False r1b2kn1/pppp1Qpr/n6p/2P1N1B1/4P3/8/PPP2PPP/RN2KB1R\n","True r1b1k2r/ppp2n1p/2n3p1/3P1p2/1q2p2P/2K1P3/P1P1P1P1/2b2B1R\n","False rnbq2kr/pppp1Qpp/8/2P1P2B/8/2NN4/PPP2PPP/R1B2RK1\n","False r1bk1b2/pppn1Npp/5P2/3p4/8/8/PPPPQPPP/RNB1KB1R\n","False r1bk1Q2/pppp2pp/7B/4N3/3PP2b/2NB4/PPP2PPP/R3K2R\n","False 2b3r1/ppkp1Np1/2nQ3p/2B5/4P2P/8/PPP2PP1/R2K1B1R\n","False rnk4Q/pp1p3p/8/1N6/8/5P2/PPP3PP/R1B1KB1R\n","Self-play completed\n","Loaded existing validation data from validation_data.npy\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.1583 - policy_output_accuracy: 0.3000 - policy_output_loss: 3.1583 - value_output_loss: 3.5763e-08 - value_output_mse: 3.5763e-08\n","Epoch 1: loss improved from inf to 3.15831, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 3.1583 - policy_output_accuracy: 0.3000 - policy_output_loss: 3.1583 - value_output_loss: 3.5763e-08 - value_output_mse: 3.5763e-08  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1.2219 - policy_output_accuracy: 0.6167 - policy_output_loss: 1.2219 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","Epoch 1: loss improved from 3.15831 to 1.22192, saving model to /content/drive/My Drive/sigma_checkpoint.weights.h5\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.2219 - policy_output_accuracy: 0.6167 - policy_output_loss: 1.2219 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00  \n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 1.9296 - policy_output_accuracy: 0.4667 - policy_output_loss: 1.9296 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","Epoch 1: loss did not improve from 1.22192\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 1.9296 - policy_output_accuracy: 0.4667 - policy_output_loss: 1.9296 - value_output_loss: 0.0000e+00 - value_output_mse: 0.0000e+00\n","\n","Average Policy Output Loss: 2.1032725175221763\n","Average Value Output Loss: 1.1920929428773283e-08\n","Iteration 5/7000\n","False rnbk1Q2/pp1p2p1/2p1p3/2P4p/8/2NB2Q1/PPPB1PPP/R3K1NR\n","True 3rqbkr/3b2pp/p1n5/1pp1p3/6n1/4P2P/PPP1NPP1/R1Bq1K1R\n","True r1b2r1k/ppp1npbp/7p/8/2p1n2P/7N/PPPqPPP1/R3KB1R\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'NoneType' object has no attribute 'is_expanded'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-41edc190f754>\u001b[0m in \u001b[0;36m<cell line: 261>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m \u001b[0mtrain_sigmachess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games_per_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-41edc190f754>\u001b[0m in \u001b[0;36mtrain_sigmachess\u001b[0;34m(model, num_iterations, num_games_per_iteration)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration {iteration + 1}/{num_iterations}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mreplay_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_games_per_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-41edc190f754>\u001b[0m in \u001b[0;36mself_play\u001b[0;34m(model, num_games, max_workers)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Self-play completed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-41edc190f754>\u001b[0m in \u001b[0;36mplay_vs_stockfish\u001b[0;34m(model, game, replay_buffer)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mmcts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMCTS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0maction_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmcts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mw_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_current_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-84dbe9e5ace4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, initial_state, temperature)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;31m# Selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_expanded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_puct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'is_expanded'"]}]}]}